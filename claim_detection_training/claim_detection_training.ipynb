{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12423990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "import mlflow\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from mlflow import pytorch\n",
    "from pprint import pformat\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec8091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col = pd.read_table('collection.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de6dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col_20k = df_col.loc[:20000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68db3034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', 'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_col_20k.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d30dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Manhattan Project and its atomic bomb help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Essay on The Manhattan Project - The Manhattan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The Manhattan Project was the name for a proje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>versions of each volume as well as complementa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Manhattan Project. This once classified ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>19997</td>\n",
       "      <td>The leaner the beef the faster this process ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>19998</td>\n",
       "      <td>A: It takes 24 to 72 hours for beef to digest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>19999</td>\n",
       "      <td>Digestion Essay The path of digestion begins i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>20000</td>\n",
       "      <td>It takes 24 to 72 hours for beef to digest wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>20001</td>\n",
       "      <td>- Process of how a human body digests a hambur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20001 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  \\\n",
       "0          1   \n",
       "1          2   \n",
       "2          3   \n",
       "3          4   \n",
       "4          5   \n",
       "...      ...   \n",
       "19996  19997   \n",
       "19997  19998   \n",
       "19998  19999   \n",
       "19999  20000   \n",
       "20000  20001   \n",
       "\n",
       "      The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.  \n",
       "0      The Manhattan Project and its atomic bomb help...                                                                                                                                                                                                                                                                                     \n",
       "1      Essay on The Manhattan Project - The Manhattan...                                                                                                                                                                                                                                                                                     \n",
       "2      The Manhattan Project was the name for a proje...                                                                                                                                                                                                                                                                                     \n",
       "3      versions of each volume as well as complementa...                                                                                                                                                                                                                                                                                     \n",
       "4      The Manhattan Project. This once classified ph...                                                                                                                                                                                                                                                                                     \n",
       "...                                                  ...                                                                                                                                                                                                                                                                                     \n",
       "19996  The leaner the beef the faster this process ta...                                                                                                                                                                                                                                                                                     \n",
       "19997  A: It takes 24 to 72 hours for beef to digest ...                                                                                                                                                                                                                                                                                     \n",
       "19998  Digestion Essay The path of digestion begins i...                                                                                                                                                                                                                                                                                     \n",
       "19999  It takes 24 to 72 hours for beef to digest wit...                                                                                                                                                                                                                                                                                     \n",
       "20000  - Process of how a human body digests a hambur...                                                                                                                                                                                                                                                                                     \n",
       "\n",
       "[20001 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_col_20k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07758be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibl1\\AppData\\Local\\Temp\\ipykernel_21812\\662054260.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_col_20k.rename(columns={'0': 'Index', 'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.': 'Text'}, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Manhattan Project and its atomic bomb help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Essay on The Manhattan Project - The Manhattan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Manhattan Project was the name for a proje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>versions of each volume as well as complementa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Manhattan Project. This once classified ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>The leaner the beef the faster this process ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>A: It takes 24 to 72 hours for beef to digest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>Digestion Essay The path of digestion begins i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>It takes 24 to 72 hours for beef to digest wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20001</th>\n",
       "      <td>- Process of how a human body digests a hambur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20001 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "Index                                                   \n",
       "1      The Manhattan Project and its atomic bomb help...\n",
       "2      Essay on The Manhattan Project - The Manhattan...\n",
       "3      The Manhattan Project was the name for a proje...\n",
       "4      versions of each volume as well as complementa...\n",
       "5      The Manhattan Project. This once classified ph...\n",
       "...                                                  ...\n",
       "19997  The leaner the beef the faster this process ta...\n",
       "19998  A: It takes 24 to 72 hours for beef to digest ...\n",
       "19999  Digestion Essay The path of digestion begins i...\n",
       "20000  It takes 24 to 72 hours for beef to digest wit...\n",
       "20001  - Process of how a human body digests a hambur...\n",
       "\n",
       "[20001 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_col_20k.rename(columns={'0': 'Index', 'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.': 'Text'}, inplace=True)\n",
    "df_col_20k.set_index('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a86c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibl1\\AppData\\Local\\Temp\\ipykernel_21812\\2742149399.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_col_20k['label'] = 1\n"
     ]
    }
   ],
   "source": [
    "df_col_20k['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec051e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibl1\\AppData\\Local\\Temp\\ipykernel_21812\\3056883093.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_col_20k.rename(columns={'label': 'labels'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_col_20k.rename(columns={'label': 'labels'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf428c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380fcef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cb = pd.read_csv('crowdsourced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "739057df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22501, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6a48f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df_cb['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e3b6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cb_data = df_cb[['Text', 'Verdict']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9dd99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cb_data.loc[df_cb_data['Verdict'] < 1, 'Verdict'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7310e793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17088"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cb_data[df_cb_data['Verdict'] == 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4133652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5413"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cb_data[df_cb_data['Verdict'] == 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05ded840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibl1\\AppData\\Local\\Temp\\ipykernel_21812\\2552218535.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cb_data.rename(columns={'Verdict': 'labels'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_cb_data.rename(columns={'Verdict': 'labels'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64b9f8e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I think we've seen a deterioration of values.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I think for a while as a nation we condoned th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For a while, as I recall, it even seems to me ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So we've seen a deterioration in values, and o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We got away, we got into this feeling that val...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>You get shot walking to the store.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22497</th>\n",
       "      <td>I will do more for African-Americans and Latin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>All she's done is talk to the African-American...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22499</th>\n",
       "      <td>We are going to make America strong again, and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>We cannot take four more years of Barack Obama...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22501 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  labels\n",
       "0          I think we've seen a deterioration of values.       0\n",
       "1      I think for a while as a nation we condoned th...       0\n",
       "2      For a while, as I recall, it even seems to me ...       0\n",
       "3      So we've seen a deterioration in values, and o...       0\n",
       "4      We got away, we got into this feeling that val...       0\n",
       "...                                                  ...     ...\n",
       "22496                 You get shot walking to the store.       0\n",
       "22497  I will do more for African-Americans and Latin...       0\n",
       "22498  All she's done is talk to the African-American...       0\n",
       "22499  We are going to make America strong again, and...       0\n",
       "22500  We cannot take four more years of Barack Obama...       0\n",
       "\n",
       "[22501 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d6865e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibl1\\AppData\\Local\\Temp\\ipykernel_21812\\1451644329.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_merged = df_cb_data.append(df_col_20k, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df_merged = df_cb_data.append(df_col_20k, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e945f4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I think we've seen a deterioration of values.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I think for a while as a nation we condoned th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For a while, as I recall, it even seems to me ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So we've seen a deterioration in values, and o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We got away, we got into this feeling that val...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42497</th>\n",
       "      <td>The leaner the beef the faster this process ta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42498</th>\n",
       "      <td>A: It takes 24 to 72 hours for beef to digest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42499</th>\n",
       "      <td>Digestion Essay The path of digestion begins i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42500</th>\n",
       "      <td>It takes 24 to 72 hours for beef to digest wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42501</th>\n",
       "      <td>- Process of how a human body digests a hambur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42502 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  labels\n",
       "0          I think we've seen a deterioration of values.       0\n",
       "1      I think for a while as a nation we condoned th...       0\n",
       "2      For a while, as I recall, it even seems to me ...       0\n",
       "3      So we've seen a deterioration in values, and o...       0\n",
       "4      We got away, we got into this feeling that val...       0\n",
       "...                                                  ...     ...\n",
       "42497  The leaner the beef the faster this process ta...       1\n",
       "42498  A: It takes 24 to 72 hours for beef to digest ...       1\n",
       "42499  Digestion Essay The path of digestion begins i...       1\n",
       "42500  It takes 24 to 72 hours for beef to digest wit...       1\n",
       "42501  - Process of how a human body digests a hambur...       1\n",
       "\n",
       "[42502 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.drop('Index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d771a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Digestion Essay The path of digestion begins in the mouth when food is broken down mechanically, into smaller pieces, by the teeth and tongue. The complex carbohydrates found in the food are also broken down chemically with the help of the enzyme (ptyalin), found in saliva.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.loc[42499, 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dc88bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b07b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_dataload_df(model_name, batch_size):\n",
    "    if model_name == 'bert':\n",
    "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif model_name == 'distilbert':\n",
    "        tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    elif model_name == 'roberta':\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('albert-base-v2')\n",
    "    dataset = Dataset.from_pandas(df_merged)\n",
    "    dataset_torch = dataset.map(lambda e: tokenizer(e['Text'], truncation=True, max_length=512, padding='max_length'), batched=True)\n",
    "    dataset_torch.set_format(type='torch', columns=['input_ids', 'labels'])\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset_torch))\n",
    "    test_size = len(dataset_torch) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset_torch, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7254fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_set_size = len(train_loader.dataset)\n",
    "    num_batches = len(train_loader)\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        x, target = data['input_ids'].to(device), data['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = model(x, \n",
    "                             labels=target,\n",
    "                            return_dict=False)\n",
    "#         print(logits)\n",
    "#         loss = F.nll_loss(logits, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = logits.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            batch_size = len(x)\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * batch_size}/{train_set_size} \"\n",
    "                  f\"({100. * batch_idx / num_batches:.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "    \n",
    "        del loss\n",
    "        del logits\n",
    "        gc.collect()\n",
    "    accuracy = correct/train_set_size\n",
    "    print(f'Train Accuracy after epoch {epoch}: {accuracy}')\n",
    "    avg_train_loss = train_loss / num_batches\n",
    "\n",
    "    return avg_train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d22682c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_type, params, epoch, val_accuracy):\n",
    "    params['current_epoch'] = epoch\n",
    "    params['val_accuracy'] = val_accuracy\n",
    "    now = datetime.now()\n",
    "    original_stdout = sys.stdout\n",
    "    accuracy = round(val_accuracy*100)\n",
    "#     with open(f'best_params_{now.month}_{now.day}_{now.hour}_{now.minute}.txt', 'w') as f:\n",
    "#         sys.stdout = f \n",
    "#         print(params)\n",
    "#         sys.stdout = original_stdout\n",
    "#         f.close()\n",
    "#     torch.save(model.state_dict(), f'cd_cb_{model_type}_{accuracy}')\n",
    "#     mlflow.pytorch.save_model(model, f'best_cd_{model_type}_{now.month}_{now.day}_{now.hour}_{now.minute}')\n",
    "    mlflow.pytorch.log_model(model, f'cd_cb_{model_type}_epoch_{epoch}_acc_{accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36c1ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, val_loader):\n",
    "    model.eval()\n",
    "    val_set_size = len(val_loader.dataset)\n",
    "    val_num_batches = len(val_loader)\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            x, target = data['input_ids'].to(device), data['labels'].to(device)\n",
    "            loss, logits = model(x, \n",
    "                             labels=target,\n",
    "                                return_dict=False)\n",
    "#             val_loss += F.nll_loss(logits, target, reduction='sum').item()\n",
    "            val_loss += loss.item()\n",
    "            pred = logits.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss /= val_set_size\n",
    "    accuracy = correct/val_set_size\n",
    "    \n",
    "    del loss\n",
    "    del logits\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Validation set: Average loss: {val_loss:.4f}, Accuracy: {correct}/{val_set_size} \"\n",
    "          f\"({100. * correct / val_set_size:.0f}%)\\n\")\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff67e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def suggest_hyperparameters(trial):\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "#     optimizer_name = trial.suggest_categorical(\"optimizer_name\", [\"Adam\", \"AdamW\"])\n",
    "#     epochs = trial.suggest_int(\"epochs\", 1, 10, step=1)\n",
    "#     hidden_size = trial.suggest_categorical(\"hidden_size\", [64, 128, 256, 512])\n",
    "#     num_layers = trial.suggest_categorical(\"num_layers\", [2, 3, 4])\n",
    "#     dropout = trial.suggest_float(\"dropout\", 0.1, 0.9, step=0.1)\n",
    "#     model = trial.suggest_categorical(\"model\", ['bert', 'distilbert', 'roberta'])\n",
    "#     batch_size = trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "#     gamma = trial.suggest_float(\"gamma\", 0.1, 0.9, step=0.1)\n",
    "#     scheduler = trial.suggest_categorical(\"scheduler\", ['step', 'exponential'])\n",
    "\n",
    "#     return lr, optimizer_name, epochs, hidden_size, num_layers, dropout, model, batch_size, gamma, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b9c89ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_hyperparameters(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer_name\", [\"Adam\", \"AdamW\"])\n",
    "    epochs = trial.suggest_int(\"epochs\", 1, 5, step=1)\n",
    "#     model = trial.suggest_categorical(\"model\", ['bert', 'distilbert', 'roberta', 'albert'])\n",
    "    model = trial.suggest_categorical(\"model\", ['distilbert'])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32])\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.1, 0.9, step=0.1)\n",
    "    scheduler = trial.suggest_categorical(\"scheduler\", ['step', 'exponential'])\n",
    "\n",
    "    return lr, optimizer_name, epochs, model, batch_size, gamma, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afb9f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, RobertaModel, DistilBertModel, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9df2fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35212647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, DistilBertForSequenceClassification, AlbertForSequenceClassification, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4423293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    best_val_loss = float('Inf')\n",
    "    best_val_accuracy = 0\n",
    "\n",
    "    with mlflow.start_run():\n",
    "#         lr, optimizer_name, epochs, hidden_size, num_layers, dropout, model_type, batch_size, gamma, sched = suggest_hyperparameters(trial)\n",
    "        lr, optimizer_name, epochs, model_type, batch_size, gamma, sched = suggest_hyperparameters(trial)\n",
    "        mlflow.log_params(trial.params)\n",
    "        print(f'Trial params: {trial.params}')\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        mlflow.log_param(\"device\", device)\n",
    "        \n",
    "        if model_type == 'bert':\n",
    "            train_loader, test_loader = tokenize_and_dataload_df('bert', batch_size)\n",
    "#             bert = BertModel.from_pretrained('bert-base-cased').to(device)\n",
    "            model = BertForSequenceClassification.from_pretrained('bert-base-uncased', \n",
    "                                                                 num_labels = 2,\n",
    "                                                                 output_attentions = False,\n",
    "                                                                 output_hidden_states = False).to(device)\n",
    "        elif model_type == 'distilbert':\n",
    "            train_loader, test_loader = tokenize_and_dataload_df('distilbert', batch_size)\n",
    "            model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
    "                                                                         num_labels = 2,\n",
    "                                                                         output_attentions = False,\n",
    "                                                                         output_hidden_states = False).to(device)\n",
    "        elif model_type == 'roberta':\n",
    "            model = RobertaForSequenceClassification.from_pretrained('roberta-base',\n",
    "                                                                     num_labels = 2,\n",
    "                                                                     output_attentions = False,\n",
    "                                                                     output_hidden_states = False).to(device)\n",
    "            train_loader, test_loader = tokenize_and_dataload_df('roberta', batch_size)\n",
    "        \n",
    "        else:\n",
    "            model = AlbertForSequenceClassification.from_pretrained('albert-base-v2',\n",
    "                                                                     num_labels = 2,\n",
    "                                                                     output_attentions = False,\n",
    "                                                                     output_hidden_states = False).to(device)\n",
    "            train_loader, test_loader = tokenize_and_dataload_df('albert', batch_size)\n",
    "\n",
    "#         model = BERTGRUSentiment(bert,\n",
    "#                          model_type,\n",
    "#                          hidden_size,\n",
    "#                          2,\n",
    "#                          num_layers,\n",
    "#                          True,\n",
    "#                          dropout).to(device)\n",
    "\n",
    "        if optimizer_name == \"Adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        if optimizer_name == \"AdamW\":\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        \n",
    "        if sched == 'step':\n",
    "            scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "        else:\n",
    "            scheduler = ExponentialLR(optimizer, gamma=gamma)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            avg_train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "            avg_val_loss, val_accuracy = validate(model, device, test_loader)\n",
    "            \n",
    "            if avg_val_loss <= best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "\n",
    "            mlflow.log_metric(\"avg_train_losses\", avg_train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"avg_val_loss\", avg_val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"train_accuracy\", train_accuracy, step=epoch)\n",
    "            mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            global global_val_accuracy\n",
    "            if val_accuracy > global_val_accuracy:\n",
    "                print(f'Saving model. Best validation accuracy: {val_accuracy}')\n",
    "                save_model(model, model_type, trial.params, epoch, val_accuracy)\n",
    "                global_val_accuracy = val_accuracy\n",
    "    \n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "#     return best_val_loss\n",
    "#     return avg_val_loss\n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a491556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from optuna.samplers import TPESampler\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb91f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model(best_trial_params):\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lr = best_trial_params['lr']\n",
    "    optimizer_name = best_trial_params['optimizer_name']\n",
    "    epochs = best_trial_params['epochs']\n",
    "    model_type = best_trial_params['model']\n",
    "    batch_size = best_trial_params['batch_size']\n",
    "    gamma = best_trial_params['gamma']\n",
    "    sched = best_trial_params['scheduler']\n",
    "    \n",
    "    if model_type == 'bert':\n",
    "            train_loader, test_loader = tokenize_and_dataload_df('bert', batch_size)\n",
    "            model = BertForSequenceClassification.from_pretrained('bert-base-uncased', \n",
    "                                                                 num_labels = 2,\n",
    "                                                                 output_attentions = False,\n",
    "                                                                 output_hidden_states = False).to(device)\n",
    "    elif model_type == 'distilbert':\n",
    "        train_loader, test_loader = tokenize_and_dataload_df('distilbert', batch_size)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
    "                                                                     num_labels = 2,\n",
    "                                                                     output_attentions = False,\n",
    "                                                                     output_hidden_states = False).to(device)\n",
    "    else:\n",
    "        model = RobertaForSequenceClassification.from_pretrained('roberta-base',\n",
    "                                                                 num_labels = 2,\n",
    "                                                                 output_attentions = False,\n",
    "                                                                 output_hidden_states = False).to(device)\n",
    "        train_loader, test_loader = tokenize_and_dataload_df('roberta', batch_size)\n",
    "        \n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    if optimizer_name == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    if sched == 'step':\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    else:\n",
    "        scheduler = ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "        avg_val_loss, val_accuracy = validate(model, device, test_loader)\n",
    "\n",
    "    best_val_accuracy = val_accuracy\n",
    "    original_stdout = sys.stdout\n",
    "    best_trial_params['val_accuracy'] = val_accuracy\n",
    "    now = datetime.now()\n",
    "    with open(f'best_params_{now.month}_{now.day}_{now.hour}_{now.minute}.txt', 'w') as f:\n",
    "        sys.stdout = f \n",
    "        print(best_trial_params)\n",
    "        sys.stdout = original_stdout\n",
    "        f.close()\n",
    "    torch.save(model.state_dict(), f'best_cd_{model_type}_{now.month}_{now.day}_{now.hour}_{now.minute}.pth')\n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7d812dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    best_val_accuracy = 0\n",
    "\n",
    "    study = optuna.create_study(study_name=\"claim_detection\", \n",
    "                                direction=\"maximize\", \n",
    "                                sampler=TPESampler(seed=2))\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Trial number: \", trial.number)\n",
    "    print(\"  Loss (trial value): \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    \n",
    "#     train_best_model(trial.params.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d8334f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-29 10:29:50,681]\u001b[0m A new study created in memory with name: claim_detection\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial params: {'lr': 2.0322854432411518e-05, 'optimizer_name': 'AdamW', 'epochs': 3, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.4, 'scheduler': 'step'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/34001 (0%)]\tLoss: 0.695465\n",
      "Train Epoch: 0 [320/34001 (1%)]\tLoss: 0.529379\n",
      "Train Epoch: 0 [640/34001 (2%)]\tLoss: 0.502494\n",
      "Train Epoch: 0 [960/34001 (3%)]\tLoss: 0.402557\n",
      "Train Epoch: 0 [1280/34001 (4%)]\tLoss: 0.421341\n",
      "Train Epoch: 0 [1600/34001 (5%)]\tLoss: 0.329152\n",
      "Train Epoch: 0 [1920/34001 (6%)]\tLoss: 0.127791\n",
      "Train Epoch: 0 [2240/34001 (7%)]\tLoss: 0.365067\n",
      "Train Epoch: 0 [2560/34001 (8%)]\tLoss: 0.241767\n",
      "Train Epoch: 0 [2880/34001 (8%)]\tLoss: 0.160797\n",
      "Train Epoch: 0 [3200/34001 (9%)]\tLoss: 0.242198\n",
      "Train Epoch: 0 [3520/34001 (10%)]\tLoss: 0.148874\n",
      "Train Epoch: 0 [3840/34001 (11%)]\tLoss: 0.242641\n",
      "Train Epoch: 0 [4160/34001 (12%)]\tLoss: 0.367958\n",
      "Train Epoch: 0 [4480/34001 (13%)]\tLoss: 0.321229\n",
      "Train Epoch: 0 [4800/34001 (14%)]\tLoss: 0.188501\n",
      "Train Epoch: 0 [5120/34001 (15%)]\tLoss: 0.188302\n",
      "Train Epoch: 0 [5440/34001 (16%)]\tLoss: 0.380998\n",
      "Train Epoch: 0 [5760/34001 (17%)]\tLoss: 0.296413\n",
      "Train Epoch: 0 [6080/34001 (18%)]\tLoss: 0.218600\n",
      "Train Epoch: 0 [6400/34001 (19%)]\tLoss: 0.412535\n",
      "Train Epoch: 0 [6720/34001 (20%)]\tLoss: 0.251101\n",
      "Train Epoch: 0 [7040/34001 (21%)]\tLoss: 0.266994\n",
      "Train Epoch: 0 [7360/34001 (22%)]\tLoss: 0.136324\n",
      "Train Epoch: 0 [7680/34001 (23%)]\tLoss: 0.169070\n",
      "Train Epoch: 0 [8000/34001 (24%)]\tLoss: 0.241186\n",
      "Train Epoch: 0 [8320/34001 (24%)]\tLoss: 0.148072\n",
      "Train Epoch: 0 [8640/34001 (25%)]\tLoss: 0.237432\n",
      "Train Epoch: 0 [8960/34001 (26%)]\tLoss: 0.224779\n",
      "Train Epoch: 0 [9280/34001 (27%)]\tLoss: 0.113488\n",
      "Train Epoch: 0 [9600/34001 (28%)]\tLoss: 0.282613\n",
      "Train Epoch: 0 [9920/34001 (29%)]\tLoss: 0.189854\n",
      "Train Epoch: 0 [10240/34001 (30%)]\tLoss: 0.277736\n",
      "Train Epoch: 0 [10560/34001 (31%)]\tLoss: 0.160919\n",
      "Train Epoch: 0 [10880/34001 (32%)]\tLoss: 0.106698\n",
      "Train Epoch: 0 [11200/34001 (33%)]\tLoss: 0.092174\n",
      "Train Epoch: 0 [11520/34001 (34%)]\tLoss: 0.084912\n",
      "Train Epoch: 0 [11840/34001 (35%)]\tLoss: 0.133637\n",
      "Train Epoch: 0 [12160/34001 (36%)]\tLoss: 0.368646\n",
      "Train Epoch: 0 [12480/34001 (37%)]\tLoss: 0.271065\n",
      "Train Epoch: 0 [12800/34001 (38%)]\tLoss: 0.152572\n",
      "Train Epoch: 0 [13120/34001 (39%)]\tLoss: 0.235539\n",
      "Train Epoch: 0 [13440/34001 (40%)]\tLoss: 0.268107\n",
      "Train Epoch: 0 [13760/34001 (40%)]\tLoss: 0.273601\n",
      "Train Epoch: 0 [14080/34001 (41%)]\tLoss: 0.192861\n",
      "Train Epoch: 0 [14400/34001 (42%)]\tLoss: 0.125271\n",
      "Train Epoch: 0 [14720/34001 (43%)]\tLoss: 0.251062\n",
      "Train Epoch: 0 [15040/34001 (44%)]\tLoss: 0.245624\n",
      "Train Epoch: 0 [15360/34001 (45%)]\tLoss: 0.082912\n",
      "Train Epoch: 0 [15680/34001 (46%)]\tLoss: 0.216317\n",
      "Train Epoch: 0 [16000/34001 (47%)]\tLoss: 0.179306\n",
      "Train Epoch: 0 [16320/34001 (48%)]\tLoss: 0.179385\n",
      "Train Epoch: 0 [16640/34001 (49%)]\tLoss: 0.098622\n",
      "Train Epoch: 0 [16960/34001 (50%)]\tLoss: 0.350415\n",
      "Train Epoch: 0 [17280/34001 (51%)]\tLoss: 0.119475\n",
      "Train Epoch: 0 [17600/34001 (52%)]\tLoss: 0.157445\n",
      "Train Epoch: 0 [17920/34001 (53%)]\tLoss: 0.223001\n",
      "Train Epoch: 0 [18240/34001 (54%)]\tLoss: 0.087738\n",
      "Train Epoch: 0 [18560/34001 (55%)]\tLoss: 0.304983\n",
      "Train Epoch: 0 [18880/34001 (56%)]\tLoss: 0.100047\n",
      "Train Epoch: 0 [19200/34001 (56%)]\tLoss: 0.183184\n",
      "Train Epoch: 0 [19520/34001 (57%)]\tLoss: 0.185331\n",
      "Train Epoch: 0 [19840/34001 (58%)]\tLoss: 0.143097\n",
      "Train Epoch: 0 [20160/34001 (59%)]\tLoss: 0.124672\n",
      "Train Epoch: 0 [20480/34001 (60%)]\tLoss: 0.206573\n",
      "Train Epoch: 0 [20800/34001 (61%)]\tLoss: 0.070114\n",
      "Train Epoch: 0 [21120/34001 (62%)]\tLoss: 0.093496\n",
      "Train Epoch: 0 [21440/34001 (63%)]\tLoss: 0.165622\n",
      "Train Epoch: 0 [21760/34001 (64%)]\tLoss: 0.105962\n",
      "Train Epoch: 0 [22080/34001 (65%)]\tLoss: 0.107492\n",
      "Train Epoch: 0 [22400/34001 (66%)]\tLoss: 0.170082\n",
      "Train Epoch: 0 [22720/34001 (67%)]\tLoss: 0.222666\n",
      "Train Epoch: 0 [23040/34001 (68%)]\tLoss: 0.181462\n",
      "Train Epoch: 0 [23360/34001 (69%)]\tLoss: 0.174335\n",
      "Train Epoch: 0 [23680/34001 (70%)]\tLoss: 0.082581\n",
      "Train Epoch: 0 [24000/34001 (71%)]\tLoss: 0.123845\n",
      "Train Epoch: 0 [24320/34001 (71%)]\tLoss: 0.041659\n",
      "Train Epoch: 0 [24640/34001 (72%)]\tLoss: 0.093676\n",
      "Train Epoch: 0 [24960/34001 (73%)]\tLoss: 0.133156\n",
      "Train Epoch: 0 [25280/34001 (74%)]\tLoss: 0.107076\n",
      "Train Epoch: 0 [25600/34001 (75%)]\tLoss: 0.110802\n",
      "Train Epoch: 0 [25920/34001 (76%)]\tLoss: 0.080876\n",
      "Train Epoch: 0 [26240/34001 (77%)]\tLoss: 0.076351\n",
      "Train Epoch: 0 [26560/34001 (78%)]\tLoss: 0.240752\n",
      "Train Epoch: 0 [26880/34001 (79%)]\tLoss: 0.135646\n",
      "Train Epoch: 0 [27200/34001 (80%)]\tLoss: 0.185300\n",
      "Train Epoch: 0 [27520/34001 (81%)]\tLoss: 0.147673\n",
      "Train Epoch: 0 [27840/34001 (82%)]\tLoss: 0.147972\n",
      "Train Epoch: 0 [28160/34001 (83%)]\tLoss: 0.227014\n",
      "Train Epoch: 0 [28480/34001 (84%)]\tLoss: 0.233397\n",
      "Train Epoch: 0 [28800/34001 (85%)]\tLoss: 0.094239\n",
      "Train Epoch: 0 [29120/34001 (86%)]\tLoss: 0.245748\n",
      "Train Epoch: 0 [29440/34001 (87%)]\tLoss: 0.302251\n",
      "Train Epoch: 0 [29760/34001 (87%)]\tLoss: 0.178283\n",
      "Train Epoch: 0 [30080/34001 (88%)]\tLoss: 0.185256\n",
      "Train Epoch: 0 [30400/34001 (89%)]\tLoss: 0.215232\n",
      "Train Epoch: 0 [30720/34001 (90%)]\tLoss: 0.155287\n",
      "Train Epoch: 0 [31040/34001 (91%)]\tLoss: 0.096274\n",
      "Train Epoch: 0 [31360/34001 (92%)]\tLoss: 0.133988\n",
      "Train Epoch: 0 [31680/34001 (93%)]\tLoss: 0.182544\n",
      "Train Epoch: 0 [32000/34001 (94%)]\tLoss: 0.068196\n",
      "Train Epoch: 0 [32320/34001 (95%)]\tLoss: 0.259622\n",
      "Train Epoch: 0 [32640/34001 (96%)]\tLoss: 0.182265\n",
      "Train Epoch: 0 [32960/34001 (97%)]\tLoss: 0.137582\n",
      "Train Epoch: 0 [33280/34001 (98%)]\tLoss: 0.033693\n",
      "Train Epoch: 0 [33600/34001 (99%)]\tLoss: 0.064113\n",
      "Train Epoch: 0 [33920/34001 (100%)]\tLoss: 0.200400\n",
      "Train Accuracy after epoch 0: 0.910973206670392\n",
      "Validation set: Average loss: 0.0054, Accuracy: 7838/8501 (92%)\n",
      "\n",
      "Saving model. Best validation accuracy: 0.9220091753911305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibl1\\anaconda3\\envs\\edutech\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/34001 (0%)]\tLoss: 0.167902\n",
      "Train Epoch: 1 [320/34001 (1%)]\tLoss: 0.197996\n",
      "Train Epoch: 1 [640/34001 (2%)]\tLoss: 0.052029\n",
      "Train Epoch: 1 [960/34001 (3%)]\tLoss: 0.035706\n",
      "Train Epoch: 1 [1280/34001 (4%)]\tLoss: 0.203243\n",
      "Train Epoch: 1 [1600/34001 (5%)]\tLoss: 0.121479\n",
      "Train Epoch: 1 [1920/34001 (6%)]\tLoss: 0.036046\n",
      "Train Epoch: 1 [2240/34001 (7%)]\tLoss: 0.068259\n",
      "Train Epoch: 1 [2560/34001 (8%)]\tLoss: 0.085140\n",
      "Train Epoch: 1 [2880/34001 (8%)]\tLoss: 0.129480\n",
      "Train Epoch: 1 [3200/34001 (9%)]\tLoss: 0.139958\n",
      "Train Epoch: 1 [3520/34001 (10%)]\tLoss: 0.062072\n",
      "Train Epoch: 1 [3840/34001 (11%)]\tLoss: 0.178798\n",
      "Train Epoch: 1 [4160/34001 (12%)]\tLoss: 0.177926\n",
      "Train Epoch: 1 [4480/34001 (13%)]\tLoss: 0.223198\n",
      "Train Epoch: 1 [4800/34001 (14%)]\tLoss: 0.115117\n",
      "Train Epoch: 1 [5120/34001 (15%)]\tLoss: 0.097914\n",
      "Train Epoch: 1 [5440/34001 (16%)]\tLoss: 0.283124\n",
      "Train Epoch: 1 [5760/34001 (17%)]\tLoss: 0.132553\n",
      "Train Epoch: 1 [6080/34001 (18%)]\tLoss: 0.177145\n",
      "Train Epoch: 1 [6400/34001 (19%)]\tLoss: 0.372767\n",
      "Train Epoch: 1 [6720/34001 (20%)]\tLoss: 0.094139\n",
      "Train Epoch: 1 [7040/34001 (21%)]\tLoss: 0.204877\n",
      "Train Epoch: 1 [7360/34001 (22%)]\tLoss: 0.071889\n",
      "Train Epoch: 1 [7680/34001 (23%)]\tLoss: 0.088884\n",
      "Train Epoch: 1 [8000/34001 (24%)]\tLoss: 0.161447\n",
      "Train Epoch: 1 [8320/34001 (24%)]\tLoss: 0.072440\n",
      "Train Epoch: 1 [8640/34001 (25%)]\tLoss: 0.142817\n",
      "Train Epoch: 1 [8960/34001 (26%)]\tLoss: 0.121137\n",
      "Train Epoch: 1 [9280/34001 (27%)]\tLoss: 0.120899\n",
      "Train Epoch: 1 [9600/34001 (28%)]\tLoss: 0.123930\n",
      "Train Epoch: 1 [9920/34001 (29%)]\tLoss: 0.072719\n",
      "Train Epoch: 1 [10240/34001 (30%)]\tLoss: 0.309336\n",
      "Train Epoch: 1 [10560/34001 (31%)]\tLoss: 0.065071\n",
      "Train Epoch: 1 [10880/34001 (32%)]\tLoss: 0.102140\n",
      "Train Epoch: 1 [11200/34001 (33%)]\tLoss: 0.027027\n",
      "Train Epoch: 1 [11520/34001 (34%)]\tLoss: 0.027029\n",
      "Train Epoch: 1 [11840/34001 (35%)]\tLoss: 0.026322\n",
      "Train Epoch: 1 [12160/34001 (36%)]\tLoss: 0.380799\n",
      "Train Epoch: 1 [12480/34001 (37%)]\tLoss: 0.342758\n",
      "Train Epoch: 1 [12800/34001 (38%)]\tLoss: 0.142684\n",
      "Train Epoch: 1 [13120/34001 (39%)]\tLoss: 0.141100\n",
      "Train Epoch: 1 [13440/34001 (40%)]\tLoss: 0.167455\n",
      "Train Epoch: 1 [13760/34001 (40%)]\tLoss: 0.155185\n",
      "Train Epoch: 1 [14080/34001 (41%)]\tLoss: 0.124351\n",
      "Train Epoch: 1 [14400/34001 (42%)]\tLoss: 0.058672\n",
      "Train Epoch: 1 [14720/34001 (43%)]\tLoss: 0.107886\n",
      "Train Epoch: 1 [15040/34001 (44%)]\tLoss: 0.207164\n",
      "Train Epoch: 1 [15360/34001 (45%)]\tLoss: 0.068073\n",
      "Train Epoch: 1 [15680/34001 (46%)]\tLoss: 0.169754\n",
      "Train Epoch: 1 [16000/34001 (47%)]\tLoss: 0.224888\n",
      "Train Epoch: 1 [16320/34001 (48%)]\tLoss: 0.117452\n",
      "Train Epoch: 1 [16640/34001 (49%)]\tLoss: 0.112947\n",
      "Train Epoch: 1 [16960/34001 (50%)]\tLoss: 0.247748\n",
      "Train Epoch: 1 [17280/34001 (51%)]\tLoss: 0.050706\n",
      "Train Epoch: 1 [17600/34001 (52%)]\tLoss: 0.086970\n",
      "Train Epoch: 1 [17920/34001 (53%)]\tLoss: 0.051929\n",
      "Train Epoch: 1 [18240/34001 (54%)]\tLoss: 0.059333\n",
      "Train Epoch: 1 [18560/34001 (55%)]\tLoss: 0.121042\n",
      "Train Epoch: 1 [18880/34001 (56%)]\tLoss: 0.065011\n",
      "Train Epoch: 1 [19200/34001 (56%)]\tLoss: 0.050177\n",
      "Train Epoch: 1 [19520/34001 (57%)]\tLoss: 0.169918\n",
      "Train Epoch: 1 [19840/34001 (58%)]\tLoss: 0.195655\n",
      "Train Epoch: 1 [20160/34001 (59%)]\tLoss: 0.033261\n",
      "Train Epoch: 1 [20480/34001 (60%)]\tLoss: 0.101654\n",
      "Train Epoch: 1 [20800/34001 (61%)]\tLoss: 0.063793\n",
      "Train Epoch: 1 [21120/34001 (62%)]\tLoss: 0.050485\n",
      "Train Epoch: 1 [21440/34001 (63%)]\tLoss: 0.102633\n",
      "Train Epoch: 1 [21760/34001 (64%)]\tLoss: 0.086615\n",
      "Train Epoch: 1 [22080/34001 (65%)]\tLoss: 0.075357\n",
      "Train Epoch: 1 [22400/34001 (66%)]\tLoss: 0.063576\n",
      "Train Epoch: 1 [22720/34001 (67%)]\tLoss: 0.110652\n",
      "Train Epoch: 1 [23040/34001 (68%)]\tLoss: 0.108497\n",
      "Train Epoch: 1 [23360/34001 (69%)]\tLoss: 0.132652\n",
      "Train Epoch: 1 [23680/34001 (70%)]\tLoss: 0.062650\n",
      "Train Epoch: 1 [24000/34001 (71%)]\tLoss: 0.066051\n",
      "Train Epoch: 1 [24320/34001 (71%)]\tLoss: 0.010785\n",
      "Train Epoch: 1 [24640/34001 (72%)]\tLoss: 0.066328\n",
      "Train Epoch: 1 [24960/34001 (73%)]\tLoss: 0.026788\n",
      "Train Epoch: 1 [25280/34001 (74%)]\tLoss: 0.045552\n",
      "Train Epoch: 1 [25600/34001 (75%)]\tLoss: 0.038316\n",
      "Train Epoch: 1 [25920/34001 (76%)]\tLoss: 0.037323\n",
      "Train Epoch: 1 [26240/34001 (77%)]\tLoss: 0.052024\n",
      "Train Epoch: 1 [26560/34001 (78%)]\tLoss: 0.167156\n",
      "Train Epoch: 1 [26880/34001 (79%)]\tLoss: 0.125933\n",
      "Train Epoch: 1 [27200/34001 (80%)]\tLoss: 0.100262\n",
      "Train Epoch: 1 [27520/34001 (81%)]\tLoss: 0.071240\n",
      "Train Epoch: 1 [27840/34001 (82%)]\tLoss: 0.065484\n",
      "Train Epoch: 1 [28160/34001 (83%)]\tLoss: 0.161991\n",
      "Train Epoch: 1 [28480/34001 (84%)]\tLoss: 0.114969\n",
      "Train Epoch: 1 [28800/34001 (85%)]\tLoss: 0.060887\n",
      "Train Epoch: 1 [29120/34001 (86%)]\tLoss: 0.081713\n",
      "Train Epoch: 1 [29440/34001 (87%)]\tLoss: 0.225396\n",
      "Train Epoch: 1 [29760/34001 (87%)]\tLoss: 0.073745\n",
      "Train Epoch: 1 [30080/34001 (88%)]\tLoss: 0.052138\n",
      "Train Epoch: 1 [30400/34001 (89%)]\tLoss: 0.144323\n",
      "Train Epoch: 1 [30720/34001 (90%)]\tLoss: 0.150964\n",
      "Train Epoch: 1 [31040/34001 (91%)]\tLoss: 0.077809\n",
      "Train Epoch: 1 [31360/34001 (92%)]\tLoss: 0.074611\n",
      "Train Epoch: 1 [31680/34001 (93%)]\tLoss: 0.100518\n",
      "Train Epoch: 1 [32000/34001 (94%)]\tLoss: 0.031701\n",
      "Train Epoch: 1 [32320/34001 (95%)]\tLoss: 0.131019\n",
      "Train Epoch: 1 [32640/34001 (96%)]\tLoss: 0.160029\n",
      "Train Epoch: 1 [32960/34001 (97%)]\tLoss: 0.073372\n",
      "Train Epoch: 1 [33280/34001 (98%)]\tLoss: 0.014877\n",
      "Train Epoch: 1 [33600/34001 (99%)]\tLoss: 0.032557\n",
      "Train Epoch: 1 [33920/34001 (100%)]\tLoss: 0.169501\n",
      "Train Accuracy after epoch 1: 0.9499132378459457\n",
      "Validation set: Average loss: 0.0055, Accuracy: 7899/8501 (93%)\n",
      "\n",
      "Saving model. Best validation accuracy: 0.929184801788025\n",
      "Train Epoch: 2 [0/34001 (0%)]\tLoss: 0.143005\n",
      "Train Epoch: 2 [320/34001 (1%)]\tLoss: 0.182490\n",
      "Train Epoch: 2 [640/34001 (2%)]\tLoss: 0.022276\n",
      "Train Epoch: 2 [960/34001 (3%)]\tLoss: 0.042150\n",
      "Train Epoch: 2 [1280/34001 (4%)]\tLoss: 0.161269\n",
      "Train Epoch: 2 [1600/34001 (5%)]\tLoss: 0.090110\n",
      "Train Epoch: 2 [1920/34001 (6%)]\tLoss: 0.014749\n",
      "Train Epoch: 2 [2240/34001 (7%)]\tLoss: 0.068694\n",
      "Train Epoch: 2 [2560/34001 (8%)]\tLoss: 0.049436\n",
      "Train Epoch: 2 [2880/34001 (8%)]\tLoss: 0.085395\n",
      "Train Epoch: 2 [3200/34001 (9%)]\tLoss: 0.075458\n",
      "Train Epoch: 2 [3520/34001 (10%)]\tLoss: 0.044185\n",
      "Train Epoch: 2 [3840/34001 (11%)]\tLoss: 0.125519\n",
      "Train Epoch: 2 [4160/34001 (12%)]\tLoss: 0.080690\n",
      "Train Epoch: 2 [4480/34001 (13%)]\tLoss: 0.137244\n",
      "Train Epoch: 2 [4800/34001 (14%)]\tLoss: 0.039794\n",
      "Train Epoch: 2 [5120/34001 (15%)]\tLoss: 0.109871\n",
      "Train Epoch: 2 [5440/34001 (16%)]\tLoss: 0.220838\n",
      "Train Epoch: 2 [5760/34001 (17%)]\tLoss: 0.065939\n",
      "Train Epoch: 2 [6080/34001 (18%)]\tLoss: 0.094301\n",
      "Train Epoch: 2 [6400/34001 (19%)]\tLoss: 0.221624\n",
      "Train Epoch: 2 [6720/34001 (20%)]\tLoss: 0.078604\n",
      "Train Epoch: 2 [7040/34001 (21%)]\tLoss: 0.172858\n",
      "Train Epoch: 2 [7360/34001 (22%)]\tLoss: 0.065023\n",
      "Train Epoch: 2 [7680/34001 (23%)]\tLoss: 0.104284\n",
      "Train Epoch: 2 [8000/34001 (24%)]\tLoss: 0.100379\n",
      "Train Epoch: 2 [8320/34001 (24%)]\tLoss: 0.074648\n",
      "Train Epoch: 2 [8640/34001 (25%)]\tLoss: 0.166844\n",
      "Train Epoch: 2 [8960/34001 (26%)]\tLoss: 0.061519\n",
      "Train Epoch: 2 [9280/34001 (27%)]\tLoss: 0.041065\n",
      "Train Epoch: 2 [9600/34001 (28%)]\tLoss: 0.078707\n",
      "Train Epoch: 2 [9920/34001 (29%)]\tLoss: 0.033357\n",
      "Train Epoch: 2 [10240/34001 (30%)]\tLoss: 0.206624\n",
      "Train Epoch: 2 [10560/34001 (31%)]\tLoss: 0.102702\n",
      "Train Epoch: 2 [10880/34001 (32%)]\tLoss: 0.072418\n",
      "Train Epoch: 2 [11200/34001 (33%)]\tLoss: 0.015774\n",
      "Train Epoch: 2 [11520/34001 (34%)]\tLoss: 0.009526\n",
      "Train Epoch: 2 [11840/34001 (35%)]\tLoss: 0.025119\n",
      "Train Epoch: 2 [12160/34001 (36%)]\tLoss: 0.265385\n",
      "Train Epoch: 2 [12480/34001 (37%)]\tLoss: 0.214412\n",
      "Train Epoch: 2 [12800/34001 (38%)]\tLoss: 0.085790\n",
      "Train Epoch: 2 [13120/34001 (39%)]\tLoss: 0.048743\n",
      "Train Epoch: 2 [13440/34001 (40%)]\tLoss: 0.048307\n",
      "Train Epoch: 2 [13760/34001 (40%)]\tLoss: 0.231624\n",
      "Train Epoch: 2 [14080/34001 (41%)]\tLoss: 0.108374\n",
      "Train Epoch: 2 [14400/34001 (42%)]\tLoss: 0.048235\n",
      "Train Epoch: 2 [14720/34001 (43%)]\tLoss: 0.052110\n",
      "Train Epoch: 2 [15040/34001 (44%)]\tLoss: 0.158255\n",
      "Train Epoch: 2 [15360/34001 (45%)]\tLoss: 0.046719\n",
      "Train Epoch: 2 [15680/34001 (46%)]\tLoss: 0.080546\n",
      "Train Epoch: 2 [16000/34001 (47%)]\tLoss: 0.147719\n",
      "Train Epoch: 2 [16320/34001 (48%)]\tLoss: 0.055660\n",
      "Train Epoch: 2 [16640/34001 (49%)]\tLoss: 0.031616\n",
      "Train Epoch: 2 [16960/34001 (50%)]\tLoss: 0.174753\n",
      "Train Epoch: 2 [17280/34001 (51%)]\tLoss: 0.017673\n",
      "Train Epoch: 2 [17600/34001 (52%)]\tLoss: 0.054715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [17920/34001 (53%)]\tLoss: 0.039097\n",
      "Train Epoch: 2 [18240/34001 (54%)]\tLoss: 0.056453\n",
      "Train Epoch: 2 [18560/34001 (55%)]\tLoss: 0.084705\n",
      "Train Epoch: 2 [18880/34001 (56%)]\tLoss: 0.025274\n",
      "Train Epoch: 2 [19200/34001 (56%)]\tLoss: 0.028740\n",
      "Train Epoch: 2 [19520/34001 (57%)]\tLoss: 0.086612\n",
      "Train Epoch: 2 [19840/34001 (58%)]\tLoss: 0.081030\n",
      "Train Epoch: 2 [20160/34001 (59%)]\tLoss: 0.015595\n",
      "Train Epoch: 2 [20480/34001 (60%)]\tLoss: 0.059153\n",
      "Train Epoch: 2 [20800/34001 (61%)]\tLoss: 0.045711\n",
      "Train Epoch: 2 [21120/34001 (62%)]\tLoss: 0.045335\n",
      "Train Epoch: 2 [21440/34001 (63%)]\tLoss: 0.052262\n",
      "Train Epoch: 2 [21760/34001 (64%)]\tLoss: 0.091127\n",
      "Train Epoch: 2 [22080/34001 (65%)]\tLoss: 0.052796\n",
      "Train Epoch: 2 [22400/34001 (66%)]\tLoss: 0.048709\n",
      "Train Epoch: 2 [22720/34001 (67%)]\tLoss: 0.079198\n",
      "Train Epoch: 2 [23040/34001 (68%)]\tLoss: 0.073039\n",
      "Train Epoch: 2 [23360/34001 (69%)]\tLoss: 0.105565\n",
      "Train Epoch: 2 [23680/34001 (70%)]\tLoss: 0.017472\n",
      "Train Epoch: 2 [24000/34001 (71%)]\tLoss: 0.055471\n",
      "Train Epoch: 2 [24320/34001 (71%)]\tLoss: 0.005325\n",
      "Train Epoch: 2 [24640/34001 (72%)]\tLoss: 0.071810\n",
      "Train Epoch: 2 [24960/34001 (73%)]\tLoss: 0.010329\n",
      "Train Epoch: 2 [25280/34001 (74%)]\tLoss: 0.027053\n",
      "Train Epoch: 2 [25600/34001 (75%)]\tLoss: 0.033991\n",
      "Train Epoch: 2 [25920/34001 (76%)]\tLoss: 0.025859\n",
      "Train Epoch: 2 [26240/34001 (77%)]\tLoss: 0.022332\n",
      "Train Epoch: 2 [26560/34001 (78%)]\tLoss: 0.094856\n",
      "Train Epoch: 2 [26880/34001 (79%)]\tLoss: 0.077634\n",
      "Train Epoch: 2 [27200/34001 (80%)]\tLoss: 0.137039\n",
      "Train Epoch: 2 [27520/34001 (81%)]\tLoss: 0.074315\n",
      "Train Epoch: 2 [27840/34001 (82%)]\tLoss: 0.011677\n",
      "Train Epoch: 2 [28160/34001 (83%)]\tLoss: 0.096093\n",
      "Train Epoch: 2 [28480/34001 (84%)]\tLoss: 0.049962\n",
      "Train Epoch: 2 [28800/34001 (85%)]\tLoss: 0.055660\n",
      "Train Epoch: 2 [29120/34001 (86%)]\tLoss: 0.028438\n",
      "Train Epoch: 2 [29440/34001 (87%)]\tLoss: 0.156990\n",
      "Train Epoch: 2 [29760/34001 (87%)]\tLoss: 0.025197\n",
      "Train Epoch: 2 [30080/34001 (88%)]\tLoss: 0.007325\n",
      "Train Epoch: 2 [30400/34001 (89%)]\tLoss: 0.096362\n",
      "Train Epoch: 2 [30720/34001 (90%)]\tLoss: 0.063103\n",
      "Train Epoch: 2 [31040/34001 (91%)]\tLoss: 0.042644\n",
      "Train Epoch: 2 [31360/34001 (92%)]\tLoss: 0.100578\n",
      "Train Epoch: 2 [31680/34001 (93%)]\tLoss: 0.089543\n",
      "Train Epoch: 2 [32000/34001 (94%)]\tLoss: 0.015319\n",
      "Train Epoch: 2 [32320/34001 (95%)]\tLoss: 0.044917\n",
      "Train Epoch: 2 [32640/34001 (96%)]\tLoss: 0.163706\n",
      "Train Epoch: 2 [32960/34001 (97%)]\tLoss: 0.038194\n",
      "Train Epoch: 2 [33280/34001 (98%)]\tLoss: 0.002839\n",
      "Train Epoch: 2 [33600/34001 (99%)]\tLoss: 0.046630\n",
      "Train Epoch: 2 [33920/34001 (100%)]\tLoss: 0.046625\n",
      "Train Accuracy after epoch 2: 0.9682362283462251\n",
      "Validation set: Average loss: 0.0061, Accuracy: 7910/8501 (93%)\n",
      "\n",
      "Saving model. Best validation accuracy: 0.9304787672038584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-29 10:58:58,247]\u001b[0m Trial 0 finished with value: 0.9304787672038584 and parameters: {'lr': 2.0322854432411518e-05, 'optimizer_name': 'AdamW', 'epochs': 3, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.4, 'scheduler': 'step'}. Best is trial 0 with value: 0.9304787672038584.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial params: {'lr': 7.207968815585904e-05, 'optimizer_name': 'Adam', 'epochs': 4, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.5, 'scheduler': 'exponential'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/34001 (0%)]\tLoss: 0.716641\n",
      "Train Epoch: 0 [320/34001 (1%)]\tLoss: 0.617548\n",
      "Train Epoch: 0 [640/34001 (2%)]\tLoss: 0.481498\n",
      "Train Epoch: 0 [960/34001 (3%)]\tLoss: 0.339946\n",
      "Train Epoch: 0 [1280/34001 (4%)]\tLoss: 0.466420\n",
      "Train Epoch: 0 [1600/34001 (5%)]\tLoss: 0.248698\n",
      "Train Epoch: 0 [1920/34001 (6%)]\tLoss: 0.247811\n",
      "Train Epoch: 0 [2240/34001 (7%)]\tLoss: 0.166139\n",
      "Train Epoch: 0 [2560/34001 (8%)]\tLoss: 0.293756\n",
      "Train Epoch: 0 [2880/34001 (8%)]\tLoss: 0.222793\n",
      "Train Epoch: 0 [3200/34001 (9%)]\tLoss: 0.243478\n",
      "Train Epoch: 0 [3520/34001 (10%)]\tLoss: 0.363648\n",
      "Train Epoch: 0 [3840/34001 (11%)]\tLoss: 0.139975\n",
      "Train Epoch: 0 [4160/34001 (12%)]\tLoss: 0.463894\n",
      "Train Epoch: 0 [4480/34001 (13%)]\tLoss: 0.205069\n",
      "Train Epoch: 0 [4800/34001 (14%)]\tLoss: 0.332193\n",
      "Train Epoch: 0 [5120/34001 (15%)]\tLoss: 0.194537\n",
      "Train Epoch: 0 [5440/34001 (16%)]\tLoss: 0.244496\n",
      "Train Epoch: 0 [5760/34001 (17%)]\tLoss: 0.147709\n",
      "Train Epoch: 0 [6080/34001 (18%)]\tLoss: 0.132666\n",
      "Train Epoch: 0 [6400/34001 (19%)]\tLoss: 0.070401\n",
      "Train Epoch: 0 [6720/34001 (20%)]\tLoss: 0.394272\n",
      "Train Epoch: 0 [7040/34001 (21%)]\tLoss: 0.268013\n",
      "Train Epoch: 0 [7360/34001 (22%)]\tLoss: 0.105457\n",
      "Train Epoch: 0 [7680/34001 (23%)]\tLoss: 0.304009\n",
      "Train Epoch: 0 [8000/34001 (24%)]\tLoss: 0.210231\n",
      "Train Epoch: 0 [8320/34001 (24%)]\tLoss: 0.174259\n",
      "Train Epoch: 0 [8640/34001 (25%)]\tLoss: 0.105687\n",
      "Train Epoch: 0 [8960/34001 (26%)]\tLoss: 0.312135\n",
      "Train Epoch: 0 [9280/34001 (27%)]\tLoss: 0.259678\n",
      "Train Epoch: 0 [9600/34001 (28%)]\tLoss: 0.160852\n",
      "Train Epoch: 0 [9920/34001 (29%)]\tLoss: 0.203635\n",
      "Train Epoch: 0 [10240/34001 (30%)]\tLoss: 0.285896\n",
      "Train Epoch: 0 [10560/34001 (31%)]\tLoss: 0.216645\n",
      "Train Epoch: 0 [10880/34001 (32%)]\tLoss: 0.103090\n",
      "Train Epoch: 0 [11200/34001 (33%)]\tLoss: 0.345377\n",
      "Train Epoch: 0 [11520/34001 (34%)]\tLoss: 0.110236\n",
      "Train Epoch: 0 [11840/34001 (35%)]\tLoss: 0.401873\n",
      "Train Epoch: 0 [12160/34001 (36%)]\tLoss: 0.100147\n",
      "Train Epoch: 0 [12480/34001 (37%)]\tLoss: 0.308464\n",
      "Train Epoch: 0 [12800/34001 (38%)]\tLoss: 0.119175\n",
      "Train Epoch: 0 [13120/34001 (39%)]\tLoss: 0.424319\n",
      "Train Epoch: 0 [13440/34001 (40%)]\tLoss: 0.127710\n",
      "Train Epoch: 0 [13760/34001 (40%)]\tLoss: 0.087393\n",
      "Train Epoch: 0 [14080/34001 (41%)]\tLoss: 0.216562\n",
      "Train Epoch: 0 [14400/34001 (42%)]\tLoss: 0.223159\n",
      "Train Epoch: 0 [14720/34001 (43%)]\tLoss: 0.179290\n",
      "Train Epoch: 0 [15040/34001 (44%)]\tLoss: 0.242135\n",
      "Train Epoch: 0 [15360/34001 (45%)]\tLoss: 0.165430\n",
      "Train Epoch: 0 [15680/34001 (46%)]\tLoss: 0.129244\n",
      "Train Epoch: 0 [16000/34001 (47%)]\tLoss: 0.107148\n",
      "Train Epoch: 0 [16320/34001 (48%)]\tLoss: 0.139464\n",
      "Train Epoch: 0 [16640/34001 (49%)]\tLoss: 0.218728\n",
      "Train Epoch: 0 [16960/34001 (50%)]\tLoss: 0.134309\n",
      "Train Epoch: 0 [17280/34001 (51%)]\tLoss: 0.152137\n",
      "Train Epoch: 0 [17600/34001 (52%)]\tLoss: 0.109332\n",
      "Train Epoch: 0 [17920/34001 (53%)]\tLoss: 0.091254\n",
      "Train Epoch: 0 [18240/34001 (54%)]\tLoss: 0.120722\n",
      "Train Epoch: 0 [18560/34001 (55%)]\tLoss: 0.273327\n",
      "Train Epoch: 0 [18880/34001 (56%)]\tLoss: 0.325981\n",
      "Train Epoch: 0 [19200/34001 (56%)]\tLoss: 0.285569\n",
      "Train Epoch: 0 [19520/34001 (57%)]\tLoss: 0.114767\n",
      "Train Epoch: 0 [19840/34001 (58%)]\tLoss: 0.232125\n",
      "Train Epoch: 0 [20160/34001 (59%)]\tLoss: 0.271660\n",
      "Train Epoch: 0 [20480/34001 (60%)]\tLoss: 0.115225\n",
      "Train Epoch: 0 [20800/34001 (61%)]\tLoss: 0.239253\n",
      "Train Epoch: 0 [21120/34001 (62%)]\tLoss: 0.196427\n",
      "Train Epoch: 0 [21440/34001 (63%)]\tLoss: 0.342163\n",
      "Train Epoch: 0 [21760/34001 (64%)]\tLoss: 0.323374\n",
      "Train Epoch: 0 [22080/34001 (65%)]\tLoss: 0.240840\n",
      "Train Epoch: 0 [22400/34001 (66%)]\tLoss: 0.320323\n",
      "Train Epoch: 0 [22720/34001 (67%)]\tLoss: 0.120968\n",
      "Train Epoch: 0 [23040/34001 (68%)]\tLoss: 0.183777\n",
      "Train Epoch: 0 [23360/34001 (69%)]\tLoss: 0.298579\n",
      "Train Epoch: 0 [23680/34001 (70%)]\tLoss: 0.146768\n",
      "Train Epoch: 0 [24000/34001 (71%)]\tLoss: 0.134470\n",
      "Train Epoch: 0 [24320/34001 (71%)]\tLoss: 0.098444\n",
      "Train Epoch: 0 [24640/34001 (72%)]\tLoss: 0.229188\n",
      "Train Epoch: 0 [24960/34001 (73%)]\tLoss: 0.098441\n",
      "Train Epoch: 0 [25280/34001 (74%)]\tLoss: 0.115841\n",
      "Train Epoch: 0 [25600/34001 (75%)]\tLoss: 0.159243\n",
      "Train Epoch: 0 [25920/34001 (76%)]\tLoss: 0.174887\n",
      "Train Epoch: 0 [26240/34001 (77%)]\tLoss: 0.096998\n",
      "Train Epoch: 0 [26560/34001 (78%)]\tLoss: 0.172103\n",
      "Train Epoch: 0 [26880/34001 (79%)]\tLoss: 0.047931\n",
      "Train Epoch: 0 [27200/34001 (80%)]\tLoss: 0.222461\n",
      "Train Epoch: 0 [27520/34001 (81%)]\tLoss: 0.105641\n",
      "Train Epoch: 0 [27840/34001 (82%)]\tLoss: 0.113913\n",
      "Train Epoch: 0 [28160/34001 (83%)]\tLoss: 0.227450\n",
      "Train Epoch: 0 [28480/34001 (84%)]\tLoss: 0.183525\n",
      "Train Epoch: 0 [28800/34001 (85%)]\tLoss: 0.158883\n",
      "Train Epoch: 0 [29120/34001 (86%)]\tLoss: 0.192515\n",
      "Train Epoch: 0 [29440/34001 (87%)]\tLoss: 0.187967\n",
      "Train Epoch: 0 [29760/34001 (87%)]\tLoss: 0.290077\n",
      "Train Epoch: 0 [30080/34001 (88%)]\tLoss: 0.172317\n",
      "Train Epoch: 0 [30400/34001 (89%)]\tLoss: 0.066155\n",
      "Train Epoch: 0 [30720/34001 (90%)]\tLoss: 0.230948\n",
      "Train Epoch: 0 [31040/34001 (91%)]\tLoss: 0.145292\n",
      "Train Epoch: 0 [31360/34001 (92%)]\tLoss: 0.178740\n",
      "Train Epoch: 0 [31680/34001 (93%)]\tLoss: 0.136428\n",
      "Train Epoch: 0 [32000/34001 (94%)]\tLoss: 0.322376\n",
      "Train Epoch: 0 [32320/34001 (95%)]\tLoss: 0.066370\n",
      "Train Epoch: 0 [32640/34001 (96%)]\tLoss: 0.281899\n",
      "Train Epoch: 0 [32960/34001 (97%)]\tLoss: 0.298478\n",
      "Train Epoch: 0 [33280/34001 (98%)]\tLoss: 0.049709\n",
      "Train Epoch: 0 [33600/34001 (99%)]\tLoss: 0.048644\n",
      "Train Epoch: 0 [33920/34001 (100%)]\tLoss: 0.112832\n",
      "Train Accuracy after epoch 0: 0.906149819122967\n",
      "Validation set: Average loss: 0.0057, Accuracy: 7845/8501 (92%)\n",
      "\n",
      "Train Epoch: 1 [0/34001 (0%)]\tLoss: 0.160317\n",
      "Train Epoch: 1 [320/34001 (1%)]\tLoss: 0.142408\n",
      "Train Epoch: 1 [640/34001 (2%)]\tLoss: 0.126041\n",
      "Train Epoch: 1 [960/34001 (3%)]\tLoss: 0.121832\n",
      "Train Epoch: 1 [1280/34001 (4%)]\tLoss: 0.123335\n",
      "Train Epoch: 1 [1600/34001 (5%)]\tLoss: 0.083285\n",
      "Train Epoch: 1 [1920/34001 (6%)]\tLoss: 0.099620\n",
      "Train Epoch: 1 [2240/34001 (7%)]\tLoss: 0.069357\n",
      "Train Epoch: 1 [2560/34001 (8%)]\tLoss: 0.214772\n",
      "Train Epoch: 1 [2880/34001 (8%)]\tLoss: 0.135412\n",
      "Train Epoch: 1 [3200/34001 (9%)]\tLoss: 0.148205\n",
      "Train Epoch: 1 [3520/34001 (10%)]\tLoss: 0.095844\n",
      "Train Epoch: 1 [3840/34001 (11%)]\tLoss: 0.103211\n",
      "Train Epoch: 1 [4160/34001 (12%)]\tLoss: 0.149270\n",
      "Train Epoch: 1 [4480/34001 (13%)]\tLoss: 0.095903\n",
      "Train Epoch: 1 [4800/34001 (14%)]\tLoss: 0.078901\n",
      "Train Epoch: 1 [5120/34001 (15%)]\tLoss: 0.095980\n",
      "Train Epoch: 1 [5440/34001 (16%)]\tLoss: 0.123829\n",
      "Train Epoch: 1 [5760/34001 (17%)]\tLoss: 0.032787\n",
      "Train Epoch: 1 [6080/34001 (18%)]\tLoss: 0.201468\n",
      "Train Epoch: 1 [6400/34001 (19%)]\tLoss: 0.041792\n",
      "Train Epoch: 1 [6720/34001 (20%)]\tLoss: 0.171636\n",
      "Train Epoch: 1 [7040/34001 (21%)]\tLoss: 0.118875\n",
      "Train Epoch: 1 [7360/34001 (22%)]\tLoss: 0.082243\n",
      "Train Epoch: 1 [7680/34001 (23%)]\tLoss: 0.228639\n",
      "Train Epoch: 1 [8000/34001 (24%)]\tLoss: 0.123451\n",
      "Train Epoch: 1 [8320/34001 (24%)]\tLoss: 0.040701\n",
      "Train Epoch: 1 [8640/34001 (25%)]\tLoss: 0.095866\n",
      "Train Epoch: 1 [8960/34001 (26%)]\tLoss: 0.239323\n",
      "Train Epoch: 1 [9280/34001 (27%)]\tLoss: 0.102737\n",
      "Train Epoch: 1 [9600/34001 (28%)]\tLoss: 0.167717\n",
      "Train Epoch: 1 [9920/34001 (29%)]\tLoss: 0.240760\n",
      "Train Epoch: 1 [10240/34001 (30%)]\tLoss: 0.104650\n",
      "Train Epoch: 1 [10560/34001 (31%)]\tLoss: 0.063915\n",
      "Train Epoch: 1 [10880/34001 (32%)]\tLoss: 0.039810\n",
      "Train Epoch: 1 [11200/34001 (33%)]\tLoss: 0.213243\n",
      "Train Epoch: 1 [11520/34001 (34%)]\tLoss: 0.075088\n",
      "Train Epoch: 1 [11840/34001 (35%)]\tLoss: 0.288951\n",
      "Train Epoch: 1 [12160/34001 (36%)]\tLoss: 0.051384\n",
      "Train Epoch: 1 [12480/34001 (37%)]\tLoss: 0.080975\n",
      "Train Epoch: 1 [12800/34001 (38%)]\tLoss: 0.033198\n",
      "Train Epoch: 1 [13120/34001 (39%)]\tLoss: 0.319885\n",
      "Train Epoch: 1 [13440/34001 (40%)]\tLoss: 0.098967\n",
      "Train Epoch: 1 [13760/34001 (40%)]\tLoss: 0.068486\n",
      "Train Epoch: 1 [14080/34001 (41%)]\tLoss: 0.161326\n",
      "Train Epoch: 1 [14400/34001 (42%)]\tLoss: 0.148378\n",
      "Train Epoch: 1 [14720/34001 (43%)]\tLoss: 0.128472\n",
      "Train Epoch: 1 [15040/34001 (44%)]\tLoss: 0.195159\n",
      "Train Epoch: 1 [15360/34001 (45%)]\tLoss: 0.107066\n",
      "Train Epoch: 1 [15680/34001 (46%)]\tLoss: 0.035539\n",
      "Train Epoch: 1 [16000/34001 (47%)]\tLoss: 0.122742\n",
      "Train Epoch: 1 [16320/34001 (48%)]\tLoss: 0.076704\n",
      "Train Epoch: 1 [16640/34001 (49%)]\tLoss: 0.105599\n",
      "Train Epoch: 1 [16960/34001 (50%)]\tLoss: 0.020837\n",
      "Train Epoch: 1 [17280/34001 (51%)]\tLoss: 0.208953\n",
      "Train Epoch: 1 [17600/34001 (52%)]\tLoss: 0.028286\n",
      "Train Epoch: 1 [17920/34001 (53%)]\tLoss: 0.126271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [18240/34001 (54%)]\tLoss: 0.065423\n",
      "Train Epoch: 1 [18560/34001 (55%)]\tLoss: 0.118321\n",
      "Train Epoch: 1 [18880/34001 (56%)]\tLoss: 0.216911\n",
      "Train Epoch: 1 [19200/34001 (56%)]\tLoss: 0.139691\n",
      "Train Epoch: 1 [19520/34001 (57%)]\tLoss: 0.162154\n",
      "Train Epoch: 1 [19840/34001 (58%)]\tLoss: 0.176764\n",
      "Train Epoch: 1 [20160/34001 (59%)]\tLoss: 0.146897\n",
      "Train Epoch: 1 [20480/34001 (60%)]\tLoss: 0.032577\n",
      "Train Epoch: 1 [20800/34001 (61%)]\tLoss: 0.144569\n",
      "Train Epoch: 1 [21120/34001 (62%)]\tLoss: 0.041486\n",
      "Train Epoch: 1 [21440/34001 (63%)]\tLoss: 0.057040\n",
      "Train Epoch: 1 [21760/34001 (64%)]\tLoss: 0.203920\n",
      "Train Epoch: 1 [22080/34001 (65%)]\tLoss: 0.200111\n",
      "Train Epoch: 1 [22400/34001 (66%)]\tLoss: 0.213417\n",
      "Train Epoch: 1 [22720/34001 (67%)]\tLoss: 0.027372\n",
      "Train Epoch: 1 [23040/34001 (68%)]\tLoss: 0.156054\n",
      "Train Epoch: 1 [23360/34001 (69%)]\tLoss: 0.084876\n",
      "Train Epoch: 1 [23680/34001 (70%)]\tLoss: 0.064631\n",
      "Train Epoch: 1 [24000/34001 (71%)]\tLoss: 0.085119\n",
      "Train Epoch: 1 [24320/34001 (71%)]\tLoss: 0.032278\n",
      "Train Epoch: 1 [24640/34001 (72%)]\tLoss: 0.138159\n",
      "Train Epoch: 1 [24960/34001 (73%)]\tLoss: 0.071409\n",
      "Train Epoch: 1 [25280/34001 (74%)]\tLoss: 0.048519\n",
      "Train Epoch: 1 [25600/34001 (75%)]\tLoss: 0.009581\n",
      "Train Epoch: 1 [25920/34001 (76%)]\tLoss: 0.171247\n",
      "Train Epoch: 1 [26240/34001 (77%)]\tLoss: 0.037864\n",
      "Train Epoch: 1 [26560/34001 (78%)]\tLoss: 0.045364\n",
      "Train Epoch: 1 [26880/34001 (79%)]\tLoss: 0.014655\n",
      "Train Epoch: 1 [27200/34001 (80%)]\tLoss: 0.100502\n",
      "Train Epoch: 1 [27520/34001 (81%)]\tLoss: 0.087483\n",
      "Train Epoch: 1 [27840/34001 (82%)]\tLoss: 0.039093\n",
      "Train Epoch: 1 [28160/34001 (83%)]\tLoss: 0.170058\n",
      "Train Epoch: 1 [28480/34001 (84%)]\tLoss: 0.096579\n",
      "Train Epoch: 1 [28800/34001 (85%)]\tLoss: 0.237018\n",
      "Train Epoch: 1 [29120/34001 (86%)]\tLoss: 0.104075\n",
      "Train Epoch: 1 [29440/34001 (87%)]\tLoss: 0.065851\n",
      "Train Epoch: 1 [29760/34001 (87%)]\tLoss: 0.062042\n",
      "Train Epoch: 1 [30080/34001 (88%)]\tLoss: 0.022999\n",
      "Train Epoch: 1 [30400/34001 (89%)]\tLoss: 0.045365\n",
      "Train Epoch: 1 [30720/34001 (90%)]\tLoss: 0.045990\n",
      "Train Epoch: 1 [31040/34001 (91%)]\tLoss: 0.089798\n",
      "Train Epoch: 1 [31360/34001 (92%)]\tLoss: 0.114939\n",
      "Train Epoch: 1 [31680/34001 (93%)]\tLoss: 0.098857\n",
      "Train Epoch: 1 [32000/34001 (94%)]\tLoss: 0.196762\n",
      "Train Epoch: 1 [32320/34001 (95%)]\tLoss: 0.023905\n",
      "Train Epoch: 1 [32640/34001 (96%)]\tLoss: 0.200774\n",
      "Train Epoch: 1 [32960/34001 (97%)]\tLoss: 0.282435\n",
      "Train Epoch: 1 [33280/34001 (98%)]\tLoss: 0.021710\n",
      "Train Epoch: 1 [33600/34001 (99%)]\tLoss: 0.079900\n",
      "Train Epoch: 1 [33920/34001 (100%)]\tLoss: 0.074013\n",
      "Train Accuracy after epoch 1: 0.9499720596453045\n",
      "Validation set: Average loss: 0.0052, Accuracy: 7905/8501 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/34001 (0%)]\tLoss: 0.098390\n",
      "Train Epoch: 2 [320/34001 (1%)]\tLoss: 0.111928\n",
      "Train Epoch: 2 [640/34001 (2%)]\tLoss: 0.075675\n",
      "Train Epoch: 2 [960/34001 (3%)]\tLoss: 0.115566\n",
      "Train Epoch: 2 [1280/34001 (4%)]\tLoss: 0.065774\n",
      "Train Epoch: 2 [1600/34001 (5%)]\tLoss: 0.026856\n",
      "Train Epoch: 2 [1920/34001 (6%)]\tLoss: 0.096343\n",
      "Train Epoch: 2 [2240/34001 (7%)]\tLoss: 0.023515\n",
      "Train Epoch: 2 [2560/34001 (8%)]\tLoss: 0.180089\n",
      "Train Epoch: 2 [2880/34001 (8%)]\tLoss: 0.087615\n",
      "Train Epoch: 2 [3200/34001 (9%)]\tLoss: 0.052563\n",
      "Train Epoch: 2 [3520/34001 (10%)]\tLoss: 0.048647\n",
      "Train Epoch: 2 [3840/34001 (11%)]\tLoss: 0.073148\n",
      "Train Epoch: 2 [4160/34001 (12%)]\tLoss: 0.102523\n",
      "Train Epoch: 2 [4480/34001 (13%)]\tLoss: 0.086901\n",
      "Train Epoch: 2 [4800/34001 (14%)]\tLoss: 0.042007\n",
      "Train Epoch: 2 [5120/34001 (15%)]\tLoss: 0.024498\n",
      "Train Epoch: 2 [5440/34001 (16%)]\tLoss: 0.021602\n",
      "Train Epoch: 2 [5760/34001 (17%)]\tLoss: 0.009052\n",
      "Train Epoch: 2 [6080/34001 (18%)]\tLoss: 0.041849\n",
      "Train Epoch: 2 [6400/34001 (19%)]\tLoss: 0.018385\n",
      "Train Epoch: 2 [6720/34001 (20%)]\tLoss: 0.047504\n",
      "Train Epoch: 2 [7040/34001 (21%)]\tLoss: 0.108998\n",
      "Train Epoch: 2 [7360/34001 (22%)]\tLoss: 0.062976\n",
      "Train Epoch: 2 [7680/34001 (23%)]\tLoss: 0.213622\n",
      "Train Epoch: 2 [8000/34001 (24%)]\tLoss: 0.088261\n",
      "Train Epoch: 2 [8320/34001 (24%)]\tLoss: 0.017181\n",
      "Train Epoch: 2 [8640/34001 (25%)]\tLoss: 0.115404\n",
      "Train Epoch: 2 [8960/34001 (26%)]\tLoss: 0.239939\n",
      "Train Epoch: 2 [9280/34001 (27%)]\tLoss: 0.052582\n",
      "Train Epoch: 2 [9600/34001 (28%)]\tLoss: 0.150221\n",
      "Train Epoch: 2 [9920/34001 (29%)]\tLoss: 0.121268\n",
      "Train Epoch: 2 [10240/34001 (30%)]\tLoss: 0.032733\n",
      "Train Epoch: 2 [10560/34001 (31%)]\tLoss: 0.064677\n",
      "Train Epoch: 2 [10880/34001 (32%)]\tLoss: 0.008311\n",
      "Train Epoch: 2 [11200/34001 (33%)]\tLoss: 0.155556\n",
      "Train Epoch: 2 [11520/34001 (34%)]\tLoss: 0.020890\n",
      "Train Epoch: 2 [11840/34001 (35%)]\tLoss: 0.065267\n",
      "Train Epoch: 2 [12160/34001 (36%)]\tLoss: 0.030773\n",
      "Train Epoch: 2 [12480/34001 (37%)]\tLoss: 0.025779\n",
      "Train Epoch: 2 [12800/34001 (38%)]\tLoss: 0.004553\n",
      "Train Epoch: 2 [13120/34001 (39%)]\tLoss: 0.230937\n",
      "Train Epoch: 2 [13440/34001 (40%)]\tLoss: 0.033412\n",
      "Train Epoch: 2 [13760/34001 (40%)]\tLoss: 0.133249\n",
      "Train Epoch: 2 [14080/34001 (41%)]\tLoss: 0.165099\n",
      "Train Epoch: 2 [14400/34001 (42%)]\tLoss: 0.049418\n",
      "Train Epoch: 2 [14720/34001 (43%)]\tLoss: 0.079664\n",
      "Train Epoch: 2 [15040/34001 (44%)]\tLoss: 0.037467\n",
      "Train Epoch: 2 [15360/34001 (45%)]\tLoss: 0.048449\n",
      "Train Epoch: 2 [15680/34001 (46%)]\tLoss: 0.015971\n",
      "Train Epoch: 2 [16000/34001 (47%)]\tLoss: 0.016571\n",
      "Train Epoch: 2 [16320/34001 (48%)]\tLoss: 0.021441\n",
      "Train Epoch: 2 [16640/34001 (49%)]\tLoss: 0.097211\n",
      "Train Epoch: 2 [16960/34001 (50%)]\tLoss: 0.060419\n",
      "Train Epoch: 2 [17280/34001 (51%)]\tLoss: 0.071793\n",
      "Train Epoch: 2 [17600/34001 (52%)]\tLoss: 0.012169\n",
      "Train Epoch: 2 [17920/34001 (53%)]\tLoss: 0.199487\n",
      "Train Epoch: 2 [18240/34001 (54%)]\tLoss: 0.077853\n",
      "Train Epoch: 2 [18560/34001 (55%)]\tLoss: 0.071093\n",
      "Train Epoch: 2 [18880/34001 (56%)]\tLoss: 0.064047\n",
      "Train Epoch: 2 [19200/34001 (56%)]\tLoss: 0.024944\n",
      "Train Epoch: 2 [19520/34001 (57%)]\tLoss: 0.026157\n",
      "Train Epoch: 2 [19840/34001 (58%)]\tLoss: 0.005920\n",
      "Train Epoch: 2 [20160/34001 (59%)]\tLoss: 0.031084\n",
      "Train Epoch: 2 [20480/34001 (60%)]\tLoss: 0.001811\n",
      "Train Epoch: 2 [20800/34001 (61%)]\tLoss: 0.027139\n",
      "Train Epoch: 2 [21120/34001 (62%)]\tLoss: 0.017373\n",
      "Train Epoch: 2 [21440/34001 (63%)]\tLoss: 0.046614\n",
      "Train Epoch: 2 [21760/34001 (64%)]\tLoss: 0.012051\n",
      "Train Epoch: 2 [22080/34001 (65%)]\tLoss: 0.022790\n",
      "Train Epoch: 2 [22400/34001 (66%)]\tLoss: 0.145366\n",
      "Train Epoch: 2 [22720/34001 (67%)]\tLoss: 0.002541\n",
      "Train Epoch: 2 [23040/34001 (68%)]\tLoss: 0.010605\n",
      "Train Epoch: 2 [23360/34001 (69%)]\tLoss: 0.074380\n",
      "Train Epoch: 2 [23680/34001 (70%)]\tLoss: 0.003128\n",
      "Train Epoch: 2 [24000/34001 (71%)]\tLoss: 0.106990\n",
      "Train Epoch: 2 [24320/34001 (71%)]\tLoss: 0.006248\n",
      "Train Epoch: 2 [24640/34001 (72%)]\tLoss: 0.158807\n",
      "Train Epoch: 2 [24960/34001 (73%)]\tLoss: 0.016473\n",
      "Train Epoch: 2 [25280/34001 (74%)]\tLoss: 0.009272\n",
      "Train Epoch: 2 [25600/34001 (75%)]\tLoss: 0.012611\n",
      "Train Epoch: 2 [25920/34001 (76%)]\tLoss: 0.022332\n",
      "Train Epoch: 2 [26240/34001 (77%)]\tLoss: 0.003842\n",
      "Train Epoch: 2 [26560/34001 (78%)]\tLoss: 0.007497\n",
      "Train Epoch: 2 [26880/34001 (79%)]\tLoss: 0.001419\n",
      "Train Epoch: 2 [27200/34001 (80%)]\tLoss: 0.024555\n",
      "Train Epoch: 2 [27520/34001 (81%)]\tLoss: 0.009134\n",
      "Train Epoch: 2 [27840/34001 (82%)]\tLoss: 0.035003\n",
      "Train Epoch: 2 [28160/34001 (83%)]\tLoss: 0.069796\n",
      "Train Epoch: 2 [28480/34001 (84%)]\tLoss: 0.064823\n",
      "Train Epoch: 2 [28800/34001 (85%)]\tLoss: 0.196538\n",
      "Train Epoch: 2 [29120/34001 (86%)]\tLoss: 0.031164\n",
      "Train Epoch: 2 [29440/34001 (87%)]\tLoss: 0.058040\n",
      "Train Epoch: 2 [29760/34001 (87%)]\tLoss: 0.020263\n",
      "Train Epoch: 2 [30080/34001 (88%)]\tLoss: 0.002518\n",
      "Train Epoch: 2 [30400/34001 (89%)]\tLoss: 0.039851\n",
      "Train Epoch: 2 [30720/34001 (90%)]\tLoss: 0.192390\n",
      "Train Epoch: 2 [31040/34001 (91%)]\tLoss: 0.032949\n",
      "Train Epoch: 2 [31360/34001 (92%)]\tLoss: 0.298698\n",
      "Train Epoch: 2 [31680/34001 (93%)]\tLoss: 0.043949\n",
      "Train Epoch: 2 [32000/34001 (94%)]\tLoss: 0.095802\n",
      "Train Epoch: 2 [32320/34001 (95%)]\tLoss: 0.007379\n",
      "Train Epoch: 2 [32640/34001 (96%)]\tLoss: 0.046729\n",
      "Train Epoch: 2 [32960/34001 (97%)]\tLoss: 0.185829\n",
      "Train Epoch: 2 [33280/34001 (98%)]\tLoss: 0.006961\n",
      "Train Epoch: 2 [33600/34001 (99%)]\tLoss: 0.007259\n",
      "Train Epoch: 2 [33920/34001 (100%)]\tLoss: 0.143432\n",
      "Train Accuracy after epoch 2: 0.97611834946031\n",
      "Validation set: Average loss: 0.0066, Accuracy: 7902/8501 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/34001 (0%)]\tLoss: 0.039566\n",
      "Train Epoch: 3 [320/34001 (1%)]\tLoss: 0.078573\n",
      "Train Epoch: 3 [640/34001 (2%)]\tLoss: 0.008276\n",
      "Train Epoch: 3 [960/34001 (3%)]\tLoss: 0.173588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [1280/34001 (4%)]\tLoss: 0.027600\n",
      "Train Epoch: 3 [1600/34001 (5%)]\tLoss: 0.007761\n",
      "Train Epoch: 3 [1920/34001 (6%)]\tLoss: 0.081961\n",
      "Train Epoch: 3 [2240/34001 (7%)]\tLoss: 0.008824\n",
      "Train Epoch: 3 [2560/34001 (8%)]\tLoss: 0.179340\n",
      "Train Epoch: 3 [2880/34001 (8%)]\tLoss: 0.082342\n",
      "Train Epoch: 3 [3200/34001 (9%)]\tLoss: 0.007781\n",
      "Train Epoch: 3 [3520/34001 (10%)]\tLoss: 0.023074\n",
      "Train Epoch: 3 [3840/34001 (11%)]\tLoss: 0.055689\n",
      "Train Epoch: 3 [4160/34001 (12%)]\tLoss: 0.078562\n",
      "Train Epoch: 3 [4480/34001 (13%)]\tLoss: 0.061271\n",
      "Train Epoch: 3 [4800/34001 (14%)]\tLoss: 0.011473\n",
      "Train Epoch: 3 [5120/34001 (15%)]\tLoss: 0.007371\n",
      "Train Epoch: 3 [5440/34001 (16%)]\tLoss: 0.069599\n",
      "Train Epoch: 3 [5760/34001 (17%)]\tLoss: 0.005115\n",
      "Train Epoch: 3 [6080/34001 (18%)]\tLoss: 0.003920\n",
      "Train Epoch: 3 [6400/34001 (19%)]\tLoss: 0.009382\n",
      "Train Epoch: 3 [6720/34001 (20%)]\tLoss: 0.016147\n",
      "Train Epoch: 3 [7040/34001 (21%)]\tLoss: 0.108339\n",
      "Train Epoch: 3 [7360/34001 (22%)]\tLoss: 0.036844\n",
      "Train Epoch: 3 [7680/34001 (23%)]\tLoss: 0.117857\n",
      "Train Epoch: 3 [8000/34001 (24%)]\tLoss: 0.049959\n",
      "Train Epoch: 3 [8320/34001 (24%)]\tLoss: 0.001684\n",
      "Train Epoch: 3 [8640/34001 (25%)]\tLoss: 0.064049\n",
      "Train Epoch: 3 [8960/34001 (26%)]\tLoss: 0.249997\n",
      "Train Epoch: 3 [9280/34001 (27%)]\tLoss: 0.010215\n",
      "Train Epoch: 3 [9600/34001 (28%)]\tLoss: 0.091480\n",
      "Train Epoch: 3 [9920/34001 (29%)]\tLoss: 0.128326\n",
      "Train Epoch: 3 [10240/34001 (30%)]\tLoss: 0.026803\n",
      "Train Epoch: 3 [10560/34001 (31%)]\tLoss: 0.015816\n",
      "Train Epoch: 3 [10880/34001 (32%)]\tLoss: 0.002889\n",
      "Train Epoch: 3 [11200/34001 (33%)]\tLoss: 0.072554\n",
      "Train Epoch: 3 [11520/34001 (34%)]\tLoss: 0.024225\n",
      "Train Epoch: 3 [11840/34001 (35%)]\tLoss: 0.022991\n",
      "Train Epoch: 3 [12160/34001 (36%)]\tLoss: 0.013263\n",
      "Train Epoch: 3 [12480/34001 (37%)]\tLoss: 0.026970\n",
      "Train Epoch: 3 [12800/34001 (38%)]\tLoss: 0.002011\n",
      "Train Epoch: 3 [13120/34001 (39%)]\tLoss: 0.158856\n",
      "Train Epoch: 3 [13440/34001 (40%)]\tLoss: 0.026728\n",
      "Train Epoch: 3 [13760/34001 (40%)]\tLoss: 0.035252\n",
      "Train Epoch: 3 [14080/34001 (41%)]\tLoss: 0.076957\n",
      "Train Epoch: 3 [14400/34001 (42%)]\tLoss: 0.046481\n",
      "Train Epoch: 3 [14720/34001 (43%)]\tLoss: 0.046506\n",
      "Train Epoch: 3 [15040/34001 (44%)]\tLoss: 0.090328\n",
      "Train Epoch: 3 [15360/34001 (45%)]\tLoss: 0.026202\n",
      "Train Epoch: 3 [15680/34001 (46%)]\tLoss: 0.002057\n",
      "Train Epoch: 3 [16000/34001 (47%)]\tLoss: 0.044529\n",
      "Train Epoch: 3 [16320/34001 (48%)]\tLoss: 0.058626\n",
      "Train Epoch: 3 [16640/34001 (49%)]\tLoss: 0.050009\n",
      "Train Epoch: 3 [16960/34001 (50%)]\tLoss: 0.021154\n",
      "Train Epoch: 3 [17280/34001 (51%)]\tLoss: 0.097678\n",
      "Train Epoch: 3 [17600/34001 (52%)]\tLoss: 0.001382\n",
      "Train Epoch: 3 [17920/34001 (53%)]\tLoss: 0.035447\n",
      "Train Epoch: 3 [18240/34001 (54%)]\tLoss: 0.026075\n",
      "Train Epoch: 3 [18560/34001 (55%)]\tLoss: 0.001293\n",
      "Train Epoch: 3 [18880/34001 (56%)]\tLoss: 0.027983\n",
      "Train Epoch: 3 [19200/34001 (56%)]\tLoss: 0.006191\n",
      "Train Epoch: 3 [19520/34001 (57%)]\tLoss: 0.019705\n",
      "Train Epoch: 3 [19840/34001 (58%)]\tLoss: 0.002966\n",
      "Train Epoch: 3 [20160/34001 (59%)]\tLoss: 0.013538\n",
      "Train Epoch: 3 [20480/34001 (60%)]\tLoss: 0.000415\n",
      "Train Epoch: 3 [20800/34001 (61%)]\tLoss: 0.009040\n",
      "Train Epoch: 3 [21120/34001 (62%)]\tLoss: 0.018736\n",
      "Train Epoch: 3 [21440/34001 (63%)]\tLoss: 0.003721\n",
      "Train Epoch: 3 [21760/34001 (64%)]\tLoss: 0.003859\n",
      "Train Epoch: 3 [22080/34001 (65%)]\tLoss: 0.001376\n",
      "Train Epoch: 3 [22400/34001 (66%)]\tLoss: 0.010201\n",
      "Train Epoch: 3 [22720/34001 (67%)]\tLoss: 0.006654\n",
      "Train Epoch: 3 [23040/34001 (68%)]\tLoss: 0.004051\n",
      "Train Epoch: 3 [23360/34001 (69%)]\tLoss: 0.002010\n",
      "Train Epoch: 3 [23680/34001 (70%)]\tLoss: 0.002622\n",
      "Train Epoch: 3 [24000/34001 (71%)]\tLoss: 0.008144\n",
      "Train Epoch: 3 [24320/34001 (71%)]\tLoss: 0.001385\n",
      "Train Epoch: 3 [24640/34001 (72%)]\tLoss: 0.028936\n",
      "Train Epoch: 3 [24960/34001 (73%)]\tLoss: 0.006180\n",
      "Train Epoch: 3 [25280/34001 (74%)]\tLoss: 0.004591\n",
      "Train Epoch: 3 [25600/34001 (75%)]\tLoss: 0.002567\n",
      "Train Epoch: 3 [25920/34001 (76%)]\tLoss: 0.008498\n",
      "Train Epoch: 3 [26240/34001 (77%)]\tLoss: 0.000736\n",
      "Train Epoch: 3 [26560/34001 (78%)]\tLoss: 0.005177\n",
      "Train Epoch: 3 [26880/34001 (79%)]\tLoss: 0.000810\n",
      "Train Epoch: 3 [27200/34001 (80%)]\tLoss: 0.006924\n",
      "Train Epoch: 3 [27520/34001 (81%)]\tLoss: 0.000795\n",
      "Train Epoch: 3 [27840/34001 (82%)]\tLoss: 0.001700\n",
      "Train Epoch: 3 [28160/34001 (83%)]\tLoss: 0.009569\n",
      "Train Epoch: 3 [28480/34001 (84%)]\tLoss: 0.005651\n",
      "Train Epoch: 3 [28800/34001 (85%)]\tLoss: 0.007773\n",
      "Train Epoch: 3 [29120/34001 (86%)]\tLoss: 0.000475\n",
      "Train Epoch: 3 [29440/34001 (87%)]\tLoss: 0.013861\n",
      "Train Epoch: 3 [29760/34001 (87%)]\tLoss: 0.004172\n",
      "Train Epoch: 3 [30080/34001 (88%)]\tLoss: 0.000839\n",
      "Train Epoch: 3 [30400/34001 (89%)]\tLoss: 0.000788\n",
      "Train Epoch: 3 [30720/34001 (90%)]\tLoss: 0.001213\n",
      "Train Epoch: 3 [31040/34001 (91%)]\tLoss: 0.003404\n",
      "Train Epoch: 3 [31360/34001 (92%)]\tLoss: 0.095614\n",
      "Train Epoch: 3 [31680/34001 (93%)]\tLoss: 0.111127\n",
      "Train Epoch: 3 [32000/34001 (94%)]\tLoss: 0.001870\n",
      "Train Epoch: 3 [32320/34001 (95%)]\tLoss: 0.002838\n",
      "Train Epoch: 3 [32640/34001 (96%)]\tLoss: 0.002583\n",
      "Train Epoch: 3 [32960/34001 (97%)]\tLoss: 0.026398\n",
      "Train Epoch: 3 [33280/34001 (98%)]\tLoss: 0.000406\n",
      "Train Epoch: 3 [33600/34001 (99%)]\tLoss: 0.015979\n",
      "Train Epoch: 3 [33920/34001 (100%)]\tLoss: 0.055897\n",
      "Train Accuracy after epoch 3: 0.9890591453192553\n",
      "Validation set: Average loss: 0.0091, Accuracy: 7903/8501 (93%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-29 11:37:46,986]\u001b[0m Trial 1 finished with value: 0.9296553346665098 and parameters: {'lr': 7.207968815585904e-05, 'optimizer_name': 'Adam', 'epochs': 4, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.5, 'scheduler': 'exponential'}. Best is trial 0 with value: 0.9304787672038584.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial params: {'lr': 3.575358516942407e-06, 'optimizer_name': 'AdamW', 'epochs': 3, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.8, 'scheduler': 'exponential'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/34001 (0%)]\tLoss: 0.688561\n",
      "Train Epoch: 0 [320/34001 (1%)]\tLoss: 0.635063\n",
      "Train Epoch: 0 [640/34001 (2%)]\tLoss: 0.623907\n",
      "Train Epoch: 0 [960/34001 (3%)]\tLoss: 0.560250\n",
      "Train Epoch: 0 [1280/34001 (4%)]\tLoss: 0.519091\n",
      "Train Epoch: 0 [1600/34001 (5%)]\tLoss: 0.503392\n",
      "Train Epoch: 0 [1920/34001 (6%)]\tLoss: 0.436299\n",
      "Train Epoch: 0 [2240/34001 (7%)]\tLoss: 0.354191\n",
      "Train Epoch: 0 [2560/34001 (8%)]\tLoss: 0.447248\n",
      "Train Epoch: 0 [2880/34001 (8%)]\tLoss: 0.264922\n",
      "Train Epoch: 0 [3200/34001 (9%)]\tLoss: 0.339973\n",
      "Train Epoch: 0 [3520/34001 (10%)]\tLoss: 0.435684\n",
      "Train Epoch: 0 [3840/34001 (11%)]\tLoss: 0.439294\n",
      "Train Epoch: 0 [4160/34001 (12%)]\tLoss: 0.465716\n",
      "Train Epoch: 0 [4480/34001 (13%)]\tLoss: 0.327102\n",
      "Train Epoch: 0 [4800/34001 (14%)]\tLoss: 0.311348\n",
      "Train Epoch: 0 [5120/34001 (15%)]\tLoss: 0.368961\n",
      "Train Epoch: 0 [5440/34001 (16%)]\tLoss: 0.252614\n",
      "Train Epoch: 0 [5760/34001 (17%)]\tLoss: 0.551583\n",
      "Train Epoch: 0 [6080/34001 (18%)]\tLoss: 0.324030\n",
      "Train Epoch: 0 [6400/34001 (19%)]\tLoss: 0.395675\n",
      "Train Epoch: 0 [6720/34001 (20%)]\tLoss: 0.230833\n",
      "Train Epoch: 0 [7040/34001 (21%)]\tLoss: 0.574392\n",
      "Train Epoch: 0 [7360/34001 (22%)]\tLoss: 0.284560\n",
      "Train Epoch: 0 [7680/34001 (23%)]\tLoss: 0.284071\n",
      "Train Epoch: 0 [8000/34001 (24%)]\tLoss: 0.253775\n",
      "Train Epoch: 0 [8320/34001 (24%)]\tLoss: 0.388162\n",
      "Train Epoch: 0 [8640/34001 (25%)]\tLoss: 0.411223\n",
      "Train Epoch: 0 [8960/34001 (26%)]\tLoss: 0.233681\n",
      "Train Epoch: 0 [9280/34001 (27%)]\tLoss: 0.443958\n",
      "Train Epoch: 0 [9600/34001 (28%)]\tLoss: 0.373044\n",
      "Train Epoch: 0 [9920/34001 (29%)]\tLoss: 0.293662\n",
      "Train Epoch: 0 [10240/34001 (30%)]\tLoss: 0.293086\n",
      "Train Epoch: 0 [10560/34001 (31%)]\tLoss: 0.338654\n",
      "Train Epoch: 0 [10880/34001 (32%)]\tLoss: 0.268076\n",
      "Train Epoch: 0 [11200/34001 (33%)]\tLoss: 0.390805\n",
      "Train Epoch: 0 [11520/34001 (34%)]\tLoss: 0.238074\n",
      "Train Epoch: 0 [11840/34001 (35%)]\tLoss: 0.177883\n",
      "Train Epoch: 0 [12160/34001 (36%)]\tLoss: 0.278496\n",
      "Train Epoch: 0 [12480/34001 (37%)]\tLoss: 0.177911\n",
      "Train Epoch: 0 [12800/34001 (38%)]\tLoss: 0.170106\n",
      "Train Epoch: 0 [13120/34001 (39%)]\tLoss: 0.209202\n",
      "Train Epoch: 0 [13440/34001 (40%)]\tLoss: 0.387834\n",
      "Train Epoch: 0 [13760/34001 (40%)]\tLoss: 0.339698\n",
      "Train Epoch: 0 [14080/34001 (41%)]\tLoss: 0.147309\n",
      "Train Epoch: 0 [14400/34001 (42%)]\tLoss: 0.132849\n",
      "Train Epoch: 0 [14720/34001 (43%)]\tLoss: 0.297145\n",
      "Train Epoch: 0 [15040/34001 (44%)]\tLoss: 0.324632\n",
      "Train Epoch: 0 [15360/34001 (45%)]\tLoss: 0.196898\n",
      "Train Epoch: 0 [15680/34001 (46%)]\tLoss: 0.219170\n",
      "Train Epoch: 0 [16000/34001 (47%)]\tLoss: 0.151093\n",
      "Train Epoch: 0 [16320/34001 (48%)]\tLoss: 0.232114\n",
      "Train Epoch: 0 [16640/34001 (49%)]\tLoss: 0.163313\n",
      "Train Epoch: 0 [16960/34001 (50%)]\tLoss: 0.159903\n",
      "Train Epoch: 0 [17280/34001 (51%)]\tLoss: 0.332040\n",
      "Train Epoch: 0 [17600/34001 (52%)]\tLoss: 0.229344\n",
      "Train Epoch: 0 [17920/34001 (53%)]\tLoss: 0.273258\n",
      "Train Epoch: 0 [18240/34001 (54%)]\tLoss: 0.134637\n",
      "Train Epoch: 0 [18560/34001 (55%)]\tLoss: 0.215999\n",
      "Train Epoch: 0 [18880/34001 (56%)]\tLoss: 0.315987\n",
      "Train Epoch: 0 [19200/34001 (56%)]\tLoss: 0.287143\n",
      "Train Epoch: 0 [19520/34001 (57%)]\tLoss: 0.221018\n",
      "Train Epoch: 0 [19840/34001 (58%)]\tLoss: 0.305876\n",
      "Train Epoch: 0 [20160/34001 (59%)]\tLoss: 0.285351\n",
      "Train Epoch: 0 [20480/34001 (60%)]\tLoss: 0.239403\n",
      "Train Epoch: 0 [20800/34001 (61%)]\tLoss: 0.304261\n",
      "Train Epoch: 0 [21120/34001 (62%)]\tLoss: 0.176717\n",
      "Train Epoch: 0 [21440/34001 (63%)]\tLoss: 0.231142\n",
      "Train Epoch: 0 [21760/34001 (64%)]\tLoss: 0.199191\n",
      "Train Epoch: 0 [22080/34001 (65%)]\tLoss: 0.223062\n",
      "Train Epoch: 0 [22400/34001 (66%)]\tLoss: 0.078386\n",
      "Train Epoch: 0 [22720/34001 (67%)]\tLoss: 0.200124\n",
      "Train Epoch: 0 [23040/34001 (68%)]\tLoss: 0.082606\n",
      "Train Epoch: 0 [23360/34001 (69%)]\tLoss: 0.177900\n",
      "Train Epoch: 0 [23680/34001 (70%)]\tLoss: 0.234204\n",
      "Train Epoch: 0 [24000/34001 (71%)]\tLoss: 0.203582\n",
      "Train Epoch: 0 [24320/34001 (71%)]\tLoss: 0.177648\n",
      "Train Epoch: 0 [24640/34001 (72%)]\tLoss: 0.180615\n",
      "Train Epoch: 0 [24960/34001 (73%)]\tLoss: 0.228599\n",
      "Train Epoch: 0 [25280/34001 (74%)]\tLoss: 0.101875\n",
      "Train Epoch: 0 [25600/34001 (75%)]\tLoss: 0.065703\n",
      "Train Epoch: 0 [25920/34001 (76%)]\tLoss: 0.141198\n",
      "Train Epoch: 0 [26240/34001 (77%)]\tLoss: 0.259046\n",
      "Train Epoch: 0 [26560/34001 (78%)]\tLoss: 0.292965\n",
      "Train Epoch: 0 [26880/34001 (79%)]\tLoss: 0.214819\n",
      "Train Epoch: 0 [27200/34001 (80%)]\tLoss: 0.109463\n",
      "Train Epoch: 0 [27520/34001 (81%)]\tLoss: 0.288674\n",
      "Train Epoch: 0 [27840/34001 (82%)]\tLoss: 0.162054\n",
      "Train Epoch: 0 [28160/34001 (83%)]\tLoss: 0.195555\n",
      "Train Epoch: 0 [28480/34001 (84%)]\tLoss: 0.311508\n",
      "Train Epoch: 0 [28800/34001 (85%)]\tLoss: 0.087729\n",
      "Train Epoch: 0 [29120/34001 (86%)]\tLoss: 0.136425\n",
      "Train Epoch: 0 [29440/34001 (87%)]\tLoss: 0.222448\n",
      "Train Epoch: 0 [29760/34001 (87%)]\tLoss: 0.177154\n",
      "Train Epoch: 0 [30080/34001 (88%)]\tLoss: 0.146224\n",
      "Train Epoch: 0 [30400/34001 (89%)]\tLoss: 0.202189\n",
      "Train Epoch: 0 [30720/34001 (90%)]\tLoss: 0.242585\n",
      "Train Epoch: 0 [31040/34001 (91%)]\tLoss: 0.338070\n",
      "Train Epoch: 0 [31360/34001 (92%)]\tLoss: 0.111297\n",
      "Train Epoch: 0 [31680/34001 (93%)]\tLoss: 0.149148\n",
      "Train Epoch: 0 [32000/34001 (94%)]\tLoss: 0.595852\n",
      "Train Epoch: 0 [32320/34001 (95%)]\tLoss: 0.217402\n",
      "Train Epoch: 0 [32640/34001 (96%)]\tLoss: 0.182560\n",
      "Train Epoch: 0 [32960/34001 (97%)]\tLoss: 0.227755\n",
      "Train Epoch: 0 [33280/34001 (98%)]\tLoss: 0.270387\n",
      "Train Epoch: 0 [33600/34001 (99%)]\tLoss: 0.154586\n",
      "Train Epoch: 0 [33920/34001 (100%)]\tLoss: 0.139074\n",
      "Train Accuracy after epoch 0: 0.8812093761948178\n",
      "Validation set: Average loss: 0.0055, Accuracy: 7876/8501 (93%)\n",
      "\n",
      "Train Epoch: 1 [0/34001 (0%)]\tLoss: 0.123200\n",
      "Train Epoch: 1 [320/34001 (1%)]\tLoss: 0.212246\n",
      "Train Epoch: 1 [640/34001 (2%)]\tLoss: 0.203174\n",
      "Train Epoch: 1 [960/34001 (3%)]\tLoss: 0.234154\n",
      "Train Epoch: 1 [1280/34001 (4%)]\tLoss: 0.218377\n",
      "Train Epoch: 1 [1600/34001 (5%)]\tLoss: 0.348167\n",
      "Train Epoch: 1 [1920/34001 (6%)]\tLoss: 0.175651\n",
      "Train Epoch: 1 [2240/34001 (7%)]\tLoss: 0.187783\n",
      "Train Epoch: 1 [2560/34001 (8%)]\tLoss: 0.224352\n",
      "Train Epoch: 1 [2880/34001 (8%)]\tLoss: 0.082956\n",
      "Train Epoch: 1 [3200/34001 (9%)]\tLoss: 0.111707\n",
      "Train Epoch: 1 [3520/34001 (10%)]\tLoss: 0.126173\n",
      "Train Epoch: 1 [3840/34001 (11%)]\tLoss: 0.268040\n",
      "Train Epoch: 1 [4160/34001 (12%)]\tLoss: 0.281160\n",
      "Train Epoch: 1 [4480/34001 (13%)]\tLoss: 0.119502\n",
      "Train Epoch: 1 [4800/34001 (14%)]\tLoss: 0.084336\n",
      "Train Epoch: 1 [5120/34001 (15%)]\tLoss: 0.111380\n",
      "Train Epoch: 1 [5440/34001 (16%)]\tLoss: 0.087182\n",
      "Train Epoch: 1 [5760/34001 (17%)]\tLoss: 0.147860\n",
      "Train Epoch: 1 [6080/34001 (18%)]\tLoss: 0.127067\n",
      "Train Epoch: 1 [6400/34001 (19%)]\tLoss: 0.183027\n",
      "Train Epoch: 1 [6720/34001 (20%)]\tLoss: 0.104696\n",
      "Train Epoch: 1 [7040/34001 (21%)]\tLoss: 0.265357\n",
      "Train Epoch: 1 [7360/34001 (22%)]\tLoss: 0.108228\n",
      "Train Epoch: 1 [7680/34001 (23%)]\tLoss: 0.148836\n",
      "Train Epoch: 1 [8000/34001 (24%)]\tLoss: 0.148675\n",
      "Train Epoch: 1 [8320/34001 (24%)]\tLoss: 0.533818\n",
      "Train Epoch: 1 [8640/34001 (25%)]\tLoss: 0.389227\n",
      "Train Epoch: 1 [8960/34001 (26%)]\tLoss: 0.140260\n",
      "Train Epoch: 1 [9280/34001 (27%)]\tLoss: 0.231597\n",
      "Train Epoch: 1 [9600/34001 (28%)]\tLoss: 0.204059\n",
      "Train Epoch: 1 [9920/34001 (29%)]\tLoss: 0.123268\n",
      "Train Epoch: 1 [10240/34001 (30%)]\tLoss: 0.113951\n",
      "Train Epoch: 1 [10560/34001 (31%)]\tLoss: 0.271298\n",
      "Train Epoch: 1 [10880/34001 (32%)]\tLoss: 0.171304\n",
      "Train Epoch: 1 [11200/34001 (33%)]\tLoss: 0.285476\n",
      "Train Epoch: 1 [11520/34001 (34%)]\tLoss: 0.105613\n",
      "Train Epoch: 1 [11840/34001 (35%)]\tLoss: 0.092656\n",
      "Train Epoch: 1 [12160/34001 (36%)]\tLoss: 0.195734\n",
      "Train Epoch: 1 [12480/34001 (37%)]\tLoss: 0.089870\n",
      "Train Epoch: 1 [12800/34001 (38%)]\tLoss: 0.114988\n",
      "Train Epoch: 1 [13120/34001 (39%)]\tLoss: 0.090756\n",
      "Train Epoch: 1 [13440/34001 (40%)]\tLoss: 0.307312\n",
      "Train Epoch: 1 [13760/34001 (40%)]\tLoss: 0.158330\n",
      "Train Epoch: 1 [14080/34001 (41%)]\tLoss: 0.082496\n",
      "Train Epoch: 1 [14400/34001 (42%)]\tLoss: 0.089494\n",
      "Train Epoch: 1 [14720/34001 (43%)]\tLoss: 0.173219\n",
      "Train Epoch: 1 [15040/34001 (44%)]\tLoss: 0.321225\n",
      "Train Epoch: 1 [15360/34001 (45%)]\tLoss: 0.142439\n",
      "Train Epoch: 1 [15680/34001 (46%)]\tLoss: 0.115577\n",
      "Train Epoch: 1 [16000/34001 (47%)]\tLoss: 0.069865\n",
      "Train Epoch: 1 [16320/34001 (48%)]\tLoss: 0.117727\n",
      "Train Epoch: 1 [16640/34001 (49%)]\tLoss: 0.153351\n",
      "Train Epoch: 1 [16960/34001 (50%)]\tLoss: 0.119847\n",
      "Train Epoch: 1 [17280/34001 (51%)]\tLoss: 0.250757\n",
      "Train Epoch: 1 [17600/34001 (52%)]\tLoss: 0.144144\n",
      "Train Epoch: 1 [17920/34001 (53%)]\tLoss: 0.206086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [18240/34001 (54%)]\tLoss: 0.089821\n",
      "Train Epoch: 1 [18560/34001 (55%)]\tLoss: 0.263206\n",
      "Train Epoch: 1 [18880/34001 (56%)]\tLoss: 0.173155\n",
      "Train Epoch: 1 [19200/34001 (56%)]\tLoss: 0.177310\n",
      "Train Epoch: 1 [19520/34001 (57%)]\tLoss: 0.110793\n",
      "Train Epoch: 1 [19840/34001 (58%)]\tLoss: 0.208444\n",
      "Train Epoch: 1 [20160/34001 (59%)]\tLoss: 0.151386\n",
      "Train Epoch: 1 [20480/34001 (60%)]\tLoss: 0.182702\n",
      "Train Epoch: 1 [20800/34001 (61%)]\tLoss: 0.204991\n",
      "Train Epoch: 1 [21120/34001 (62%)]\tLoss: 0.112058\n",
      "Train Epoch: 1 [21440/34001 (63%)]\tLoss: 0.190593\n",
      "Train Epoch: 1 [21760/34001 (64%)]\tLoss: 0.178210\n",
      "Train Epoch: 1 [22080/34001 (65%)]\tLoss: 0.199873\n",
      "Train Epoch: 1 [22400/34001 (66%)]\tLoss: 0.092876\n",
      "Train Epoch: 1 [22720/34001 (67%)]\tLoss: 0.156246\n",
      "Train Epoch: 1 [23040/34001 (68%)]\tLoss: 0.048956\n",
      "Train Epoch: 1 [23360/34001 (69%)]\tLoss: 0.209048\n",
      "Train Epoch: 1 [23680/34001 (70%)]\tLoss: 0.189999\n",
      "Train Epoch: 1 [24000/34001 (71%)]\tLoss: 0.189622\n",
      "Train Epoch: 1 [24320/34001 (71%)]\tLoss: 0.136207\n",
      "Train Epoch: 1 [24640/34001 (72%)]\tLoss: 0.086653\n",
      "Train Epoch: 1 [24960/34001 (73%)]\tLoss: 0.235198\n",
      "Train Epoch: 1 [25280/34001 (74%)]\tLoss: 0.035523\n",
      "Train Epoch: 1 [25600/34001 (75%)]\tLoss: 0.037140\n",
      "Train Epoch: 1 [25920/34001 (76%)]\tLoss: 0.122159\n",
      "Train Epoch: 1 [26240/34001 (77%)]\tLoss: 0.159948\n",
      "Train Epoch: 1 [26560/34001 (78%)]\tLoss: 0.181066\n",
      "Train Epoch: 1 [26880/34001 (79%)]\tLoss: 0.126378\n",
      "Train Epoch: 1 [27200/34001 (80%)]\tLoss: 0.092332\n",
      "Train Epoch: 1 [27520/34001 (81%)]\tLoss: 0.273406\n",
      "Train Epoch: 1 [27840/34001 (82%)]\tLoss: 0.102373\n",
      "Train Epoch: 1 [28160/34001 (83%)]\tLoss: 0.114084\n",
      "Train Epoch: 1 [28480/34001 (84%)]\tLoss: 0.231904\n",
      "Train Epoch: 1 [28800/34001 (85%)]\tLoss: 0.078785\n",
      "Train Epoch: 1 [29120/34001 (86%)]\tLoss: 0.083058\n",
      "Train Epoch: 1 [29440/34001 (87%)]\tLoss: 0.147835\n",
      "Train Epoch: 1 [29760/34001 (87%)]\tLoss: 0.132634\n",
      "Train Epoch: 1 [30080/34001 (88%)]\tLoss: 0.104840\n",
      "Train Epoch: 1 [30400/34001 (89%)]\tLoss: 0.227267\n",
      "Train Epoch: 1 [30720/34001 (90%)]\tLoss: 0.224882\n",
      "Train Epoch: 1 [31040/34001 (91%)]\tLoss: 0.219212\n",
      "Train Epoch: 1 [31360/34001 (92%)]\tLoss: 0.075290\n",
      "Train Epoch: 1 [31680/34001 (93%)]\tLoss: 0.177084\n",
      "Train Epoch: 1 [32000/34001 (94%)]\tLoss: 0.539300\n",
      "Train Epoch: 1 [32320/34001 (95%)]\tLoss: 0.205268\n",
      "Train Epoch: 1 [32640/34001 (96%)]\tLoss: 0.166439\n",
      "Train Epoch: 1 [32960/34001 (97%)]\tLoss: 0.226868\n",
      "Train Epoch: 1 [33280/34001 (98%)]\tLoss: 0.259568\n",
      "Train Epoch: 1 [33600/34001 (99%)]\tLoss: 0.082899\n",
      "Train Epoch: 1 [33920/34001 (100%)]\tLoss: 0.128988\n",
      "Train Accuracy after epoch 1: 0.9270609687950354\n",
      "Validation set: Average loss: 0.0050, Accuracy: 7922/8501 (93%)\n",
      "\n",
      "Saving model. Best validation accuracy: 0.9318903658393131\n",
      "Train Epoch: 2 [0/34001 (0%)]\tLoss: 0.083257\n",
      "Train Epoch: 2 [320/34001 (1%)]\tLoss: 0.172829\n",
      "Train Epoch: 2 [640/34001 (2%)]\tLoss: 0.168541\n",
      "Train Epoch: 2 [960/34001 (3%)]\tLoss: 0.222793\n",
      "Train Epoch: 2 [1280/34001 (4%)]\tLoss: 0.115904\n",
      "Train Epoch: 2 [1600/34001 (5%)]\tLoss: 0.310644\n",
      "Train Epoch: 2 [1920/34001 (6%)]\tLoss: 0.175284\n",
      "Train Epoch: 2 [2240/34001 (7%)]\tLoss: 0.125725\n",
      "Train Epoch: 2 [2560/34001 (8%)]\tLoss: 0.140584\n",
      "Train Epoch: 2 [2880/34001 (8%)]\tLoss: 0.050489\n",
      "Train Epoch: 2 [3200/34001 (9%)]\tLoss: 0.061153\n",
      "Train Epoch: 2 [3520/34001 (10%)]\tLoss: 0.112686\n",
      "Train Epoch: 2 [3840/34001 (11%)]\tLoss: 0.208892\n",
      "Train Epoch: 2 [4160/34001 (12%)]\tLoss: 0.297415\n",
      "Train Epoch: 2 [4480/34001 (13%)]\tLoss: 0.144616\n",
      "Train Epoch: 2 [4800/34001 (14%)]\tLoss: 0.047841\n",
      "Train Epoch: 2 [5120/34001 (15%)]\tLoss: 0.086825\n",
      "Train Epoch: 2 [5440/34001 (16%)]\tLoss: 0.069174\n",
      "Train Epoch: 2 [5760/34001 (17%)]\tLoss: 0.108551\n",
      "Train Epoch: 2 [6080/34001 (18%)]\tLoss: 0.064265\n",
      "Train Epoch: 2 [6400/34001 (19%)]\tLoss: 0.152912\n",
      "Train Epoch: 2 [6720/34001 (20%)]\tLoss: 0.074458\n",
      "Train Epoch: 2 [7040/34001 (21%)]\tLoss: 0.278906\n",
      "Train Epoch: 2 [7360/34001 (22%)]\tLoss: 0.102959\n",
      "Train Epoch: 2 [7680/34001 (23%)]\tLoss: 0.117221\n",
      "Train Epoch: 2 [8000/34001 (24%)]\tLoss: 0.204594\n",
      "Train Epoch: 2 [8320/34001 (24%)]\tLoss: 0.482261\n",
      "Train Epoch: 2 [8640/34001 (25%)]\tLoss: 0.400134\n",
      "Train Epoch: 2 [8960/34001 (26%)]\tLoss: 0.067060\n",
      "Train Epoch: 2 [9280/34001 (27%)]\tLoss: 0.167961\n",
      "Train Epoch: 2 [9600/34001 (28%)]\tLoss: 0.208141\n",
      "Train Epoch: 2 [9920/34001 (29%)]\tLoss: 0.107713\n",
      "Train Epoch: 2 [10240/34001 (30%)]\tLoss: 0.090801\n",
      "Train Epoch: 2 [10560/34001 (31%)]\tLoss: 0.204927\n",
      "Train Epoch: 2 [10880/34001 (32%)]\tLoss: 0.169362\n",
      "Train Epoch: 2 [11200/34001 (33%)]\tLoss: 0.242930\n",
      "Train Epoch: 2 [11520/34001 (34%)]\tLoss: 0.095928\n",
      "Train Epoch: 2 [11840/34001 (35%)]\tLoss: 0.061631\n",
      "Train Epoch: 2 [12160/34001 (36%)]\tLoss: 0.179792\n",
      "Train Epoch: 2 [12480/34001 (37%)]\tLoss: 0.090783\n",
      "Train Epoch: 2 [12800/34001 (38%)]\tLoss: 0.105007\n",
      "Train Epoch: 2 [13120/34001 (39%)]\tLoss: 0.082031\n",
      "Train Epoch: 2 [13440/34001 (40%)]\tLoss: 0.283918\n",
      "Train Epoch: 2 [13760/34001 (40%)]\tLoss: 0.148169\n",
      "Train Epoch: 2 [14080/34001 (41%)]\tLoss: 0.064949\n",
      "Train Epoch: 2 [14400/34001 (42%)]\tLoss: 0.067342\n",
      "Train Epoch: 2 [14720/34001 (43%)]\tLoss: 0.133272\n",
      "Train Epoch: 2 [15040/34001 (44%)]\tLoss: 0.299299\n",
      "Train Epoch: 2 [15360/34001 (45%)]\tLoss: 0.145011\n",
      "Train Epoch: 2 [15680/34001 (46%)]\tLoss: 0.080609\n",
      "Train Epoch: 2 [16000/34001 (47%)]\tLoss: 0.062009\n",
      "Train Epoch: 2 [16320/34001 (48%)]\tLoss: 0.116953\n",
      "Train Epoch: 2 [16640/34001 (49%)]\tLoss: 0.148404\n",
      "Train Epoch: 2 [16960/34001 (50%)]\tLoss: 0.104812\n",
      "Train Epoch: 2 [17280/34001 (51%)]\tLoss: 0.186487\n",
      "Train Epoch: 2 [17600/34001 (52%)]\tLoss: 0.125257\n",
      "Train Epoch: 2 [17920/34001 (53%)]\tLoss: 0.186012\n",
      "Train Epoch: 2 [18240/34001 (54%)]\tLoss: 0.084151\n",
      "Train Epoch: 2 [18560/34001 (55%)]\tLoss: 0.284784\n",
      "Train Epoch: 2 [18880/34001 (56%)]\tLoss: 0.162497\n",
      "Train Epoch: 2 [19200/34001 (56%)]\tLoss: 0.128276\n",
      "Train Epoch: 2 [19520/34001 (57%)]\tLoss: 0.125958\n",
      "Train Epoch: 2 [19840/34001 (58%)]\tLoss: 0.158712\n",
      "Train Epoch: 2 [20160/34001 (59%)]\tLoss: 0.135997\n",
      "Train Epoch: 2 [20480/34001 (60%)]\tLoss: 0.158642\n",
      "Train Epoch: 2 [20800/34001 (61%)]\tLoss: 0.118949\n",
      "Train Epoch: 2 [21120/34001 (62%)]\tLoss: 0.073244\n",
      "Train Epoch: 2 [21440/34001 (63%)]\tLoss: 0.126023\n",
      "Train Epoch: 2 [21760/34001 (64%)]\tLoss: 0.151658\n",
      "Train Epoch: 2 [22080/34001 (65%)]\tLoss: 0.201750\n",
      "Train Epoch: 2 [22400/34001 (66%)]\tLoss: 0.048365\n",
      "Train Epoch: 2 [22720/34001 (67%)]\tLoss: 0.138163\n",
      "Train Epoch: 2 [23040/34001 (68%)]\tLoss: 0.045301\n",
      "Train Epoch: 2 [23360/34001 (69%)]\tLoss: 0.157572\n",
      "Train Epoch: 2 [23680/34001 (70%)]\tLoss: 0.175261\n",
      "Train Epoch: 2 [24000/34001 (71%)]\tLoss: 0.201068\n",
      "Train Epoch: 2 [24320/34001 (71%)]\tLoss: 0.115917\n",
      "Train Epoch: 2 [24640/34001 (72%)]\tLoss: 0.099052\n",
      "Train Epoch: 2 [24960/34001 (73%)]\tLoss: 0.199149\n",
      "Train Epoch: 2 [25280/34001 (74%)]\tLoss: 0.038601\n",
      "Train Epoch: 2 [25600/34001 (75%)]\tLoss: 0.029448\n",
      "Train Epoch: 2 [25920/34001 (76%)]\tLoss: 0.132196\n",
      "Train Epoch: 2 [26240/34001 (77%)]\tLoss: 0.196841\n",
      "Train Epoch: 2 [26560/34001 (78%)]\tLoss: 0.168262\n",
      "Train Epoch: 2 [26880/34001 (79%)]\tLoss: 0.089090\n",
      "Train Epoch: 2 [27200/34001 (80%)]\tLoss: 0.135341\n",
      "Train Epoch: 2 [27520/34001 (81%)]\tLoss: 0.255766\n",
      "Train Epoch: 2 [27840/34001 (82%)]\tLoss: 0.118311\n",
      "Train Epoch: 2 [28160/34001 (83%)]\tLoss: 0.153334\n",
      "Train Epoch: 2 [28480/34001 (84%)]\tLoss: 0.174457\n",
      "Train Epoch: 2 [28800/34001 (85%)]\tLoss: 0.079651\n",
      "Train Epoch: 2 [29120/34001 (86%)]\tLoss: 0.089839\n",
      "Train Epoch: 2 [29440/34001 (87%)]\tLoss: 0.150078\n",
      "Train Epoch: 2 [29760/34001 (87%)]\tLoss: 0.151624\n",
      "Train Epoch: 2 [30080/34001 (88%)]\tLoss: 0.086226\n",
      "Train Epoch: 2 [30400/34001 (89%)]\tLoss: 0.125234\n",
      "Train Epoch: 2 [30720/34001 (90%)]\tLoss: 0.216231\n",
      "Train Epoch: 2 [31040/34001 (91%)]\tLoss: 0.239483\n",
      "Train Epoch: 2 [31360/34001 (92%)]\tLoss: 0.040838\n",
      "Train Epoch: 2 [31680/34001 (93%)]\tLoss: 0.130786\n",
      "Train Epoch: 2 [32000/34001 (94%)]\tLoss: 0.490928\n",
      "Train Epoch: 2 [32320/34001 (95%)]\tLoss: 0.176236\n",
      "Train Epoch: 2 [32640/34001 (96%)]\tLoss: 0.164950\n",
      "Train Epoch: 2 [32960/34001 (97%)]\tLoss: 0.200911\n",
      "Train Epoch: 2 [33280/34001 (98%)]\tLoss: 0.207714\n",
      "Train Epoch: 2 [33600/34001 (99%)]\tLoss: 0.059703\n",
      "Train Epoch: 2 [33920/34001 (100%)]\tLoss: 0.113791\n",
      "Train Accuracy after epoch 2: 0.9362077585953354\n",
      "Validation set: Average loss: 0.0048, Accuracy: 7948/8501 (93%)\n",
      "\n",
      "Saving model. Best validation accuracy: 0.9349488295494648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-29 12:06:48,977]\u001b[0m Trial 2 finished with value: 0.9349488295494648 and parameters: {'lr': 3.575358516942407e-06, 'optimizer_name': 'AdamW', 'epochs': 3, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.8, 'scheduler': 'exponential'}. Best is trial 2 with value: 0.9349488295494648.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial params: {'lr': 1.5698549009697361e-06, 'optimizer_name': 'Adam', 'epochs': 1, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.6, 'scheduler': 'step'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/34001 (0%)]\tLoss: 0.682451\n",
      "Train Epoch: 0 [320/34001 (1%)]\tLoss: 0.678284\n",
      "Train Epoch: 0 [640/34001 (2%)]\tLoss: 0.658758\n",
      "Train Epoch: 0 [960/34001 (3%)]\tLoss: 0.663238\n",
      "Train Epoch: 0 [1280/34001 (4%)]\tLoss: 0.606549\n",
      "Train Epoch: 0 [1600/34001 (5%)]\tLoss: 0.570314\n",
      "Train Epoch: 0 [1920/34001 (6%)]\tLoss: 0.619805\n",
      "Train Epoch: 0 [2240/34001 (7%)]\tLoss: 0.557086\n",
      "Train Epoch: 0 [2560/34001 (8%)]\tLoss: 0.491341\n",
      "Train Epoch: 0 [2880/34001 (8%)]\tLoss: 0.600100\n",
      "Train Epoch: 0 [3200/34001 (9%)]\tLoss: 0.531749\n",
      "Train Epoch: 0 [3520/34001 (10%)]\tLoss: 0.457393\n",
      "Train Epoch: 0 [3840/34001 (11%)]\tLoss: 0.407130\n",
      "Train Epoch: 0 [4160/34001 (12%)]\tLoss: 0.398895\n",
      "Train Epoch: 0 [4480/34001 (13%)]\tLoss: 0.372894\n",
      "Train Epoch: 0 [4800/34001 (14%)]\tLoss: 0.516021\n",
      "Train Epoch: 0 [5120/34001 (15%)]\tLoss: 0.357515\n",
      "Train Epoch: 0 [5440/34001 (16%)]\tLoss: 0.344828\n",
      "Train Epoch: 0 [5760/34001 (17%)]\tLoss: 0.325024\n",
      "Train Epoch: 0 [6080/34001 (18%)]\tLoss: 0.516431\n",
      "Train Epoch: 0 [6400/34001 (19%)]\tLoss: 0.513122\n",
      "Train Epoch: 0 [6720/34001 (20%)]\tLoss: 0.381542\n",
      "Train Epoch: 0 [7040/34001 (21%)]\tLoss: 0.383661\n",
      "Train Epoch: 0 [7360/34001 (22%)]\tLoss: 0.323702\n",
      "Train Epoch: 0 [7680/34001 (23%)]\tLoss: 0.558877\n",
      "Train Epoch: 0 [8000/34001 (24%)]\tLoss: 0.455884\n",
      "Train Epoch: 0 [8320/34001 (24%)]\tLoss: 0.320764\n",
      "Train Epoch: 0 [8640/34001 (25%)]\tLoss: 0.256205\n",
      "Train Epoch: 0 [8960/34001 (26%)]\tLoss: 0.481119\n",
      "Train Epoch: 0 [9280/34001 (27%)]\tLoss: 0.402178\n",
      "Train Epoch: 0 [9600/34001 (28%)]\tLoss: 0.414221\n",
      "Train Epoch: 0 [9920/34001 (29%)]\tLoss: 0.312549\n",
      "Train Epoch: 0 [10240/34001 (30%)]\tLoss: 0.222532\n",
      "Train Epoch: 0 [10560/34001 (31%)]\tLoss: 0.289981\n",
      "Train Epoch: 0 [10880/34001 (32%)]\tLoss: 0.349872\n",
      "Train Epoch: 0 [11200/34001 (33%)]\tLoss: 0.330735\n",
      "Train Epoch: 0 [11520/34001 (34%)]\tLoss: 0.373193\n",
      "Train Epoch: 0 [11840/34001 (35%)]\tLoss: 0.309350\n",
      "Train Epoch: 0 [12160/34001 (36%)]\tLoss: 0.257723\n",
      "Train Epoch: 0 [12480/34001 (37%)]\tLoss: 0.429231\n",
      "Train Epoch: 0 [12800/34001 (38%)]\tLoss: 0.356994\n",
      "Train Epoch: 0 [13120/34001 (39%)]\tLoss: 0.356449\n",
      "Train Epoch: 0 [13440/34001 (40%)]\tLoss: 0.369958\n",
      "Train Epoch: 0 [13760/34001 (40%)]\tLoss: 0.252778\n",
      "Train Epoch: 0 [14080/34001 (41%)]\tLoss: 0.509646\n",
      "Train Epoch: 0 [14400/34001 (42%)]\tLoss: 0.299777\n",
      "Train Epoch: 0 [14720/34001 (43%)]\tLoss: 0.388434\n",
      "Train Epoch: 0 [15040/34001 (44%)]\tLoss: 0.266417\n",
      "Train Epoch: 0 [15360/34001 (45%)]\tLoss: 0.180705\n",
      "Train Epoch: 0 [15680/34001 (46%)]\tLoss: 0.285644\n",
      "Train Epoch: 0 [16000/34001 (47%)]\tLoss: 0.120695\n",
      "Train Epoch: 0 [16320/34001 (48%)]\tLoss: 0.239111\n",
      "Train Epoch: 0 [16640/34001 (49%)]\tLoss: 0.241033\n",
      "Train Epoch: 0 [16960/34001 (50%)]\tLoss: 0.209560\n",
      "Train Epoch: 0 [17280/34001 (51%)]\tLoss: 0.283175\n",
      "Train Epoch: 0 [17600/34001 (52%)]\tLoss: 0.318796\n",
      "Train Epoch: 0 [17920/34001 (53%)]\tLoss: 0.389555\n",
      "Train Epoch: 0 [18240/34001 (54%)]\tLoss: 0.293902\n",
      "Train Epoch: 0 [18560/34001 (55%)]\tLoss: 0.227947\n",
      "Train Epoch: 0 [18880/34001 (56%)]\tLoss: 0.162937\n",
      "Train Epoch: 0 [19200/34001 (56%)]\tLoss: 0.266056\n",
      "Train Epoch: 0 [19520/34001 (57%)]\tLoss: 0.190355\n",
      "Train Epoch: 0 [19840/34001 (58%)]\tLoss: 0.251266\n",
      "Train Epoch: 0 [20160/34001 (59%)]\tLoss: 0.371860\n",
      "Train Epoch: 0 [20480/34001 (60%)]\tLoss: 0.401495\n",
      "Train Epoch: 0 [20800/34001 (61%)]\tLoss: 0.408383\n",
      "Train Epoch: 0 [21120/34001 (62%)]\tLoss: 0.338784\n",
      "Train Epoch: 0 [21440/34001 (63%)]\tLoss: 0.301606\n",
      "Train Epoch: 0 [21760/34001 (64%)]\tLoss: 0.256220\n",
      "Train Epoch: 0 [22080/34001 (65%)]\tLoss: 0.205744\n",
      "Train Epoch: 0 [22400/34001 (66%)]\tLoss: 0.254369\n",
      "Train Epoch: 0 [22720/34001 (67%)]\tLoss: 0.155237\n",
      "Train Epoch: 0 [23040/34001 (68%)]\tLoss: 0.121637\n",
      "Train Epoch: 0 [23360/34001 (69%)]\tLoss: 0.295330\n",
      "Train Epoch: 0 [23680/34001 (70%)]\tLoss: 0.317740\n",
      "Train Epoch: 0 [24000/34001 (71%)]\tLoss: 0.386579\n",
      "Train Epoch: 0 [24320/34001 (71%)]\tLoss: 0.393238\n",
      "Train Epoch: 0 [24640/34001 (72%)]\tLoss: 0.277296\n",
      "Train Epoch: 0 [24960/34001 (73%)]\tLoss: 0.275361\n",
      "Train Epoch: 0 [25280/34001 (74%)]\tLoss: 0.380239\n",
      "Train Epoch: 0 [25600/34001 (75%)]\tLoss: 0.244002\n",
      "Train Epoch: 0 [25920/34001 (76%)]\tLoss: 0.200287\n",
      "Train Epoch: 0 [26240/34001 (77%)]\tLoss: 0.192152\n",
      "Train Epoch: 0 [26560/34001 (78%)]\tLoss: 0.141685\n",
      "Train Epoch: 0 [26880/34001 (79%)]\tLoss: 0.196908\n",
      "Train Epoch: 0 [27200/34001 (80%)]\tLoss: 0.228801\n",
      "Train Epoch: 0 [27520/34001 (81%)]\tLoss: 0.206613\n",
      "Train Epoch: 0 [27840/34001 (82%)]\tLoss: 0.287708\n",
      "Train Epoch: 0 [28160/34001 (83%)]\tLoss: 0.248286\n",
      "Train Epoch: 0 [28480/34001 (84%)]\tLoss: 0.162812\n",
      "Train Epoch: 0 [28800/34001 (85%)]\tLoss: 0.260465\n",
      "Train Epoch: 0 [29120/34001 (86%)]\tLoss: 0.466458\n",
      "Train Epoch: 0 [29440/34001 (87%)]\tLoss: 0.256532\n",
      "Train Epoch: 0 [29760/34001 (87%)]\tLoss: 0.493783\n",
      "Train Epoch: 0 [30080/34001 (88%)]\tLoss: 0.182202\n",
      "Train Epoch: 0 [30400/34001 (89%)]\tLoss: 0.274333\n",
      "Train Epoch: 0 [30720/34001 (90%)]\tLoss: 0.219317\n",
      "Train Epoch: 0 [31040/34001 (91%)]\tLoss: 0.274135\n",
      "Train Epoch: 0 [31360/34001 (92%)]\tLoss: 0.326616\n",
      "Train Epoch: 0 [31680/34001 (93%)]\tLoss: 0.142823\n",
      "Train Epoch: 0 [32000/34001 (94%)]\tLoss: 0.276974\n",
      "Train Epoch: 0 [32320/34001 (95%)]\tLoss: 0.194912\n",
      "Train Epoch: 0 [32640/34001 (96%)]\tLoss: 0.218681\n",
      "Train Epoch: 0 [32960/34001 (97%)]\tLoss: 0.358349\n",
      "Train Epoch: 0 [33280/34001 (98%)]\tLoss: 0.151049\n",
      "Train Epoch: 0 [33600/34001 (99%)]\tLoss: 0.481523\n",
      "Train Epoch: 0 [33920/34001 (100%)]\tLoss: 0.403702\n",
      "Train Accuracy after epoch 0: 0.8570924384576925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-29 12:16:31,249]\u001b[0m Trial 3 finished with value: 0.8971885660510528 and parameters: {'lr': 1.5698549009697361e-06, 'optimizer_name': 'Adam', 'epochs': 1, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.6, 'scheduler': 'step'}. Best is trial 2 with value: 0.9349488295494648.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Average loss: 0.0075, Accuracy: 7627/8501 (90%)\n",
      "\n",
      "Trial params: {'lr': 4.580560474043079e-06, 'optimizer_name': 'AdamW', 'epochs': 2, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.6, 'scheduler': 'exponential'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/34001 (0%)]\tLoss: 0.683963\n",
      "Train Epoch: 0 [320/34001 (1%)]\tLoss: 0.699786\n",
      "Train Epoch: 0 [640/34001 (2%)]\tLoss: 0.615513\n",
      "Train Epoch: 0 [960/34001 (3%)]\tLoss: 0.615206\n",
      "Train Epoch: 0 [1280/34001 (4%)]\tLoss: 0.356196\n",
      "Train Epoch: 0 [1600/34001 (5%)]\tLoss: 0.489940\n",
      "Train Epoch: 0 [1920/34001 (6%)]\tLoss: 0.361635\n",
      "Train Epoch: 0 [2240/34001 (7%)]\tLoss: 0.537275\n",
      "Train Epoch: 0 [2560/34001 (8%)]\tLoss: 0.404431\n",
      "Train Epoch: 0 [2880/34001 (8%)]\tLoss: 0.392421\n",
      "Train Epoch: 0 [3200/34001 (9%)]\tLoss: 0.264121\n",
      "Train Epoch: 0 [3520/34001 (10%)]\tLoss: 0.272554\n",
      "Train Epoch: 0 [3840/34001 (11%)]\tLoss: 0.272825\n",
      "Train Epoch: 0 [4160/34001 (12%)]\tLoss: 0.373135\n",
      "Train Epoch: 0 [4480/34001 (13%)]\tLoss: 0.385819\n",
      "Train Epoch: 0 [4800/34001 (14%)]\tLoss: 0.388671\n",
      "Train Epoch: 0 [5120/34001 (15%)]\tLoss: 0.358752\n",
      "Train Epoch: 0 [5440/34001 (16%)]\tLoss: 0.360910\n",
      "Train Epoch: 0 [5760/34001 (17%)]\tLoss: 0.402596\n",
      "Train Epoch: 0 [6080/34001 (18%)]\tLoss: 0.474710\n",
      "Train Epoch: 0 [6400/34001 (19%)]\tLoss: 0.363555\n",
      "Train Epoch: 0 [6720/34001 (20%)]\tLoss: 0.267143\n",
      "Train Epoch: 0 [7040/34001 (21%)]\tLoss: 0.264797\n",
      "Train Epoch: 0 [7360/34001 (22%)]\tLoss: 0.225920\n",
      "Train Epoch: 0 [7680/34001 (23%)]\tLoss: 0.215903\n",
      "Train Epoch: 0 [8000/34001 (24%)]\tLoss: 0.314271\n",
      "Train Epoch: 0 [8320/34001 (24%)]\tLoss: 0.251783\n",
      "Train Epoch: 0 [8640/34001 (25%)]\tLoss: 0.210098\n",
      "Train Epoch: 0 [8960/34001 (26%)]\tLoss: 0.209666\n",
      "Train Epoch: 0 [9280/34001 (27%)]\tLoss: 0.234358\n",
      "Train Epoch: 0 [9600/34001 (28%)]\tLoss: 0.203649\n",
      "Train Epoch: 0 [9920/34001 (29%)]\tLoss: 0.323268\n",
      "Train Epoch: 0 [10240/34001 (30%)]\tLoss: 0.213656\n",
      "Train Epoch: 0 [10560/34001 (31%)]\tLoss: 0.253727\n",
      "Train Epoch: 0 [10880/34001 (32%)]\tLoss: 0.220843\n",
      "Train Epoch: 0 [11200/34001 (33%)]\tLoss: 0.059774\n",
      "Train Epoch: 0 [11520/34001 (34%)]\tLoss: 0.141967\n",
      "Train Epoch: 0 [11840/34001 (35%)]\tLoss: 0.417833\n",
      "Train Epoch: 0 [12160/34001 (36%)]\tLoss: 0.233966\n",
      "Train Epoch: 0 [12480/34001 (37%)]\tLoss: 0.142834\n",
      "Train Epoch: 0 [12800/34001 (38%)]\tLoss: 0.071079\n",
      "Train Epoch: 0 [13120/34001 (39%)]\tLoss: 0.138656\n",
      "Train Epoch: 0 [13440/34001 (40%)]\tLoss: 0.202373\n",
      "Train Epoch: 0 [13760/34001 (40%)]\tLoss: 0.117826\n",
      "Train Epoch: 0 [14080/34001 (41%)]\tLoss: 0.135721\n",
      "Train Epoch: 0 [14400/34001 (42%)]\tLoss: 0.248521\n",
      "Train Epoch: 0 [14720/34001 (43%)]\tLoss: 0.110786\n",
      "Train Epoch: 0 [15040/34001 (44%)]\tLoss: 0.465166\n",
      "Train Epoch: 0 [15360/34001 (45%)]\tLoss: 0.106585\n",
      "Train Epoch: 0 [15680/34001 (46%)]\tLoss: 0.211827\n",
      "Train Epoch: 0 [16000/34001 (47%)]\tLoss: 0.186456\n",
      "Train Epoch: 0 [16320/34001 (48%)]\tLoss: 0.132147\n",
      "Train Epoch: 0 [16640/34001 (49%)]\tLoss: 0.367502\n",
      "Train Epoch: 0 [16960/34001 (50%)]\tLoss: 0.273231\n",
      "Train Epoch: 0 [17280/34001 (51%)]\tLoss: 0.152745\n",
      "Train Epoch: 0 [17600/34001 (52%)]\tLoss: 0.283021\n",
      "Train Epoch: 0 [17920/34001 (53%)]\tLoss: 0.416082\n",
      "Train Epoch: 0 [18240/34001 (54%)]\tLoss: 0.176720\n",
      "Train Epoch: 0 [18560/34001 (55%)]\tLoss: 0.152712\n",
      "Train Epoch: 0 [18880/34001 (56%)]\tLoss: 0.121669\n",
      "Train Epoch: 0 [19200/34001 (56%)]\tLoss: 0.307471\n",
      "Train Epoch: 0 [19520/34001 (57%)]\tLoss: 0.352998\n",
      "Train Epoch: 0 [19840/34001 (58%)]\tLoss: 0.062154\n",
      "Train Epoch: 0 [20160/34001 (59%)]\tLoss: 0.079716\n",
      "Train Epoch: 0 [20480/34001 (60%)]\tLoss: 0.219407\n",
      "Train Epoch: 0 [20800/34001 (61%)]\tLoss: 0.166455\n",
      "Train Epoch: 0 [21120/34001 (62%)]\tLoss: 0.264901\n",
      "Train Epoch: 0 [21440/34001 (63%)]\tLoss: 0.092958\n",
      "Train Epoch: 0 [21760/34001 (64%)]\tLoss: 0.197663\n",
      "Train Epoch: 0 [22080/34001 (65%)]\tLoss: 0.205439\n",
      "Train Epoch: 0 [22400/34001 (66%)]\tLoss: 0.104901\n",
      "Train Epoch: 0 [22720/34001 (67%)]\tLoss: 0.299514\n",
      "Train Epoch: 0 [23040/34001 (68%)]\tLoss: 0.222743\n",
      "Train Epoch: 0 [23360/34001 (69%)]\tLoss: 0.232685\n",
      "Train Epoch: 0 [23680/34001 (70%)]\tLoss: 0.175800\n",
      "Train Epoch: 0 [24000/34001 (71%)]\tLoss: 0.180559\n",
      "Train Epoch: 0 [24320/34001 (71%)]\tLoss: 0.142904\n",
      "Train Epoch: 0 [24640/34001 (72%)]\tLoss: 0.296622\n",
      "Train Epoch: 0 [24960/34001 (73%)]\tLoss: 0.077672\n",
      "Train Epoch: 0 [25280/34001 (74%)]\tLoss: 0.334885\n",
      "Train Epoch: 0 [25600/34001 (75%)]\tLoss: 0.170877\n",
      "Train Epoch: 0 [25920/34001 (76%)]\tLoss: 0.157693\n",
      "Train Epoch: 0 [26240/34001 (77%)]\tLoss: 0.086186\n",
      "Train Epoch: 0 [26560/34001 (78%)]\tLoss: 0.287379\n",
      "Train Epoch: 0 [26880/34001 (79%)]\tLoss: 0.102154\n",
      "Train Epoch: 0 [27200/34001 (80%)]\tLoss: 0.362475\n",
      "Train Epoch: 0 [27520/34001 (81%)]\tLoss: 0.194742\n",
      "Train Epoch: 0 [27840/34001 (82%)]\tLoss: 0.162419\n",
      "Train Epoch: 0 [28160/34001 (83%)]\tLoss: 0.170574\n",
      "Train Epoch: 0 [28480/34001 (84%)]\tLoss: 0.185069\n",
      "Train Epoch: 0 [28800/34001 (85%)]\tLoss: 0.362038\n",
      "Train Epoch: 0 [29120/34001 (86%)]\tLoss: 0.308128\n",
      "Train Epoch: 0 [29440/34001 (87%)]\tLoss: 0.193831\n",
      "Train Epoch: 0 [29760/34001 (87%)]\tLoss: 0.328513\n",
      "Train Epoch: 0 [30080/34001 (88%)]\tLoss: 0.100403\n",
      "Train Epoch: 0 [30400/34001 (89%)]\tLoss: 0.101858\n",
      "Train Epoch: 0 [30720/34001 (90%)]\tLoss: 0.139306\n",
      "Train Epoch: 0 [31040/34001 (91%)]\tLoss: 0.262083\n",
      "Train Epoch: 0 [31360/34001 (92%)]\tLoss: 0.270417\n",
      "Train Epoch: 0 [31680/34001 (93%)]\tLoss: 0.215071\n",
      "Train Epoch: 0 [32000/34001 (94%)]\tLoss: 0.255210\n",
      "Train Epoch: 0 [32320/34001 (95%)]\tLoss: 0.239676\n",
      "Train Epoch: 0 [32640/34001 (96%)]\tLoss: 0.097480\n",
      "Train Epoch: 0 [32960/34001 (97%)]\tLoss: 0.161011\n",
      "Train Epoch: 0 [33280/34001 (98%)]\tLoss: 0.066135\n",
      "Train Epoch: 0 [33600/34001 (99%)]\tLoss: 0.387060\n",
      "Train Epoch: 0 [33920/34001 (100%)]\tLoss: 0.263827\n",
      "Train Accuracy after epoch 0: 0.8915326019822947\n",
      "Validation set: Average loss: 0.0059, Accuracy: 7823/8501 (92%)\n",
      "\n",
      "Train Epoch: 1 [0/34001 (0%)]\tLoss: 0.069978\n",
      "Train Epoch: 1 [320/34001 (1%)]\tLoss: 0.194401\n",
      "Train Epoch: 1 [640/34001 (2%)]\tLoss: 0.121986\n",
      "Train Epoch: 1 [960/34001 (3%)]\tLoss: 0.289050\n",
      "Train Epoch: 1 [1280/34001 (4%)]\tLoss: 0.103977\n",
      "Train Epoch: 1 [1600/34001 (5%)]\tLoss: 0.193709\n",
      "Train Epoch: 1 [1920/34001 (6%)]\tLoss: 0.227976\n",
      "Train Epoch: 1 [2240/34001 (7%)]\tLoss: 0.286154\n",
      "Train Epoch: 1 [2560/34001 (8%)]\tLoss: 0.155522\n",
      "Train Epoch: 1 [2880/34001 (8%)]\tLoss: 0.178709\n",
      "Train Epoch: 1 [3200/34001 (9%)]\tLoss: 0.082057\n",
      "Train Epoch: 1 [3520/34001 (10%)]\tLoss: 0.141896\n",
      "Train Epoch: 1 [3840/34001 (11%)]\tLoss: 0.163744\n",
      "Train Epoch: 1 [4160/34001 (12%)]\tLoss: 0.294114\n",
      "Train Epoch: 1 [4480/34001 (13%)]\tLoss: 0.334866\n",
      "Train Epoch: 1 [4800/34001 (14%)]\tLoss: 0.305852\n",
      "Train Epoch: 1 [5120/34001 (15%)]\tLoss: 0.213907\n",
      "Train Epoch: 1 [5440/34001 (16%)]\tLoss: 0.225481\n",
      "Train Epoch: 1 [5760/34001 (17%)]\tLoss: 0.220963\n",
      "Train Epoch: 1 [6080/34001 (18%)]\tLoss: 0.219355\n",
      "Train Epoch: 1 [6400/34001 (19%)]\tLoss: 0.282673\n",
      "Train Epoch: 1 [6720/34001 (20%)]\tLoss: 0.130987\n",
      "Train Epoch: 1 [7040/34001 (21%)]\tLoss: 0.167497\n",
      "Train Epoch: 1 [7360/34001 (22%)]\tLoss: 0.169744\n",
      "Train Epoch: 1 [7680/34001 (23%)]\tLoss: 0.169175\n",
      "Train Epoch: 1 [8000/34001 (24%)]\tLoss: 0.207904\n",
      "Train Epoch: 1 [8320/34001 (24%)]\tLoss: 0.154922\n",
      "Train Epoch: 1 [8640/34001 (25%)]\tLoss: 0.111959\n",
      "Train Epoch: 1 [8960/34001 (26%)]\tLoss: 0.130608\n",
      "Train Epoch: 1 [9280/34001 (27%)]\tLoss: 0.228324\n",
      "Train Epoch: 1 [9600/34001 (28%)]\tLoss: 0.220410\n",
      "Train Epoch: 1 [9920/34001 (29%)]\tLoss: 0.186005\n",
      "Train Epoch: 1 [10240/34001 (30%)]\tLoss: 0.114606\n",
      "Train Epoch: 1 [10560/34001 (31%)]\tLoss: 0.160779\n",
      "Train Epoch: 1 [10880/34001 (32%)]\tLoss: 0.171409\n",
      "Train Epoch: 1 [11200/34001 (33%)]\tLoss: 0.080857\n",
      "Train Epoch: 1 [11520/34001 (34%)]\tLoss: 0.154843\n",
      "Train Epoch: 1 [11840/34001 (35%)]\tLoss: 0.303697\n",
      "Train Epoch: 1 [12160/34001 (36%)]\tLoss: 0.183913\n",
      "Train Epoch: 1 [12480/34001 (37%)]\tLoss: 0.085993\n",
      "Train Epoch: 1 [12800/34001 (38%)]\tLoss: 0.083131\n",
      "Train Epoch: 1 [13120/34001 (39%)]\tLoss: 0.106846\n",
      "Train Epoch: 1 [13440/34001 (40%)]\tLoss: 0.178028\n",
      "Train Epoch: 1 [13760/34001 (40%)]\tLoss: 0.048444\n",
      "Train Epoch: 1 [14080/34001 (41%)]\tLoss: 0.113716\n",
      "Train Epoch: 1 [14400/34001 (42%)]\tLoss: 0.220445\n",
      "Train Epoch: 1 [14720/34001 (43%)]\tLoss: 0.103111\n",
      "Train Epoch: 1 [15040/34001 (44%)]\tLoss: 0.371476\n",
      "Train Epoch: 1 [15360/34001 (45%)]\tLoss: 0.042035\n",
      "Train Epoch: 1 [15680/34001 (46%)]\tLoss: 0.092377\n",
      "Train Epoch: 1 [16000/34001 (47%)]\tLoss: 0.219905\n",
      "Train Epoch: 1 [16320/34001 (48%)]\tLoss: 0.143702\n",
      "Train Epoch: 1 [16640/34001 (49%)]\tLoss: 0.154940\n",
      "Train Epoch: 1 [16960/34001 (50%)]\tLoss: 0.216588\n",
      "Train Epoch: 1 [17280/34001 (51%)]\tLoss: 0.151934\n",
      "Train Epoch: 1 [17600/34001 (52%)]\tLoss: 0.189241\n",
      "Train Epoch: 1 [17920/34001 (53%)]\tLoss: 0.348357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [18240/34001 (54%)]\tLoss: 0.138301\n",
      "Train Epoch: 1 [18560/34001 (55%)]\tLoss: 0.106271\n",
      "Train Epoch: 1 [18880/34001 (56%)]\tLoss: 0.098847\n",
      "Train Epoch: 1 [19200/34001 (56%)]\tLoss: 0.218652\n",
      "Train Epoch: 1 [19520/34001 (57%)]\tLoss: 0.231545\n",
      "Train Epoch: 1 [19840/34001 (58%)]\tLoss: 0.046507\n",
      "Train Epoch: 1 [20160/34001 (59%)]\tLoss: 0.087400\n",
      "Train Epoch: 1 [20480/34001 (60%)]\tLoss: 0.226053\n",
      "Train Epoch: 1 [20800/34001 (61%)]\tLoss: 0.116296\n",
      "Train Epoch: 1 [21120/34001 (62%)]\tLoss: 0.226749\n",
      "Train Epoch: 1 [21440/34001 (63%)]\tLoss: 0.046547\n",
      "Train Epoch: 1 [21760/34001 (64%)]\tLoss: 0.159744\n",
      "Train Epoch: 1 [22080/34001 (65%)]\tLoss: 0.195973\n",
      "Train Epoch: 1 [22400/34001 (66%)]\tLoss: 0.045444\n",
      "Train Epoch: 1 [22720/34001 (67%)]\tLoss: 0.194935\n",
      "Train Epoch: 1 [23040/34001 (68%)]\tLoss: 0.151457\n",
      "Train Epoch: 1 [23360/34001 (69%)]\tLoss: 0.149172\n",
      "Train Epoch: 1 [23680/34001 (70%)]\tLoss: 0.148056\n",
      "Train Epoch: 1 [24000/34001 (71%)]\tLoss: 0.137996\n",
      "Train Epoch: 1 [24320/34001 (71%)]\tLoss: 0.105798\n",
      "Train Epoch: 1 [24640/34001 (72%)]\tLoss: 0.159299\n",
      "Train Epoch: 1 [24960/34001 (73%)]\tLoss: 0.042587\n",
      "Train Epoch: 1 [25280/34001 (74%)]\tLoss: 0.269033\n",
      "Train Epoch: 1 [25600/34001 (75%)]\tLoss: 0.141605\n",
      "Train Epoch: 1 [25920/34001 (76%)]\tLoss: 0.117860\n",
      "Train Epoch: 1 [26240/34001 (77%)]\tLoss: 0.075247\n",
      "Train Epoch: 1 [26560/34001 (78%)]\tLoss: 0.245326\n",
      "Train Epoch: 1 [26880/34001 (79%)]\tLoss: 0.093779\n",
      "Train Epoch: 1 [27200/34001 (80%)]\tLoss: 0.366246\n",
      "Train Epoch: 1 [27520/34001 (81%)]\tLoss: 0.099194\n",
      "Train Epoch: 1 [27840/34001 (82%)]\tLoss: 0.102118\n",
      "Train Epoch: 1 [28160/34001 (83%)]\tLoss: 0.122773\n",
      "Train Epoch: 1 [28480/34001 (84%)]\tLoss: 0.130180\n",
      "Train Epoch: 1 [28800/34001 (85%)]\tLoss: 0.323510\n",
      "Train Epoch: 1 [29120/34001 (86%)]\tLoss: 0.250331\n",
      "Train Epoch: 1 [29440/34001 (87%)]\tLoss: 0.167878\n",
      "Train Epoch: 1 [29760/34001 (87%)]\tLoss: 0.276802\n",
      "Train Epoch: 1 [30080/34001 (88%)]\tLoss: 0.091179\n",
      "Train Epoch: 1 [30400/34001 (89%)]\tLoss: 0.065944\n",
      "Train Epoch: 1 [30720/34001 (90%)]\tLoss: 0.146818\n",
      "Train Epoch: 1 [31040/34001 (91%)]\tLoss: 0.183634\n",
      "Train Epoch: 1 [31360/34001 (92%)]\tLoss: 0.286948\n",
      "Train Epoch: 1 [31680/34001 (93%)]\tLoss: 0.252872\n",
      "Train Epoch: 1 [32000/34001 (94%)]\tLoss: 0.234635\n",
      "Train Epoch: 1 [32320/34001 (95%)]\tLoss: 0.201455\n",
      "Train Epoch: 1 [32640/34001 (96%)]\tLoss: 0.094897\n",
      "Train Epoch: 1 [32960/34001 (97%)]\tLoss: 0.114267\n",
      "Train Epoch: 1 [33280/34001 (98%)]\tLoss: 0.084882\n",
      "Train Epoch: 1 [33600/34001 (99%)]\tLoss: 0.294528\n",
      "Train Epoch: 1 [33920/34001 (100%)]\tLoss: 0.251168\n",
      "Train Accuracy after epoch 1: 0.9283256374812505\n",
      "Validation set: Average loss: 0.0056, Accuracy: 7858/8501 (92%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-29 12:35:48,255]\u001b[0m Trial 4 finished with value: 0.9243618397835549 and parameters: {'lr': 4.580560474043079e-06, 'optimizer_name': 'AdamW', 'epochs': 2, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.6, 'scheduler': 'exponential'}. Best is trial 2 with value: 0.9349488295494648.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial params: {'lr': 1.4476979297784871e-05, 'optimizer_name': 'Adam', 'epochs': 1, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.7000000000000001, 'scheduler': 'step'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/34001 (0%)]\tLoss: 0.698782\n",
      "Train Epoch: 0 [320/34001 (1%)]\tLoss: 0.688401\n",
      "Train Epoch: 0 [640/34001 (2%)]\tLoss: 0.642694\n",
      "Train Epoch: 0 [960/34001 (3%)]\tLoss: 0.423341\n",
      "Train Epoch: 0 [1280/34001 (4%)]\tLoss: 0.456066\n",
      "Train Epoch: 0 [1600/34001 (5%)]\tLoss: 0.478377\n",
      "Train Epoch: 0 [1920/34001 (6%)]\tLoss: 0.337333\n",
      "Train Epoch: 0 [2240/34001 (7%)]\tLoss: 0.333076\n",
      "Train Epoch: 0 [2560/34001 (8%)]\tLoss: 0.422747\n",
      "Train Epoch: 0 [2880/34001 (8%)]\tLoss: 0.451569\n",
      "Train Epoch: 0 [3200/34001 (9%)]\tLoss: 0.306374\n",
      "Train Epoch: 0 [3520/34001 (10%)]\tLoss: 0.423173\n",
      "Train Epoch: 0 [3840/34001 (11%)]\tLoss: 0.161158\n",
      "Train Epoch: 0 [4160/34001 (12%)]\tLoss: 0.279893\n",
      "Train Epoch: 0 [4480/34001 (13%)]\tLoss: 0.218684\n",
      "Train Epoch: 0 [4800/34001 (14%)]\tLoss: 0.149125\n",
      "Train Epoch: 0 [5120/34001 (15%)]\tLoss: 0.196837\n",
      "Train Epoch: 0 [5440/34001 (16%)]\tLoss: 0.248225\n",
      "Train Epoch: 0 [5760/34001 (17%)]\tLoss: 0.137908\n",
      "Train Epoch: 0 [6080/34001 (18%)]\tLoss: 0.267018\n",
      "Train Epoch: 0 [6400/34001 (19%)]\tLoss: 0.249137\n",
      "Train Epoch: 0 [6720/34001 (20%)]\tLoss: 0.211729\n",
      "Train Epoch: 0 [7040/34001 (21%)]\tLoss: 0.135050\n",
      "Train Epoch: 0 [7360/34001 (22%)]\tLoss: 0.045097\n",
      "Train Epoch: 0 [7680/34001 (23%)]\tLoss: 0.210377\n",
      "Train Epoch: 0 [8000/34001 (24%)]\tLoss: 0.206388\n",
      "Train Epoch: 0 [8320/34001 (24%)]\tLoss: 0.155053\n",
      "Train Epoch: 0 [8640/34001 (25%)]\tLoss: 0.206817\n",
      "Train Epoch: 0 [8960/34001 (26%)]\tLoss: 0.201478\n",
      "Train Epoch: 0 [9280/34001 (27%)]\tLoss: 0.269612\n",
      "Train Epoch: 0 [9600/34001 (28%)]\tLoss: 0.223487\n",
      "Train Epoch: 0 [9920/34001 (29%)]\tLoss: 0.274752\n",
      "Train Epoch: 0 [10240/34001 (30%)]\tLoss: 0.221933\n",
      "Train Epoch: 0 [10560/34001 (31%)]\tLoss: 0.317678\n",
      "Train Epoch: 0 [10880/34001 (32%)]\tLoss: 0.181217\n",
      "Train Epoch: 0 [11200/34001 (33%)]\tLoss: 0.179885\n",
      "Train Epoch: 0 [11520/34001 (34%)]\tLoss: 0.216659\n",
      "Train Epoch: 0 [11840/34001 (35%)]\tLoss: 0.095926\n",
      "Train Epoch: 0 [12160/34001 (36%)]\tLoss: 0.197093\n",
      "Train Epoch: 0 [12480/34001 (37%)]\tLoss: 0.208361\n",
      "Train Epoch: 0 [12800/34001 (38%)]\tLoss: 0.183693\n",
      "Train Epoch: 0 [13120/34001 (39%)]\tLoss: 0.215028\n",
      "Train Epoch: 0 [13440/34001 (40%)]\tLoss: 0.126010\n",
      "Train Epoch: 0 [13760/34001 (40%)]\tLoss: 0.137901\n",
      "Train Epoch: 0 [14080/34001 (41%)]\tLoss: 0.246319\n",
      "Train Epoch: 0 [14400/34001 (42%)]\tLoss: 0.177499\n",
      "Train Epoch: 0 [14720/34001 (43%)]\tLoss: 0.287750\n",
      "Train Epoch: 0 [15040/34001 (44%)]\tLoss: 0.165734\n",
      "Train Epoch: 0 [15360/34001 (45%)]\tLoss: 0.321031\n",
      "Train Epoch: 0 [15680/34001 (46%)]\tLoss: 0.075254\n",
      "Train Epoch: 0 [16000/34001 (47%)]\tLoss: 0.068319\n",
      "Train Epoch: 0 [16320/34001 (48%)]\tLoss: 0.083713\n",
      "Train Epoch: 0 [16640/34001 (49%)]\tLoss: 0.082601\n",
      "Train Epoch: 0 [16960/34001 (50%)]\tLoss: 0.126799\n",
      "Train Epoch: 0 [17280/34001 (51%)]\tLoss: 0.373624\n",
      "Train Epoch: 0 [17600/34001 (52%)]\tLoss: 0.213678\n",
      "Train Epoch: 0 [17920/34001 (53%)]\tLoss: 0.255577\n",
      "Train Epoch: 0 [18240/34001 (54%)]\tLoss: 0.255413\n",
      "Train Epoch: 0 [18560/34001 (55%)]\tLoss: 0.183771\n",
      "Train Epoch: 0 [18880/34001 (56%)]\tLoss: 0.185247\n",
      "Train Epoch: 0 [19200/34001 (56%)]\tLoss: 0.057746\n",
      "Train Epoch: 0 [19520/34001 (57%)]\tLoss: 0.196618\n",
      "Train Epoch: 0 [19840/34001 (58%)]\tLoss: 0.159932\n",
      "Train Epoch: 0 [20160/34001 (59%)]\tLoss: 0.152443\n",
      "Train Epoch: 0 [20480/34001 (60%)]\tLoss: 0.226087\n",
      "Train Epoch: 0 [20800/34001 (61%)]\tLoss: 0.473737\n",
      "Train Epoch: 0 [21120/34001 (62%)]\tLoss: 0.245547\n",
      "Train Epoch: 0 [21440/34001 (63%)]\tLoss: 0.372334\n",
      "Train Epoch: 0 [21760/34001 (64%)]\tLoss: 0.112126\n",
      "Train Epoch: 0 [22080/34001 (65%)]\tLoss: 0.240924\n",
      "Train Epoch: 0 [22400/34001 (66%)]\tLoss: 0.184676\n",
      "Train Epoch: 0 [22720/34001 (67%)]\tLoss: 0.297765\n",
      "Train Epoch: 0 [23040/34001 (68%)]\tLoss: 0.188801\n",
      "Train Epoch: 0 [23360/34001 (69%)]\tLoss: 0.143665\n",
      "Train Epoch: 0 [23680/34001 (70%)]\tLoss: 0.196543\n",
      "Train Epoch: 0 [24000/34001 (71%)]\tLoss: 0.127744\n",
      "Train Epoch: 0 [24320/34001 (71%)]\tLoss: 0.049693\n",
      "Train Epoch: 0 [24640/34001 (72%)]\tLoss: 0.077006\n",
      "Train Epoch: 0 [24960/34001 (73%)]\tLoss: 0.145195\n",
      "Train Epoch: 0 [25280/34001 (74%)]\tLoss: 0.099696\n",
      "Train Epoch: 0 [25600/34001 (75%)]\tLoss: 0.073841\n",
      "Train Epoch: 0 [25920/34001 (76%)]\tLoss: 0.248357\n",
      "Train Epoch: 0 [26240/34001 (77%)]\tLoss: 0.230798\n",
      "Train Epoch: 0 [26560/34001 (78%)]\tLoss: 0.161430\n",
      "Train Epoch: 0 [26880/34001 (79%)]\tLoss: 0.203079\n",
      "Train Epoch: 0 [27200/34001 (80%)]\tLoss: 0.239950\n",
      "Train Epoch: 0 [27520/34001 (81%)]\tLoss: 0.279710\n",
      "Train Epoch: 0 [27840/34001 (82%)]\tLoss: 0.220555\n",
      "Train Epoch: 0 [28160/34001 (83%)]\tLoss: 0.283052\n",
      "Train Epoch: 0 [28480/34001 (84%)]\tLoss: 0.279419\n",
      "Train Epoch: 0 [28800/34001 (85%)]\tLoss: 0.120245\n",
      "Train Epoch: 0 [29120/34001 (86%)]\tLoss: 0.106743\n",
      "Train Epoch: 0 [29440/34001 (87%)]\tLoss: 0.182852\n",
      "Train Epoch: 0 [29760/34001 (87%)]\tLoss: 0.131489\n",
      "Train Epoch: 0 [30080/34001 (88%)]\tLoss: 0.231390\n",
      "Train Epoch: 0 [30400/34001 (89%)]\tLoss: 0.078970\n",
      "Train Epoch: 0 [30720/34001 (90%)]\tLoss: 0.189750\n",
      "Train Epoch: 0 [31040/34001 (91%)]\tLoss: 0.111055\n",
      "Train Epoch: 0 [31360/34001 (92%)]\tLoss: 0.125886\n",
      "Train Epoch: 0 [31680/34001 (93%)]\tLoss: 0.087766\n",
      "Train Epoch: 0 [32000/34001 (94%)]\tLoss: 0.102535\n",
      "Train Epoch: 0 [32320/34001 (95%)]\tLoss: 0.107693\n",
      "Train Epoch: 0 [32640/34001 (96%)]\tLoss: 0.211375\n",
      "Train Epoch: 0 [32960/34001 (97%)]\tLoss: 0.076797\n",
      "Train Epoch: 0 [33280/34001 (98%)]\tLoss: 0.121510\n",
      "Train Epoch: 0 [33600/34001 (99%)]\tLoss: 0.370365\n",
      "Train Epoch: 0 [33920/34001 (100%)]\tLoss: 0.195156\n",
      "Train Accuracy after epoch 0: 0.9012087879768242\n",
      "Validation set: Average loss: 0.0050, Accuracy: 7945/8501 (93%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-29 12:45:30,663]\u001b[0m Trial 5 finished with value: 0.9345959298906011 and parameters: {'lr': 1.4476979297784871e-05, 'optimizer_name': 'Adam', 'epochs': 1, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.7000000000000001, 'scheduler': 'step'}. Best is trial 2 with value: 0.9349488295494648.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial params: {'lr': 0.00046618703556985193, 'optimizer_name': 'AdamW', 'epochs': 3, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.4, 'scheduler': 'step'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/34001 (0%)]\tLoss: 0.687464\n",
      "Train Epoch: 0 [320/34001 (1%)]\tLoss: 0.685635\n",
      "Train Epoch: 0 [640/34001 (2%)]\tLoss: 0.567514\n",
      "Train Epoch: 0 [960/34001 (3%)]\tLoss: 0.596024\n",
      "Train Epoch: 0 [1280/34001 (4%)]\tLoss: 0.671776\n",
      "Train Epoch: 0 [1600/34001 (5%)]\tLoss: 0.704108\n",
      "Train Epoch: 0 [1920/34001 (6%)]\tLoss: 0.867084\n",
      "Train Epoch: 0 [2240/34001 (7%)]\tLoss: 0.695383\n",
      "Train Epoch: 0 [2560/34001 (8%)]\tLoss: 0.804598\n",
      "Train Epoch: 0 [2880/34001 (8%)]\tLoss: 0.689250\n",
      "Train Epoch: 0 [3200/34001 (9%)]\tLoss: 0.755707\n",
      "Train Epoch: 0 [3520/34001 (10%)]\tLoss: 0.657728\n",
      "Train Epoch: 0 [3840/34001 (11%)]\tLoss: 0.672104\n",
      "Train Epoch: 0 [4160/34001 (12%)]\tLoss: 0.669951\n",
      "Train Epoch: 0 [4480/34001 (13%)]\tLoss: 0.696058\n",
      "Train Epoch: 0 [4800/34001 (14%)]\tLoss: 0.689018\n",
      "Train Epoch: 0 [5120/34001 (15%)]\tLoss: 0.645958\n",
      "Train Epoch: 0 [5440/34001 (16%)]\tLoss: 0.712303\n",
      "Train Epoch: 0 [5760/34001 (17%)]\tLoss: 0.635463\n",
      "Train Epoch: 0 [6080/34001 (18%)]\tLoss: 0.746815\n",
      "Train Epoch: 0 [6400/34001 (19%)]\tLoss: 0.689846\n",
      "Train Epoch: 0 [6720/34001 (20%)]\tLoss: 0.690452\n",
      "Train Epoch: 0 [7040/34001 (21%)]\tLoss: 0.660113\n",
      "Train Epoch: 0 [7360/34001 (22%)]\tLoss: 0.695289\n",
      "Train Epoch: 0 [7680/34001 (23%)]\tLoss: 0.583007\n",
      "Train Epoch: 0 [8000/34001 (24%)]\tLoss: 0.725515\n",
      "Train Epoch: 0 [8320/34001 (24%)]\tLoss: 0.646119\n",
      "Train Epoch: 0 [8640/34001 (25%)]\tLoss: 0.679529\n",
      "Train Epoch: 0 [8960/34001 (26%)]\tLoss: 0.671276\n",
      "Train Epoch: 0 [9280/34001 (27%)]\tLoss: 0.631330\n",
      "Train Epoch: 0 [9600/34001 (28%)]\tLoss: 0.724890\n",
      "Train Epoch: 0 [9920/34001 (29%)]\tLoss: 0.645661\n",
      "Train Epoch: 0 [10240/34001 (30%)]\tLoss: 0.660447\n",
      "Train Epoch: 0 [10560/34001 (31%)]\tLoss: 0.715595\n",
      "Train Epoch: 0 [10880/34001 (32%)]\tLoss: 0.625572\n",
      "Train Epoch: 0 [11200/34001 (33%)]\tLoss: 0.608575\n",
      "Train Epoch: 0 [11520/34001 (34%)]\tLoss: 0.689366\n",
      "Train Epoch: 0 [11840/34001 (35%)]\tLoss: 0.650730\n",
      "Train Epoch: 0 [12160/34001 (36%)]\tLoss: 0.759238\n",
      "Train Epoch: 0 [12480/34001 (37%)]\tLoss: 0.723169\n",
      "Train Epoch: 0 [12800/34001 (38%)]\tLoss: 0.676998\n",
      "Train Epoch: 0 [13120/34001 (39%)]\tLoss: 0.636084\n",
      "Train Epoch: 0 [13440/34001 (40%)]\tLoss: 0.647153\n",
      "Train Epoch: 0 [13760/34001 (40%)]\tLoss: 0.679291\n",
      "Train Epoch: 0 [14080/34001 (41%)]\tLoss: 0.718684\n",
      "Train Epoch: 0 [14400/34001 (42%)]\tLoss: 0.648521\n",
      "Train Epoch: 0 [14720/34001 (43%)]\tLoss: 0.725347\n",
      "Train Epoch: 0 [15040/34001 (44%)]\tLoss: 0.647409\n",
      "Train Epoch: 0 [15360/34001 (45%)]\tLoss: 0.676454\n",
      "Train Epoch: 0 [15680/34001 (46%)]\tLoss: 0.717272\n",
      "Train Epoch: 0 [16000/34001 (47%)]\tLoss: 0.653311\n",
      "Train Epoch: 0 [16320/34001 (48%)]\tLoss: 0.635930\n",
      "Train Epoch: 0 [16640/34001 (49%)]\tLoss: 0.646899\n",
      "Train Epoch: 0 [16960/34001 (50%)]\tLoss: 0.704410\n",
      "Train Epoch: 0 [17280/34001 (51%)]\tLoss: 0.636622\n",
      "Train Epoch: 0 [17600/34001 (52%)]\tLoss: 0.683049\n",
      "Train Epoch: 0 [17920/34001 (53%)]\tLoss: 0.667637\n",
      "Train Epoch: 0 [18240/34001 (54%)]\tLoss: 0.727817\n",
      "Train Epoch: 0 [18560/34001 (55%)]\tLoss: 0.650397\n",
      "Train Epoch: 0 [18880/34001 (56%)]\tLoss: 0.625674\n",
      "Train Epoch: 0 [19200/34001 (56%)]\tLoss: 0.639619\n",
      "Train Epoch: 0 [19520/34001 (57%)]\tLoss: 0.697963\n",
      "Train Epoch: 0 [19840/34001 (58%)]\tLoss: 0.691997\n",
      "Train Epoch: 0 [20160/34001 (59%)]\tLoss: 0.675277\n",
      "Train Epoch: 0 [20480/34001 (60%)]\tLoss: 0.640276\n",
      "Train Epoch: 0 [20800/34001 (61%)]\tLoss: 0.694108\n",
      "Train Epoch: 0 [21120/34001 (62%)]\tLoss: 0.717731\n",
      "Train Epoch: 0 [21440/34001 (63%)]\tLoss: 0.725875\n",
      "Train Epoch: 0 [21760/34001 (64%)]\tLoss: 0.672172\n",
      "Train Epoch: 0 [22080/34001 (65%)]\tLoss: 0.650113\n",
      "Train Epoch: 0 [22400/34001 (66%)]\tLoss: 0.666998\n",
      "Train Epoch: 0 [22720/34001 (67%)]\tLoss: 0.682193\n",
      "Train Epoch: 0 [23040/34001 (68%)]\tLoss: 0.684889\n",
      "Train Epoch: 0 [23360/34001 (69%)]\tLoss: 0.676408\n",
      "Train Epoch: 0 [23680/34001 (70%)]\tLoss: 0.692265\n",
      "Train Epoch: 0 [24000/34001 (71%)]\tLoss: 0.706413\n",
      "Train Epoch: 0 [24320/34001 (71%)]\tLoss: 0.700755\n",
      "Train Epoch: 0 [24640/34001 (72%)]\tLoss: 0.673403\n",
      "Train Epoch: 0 [24960/34001 (73%)]\tLoss: 0.615682\n",
      "Train Epoch: 0 [25280/34001 (74%)]\tLoss: 0.655510\n",
      "Train Epoch: 0 [25600/34001 (75%)]\tLoss: 0.649894\n",
      "Train Epoch: 0 [25920/34001 (76%)]\tLoss: 0.670321\n",
      "Train Epoch: 0 [26240/34001 (77%)]\tLoss: 0.728107\n",
      "Train Epoch: 0 [26560/34001 (78%)]\tLoss: 0.662730\n",
      "Train Epoch: 0 [26880/34001 (79%)]\tLoss: 0.650248\n",
      "Train Epoch: 0 [27200/34001 (80%)]\tLoss: 0.631241\n",
      "Train Epoch: 0 [27520/34001 (81%)]\tLoss: 0.662248\n",
      "Train Epoch: 0 [27840/34001 (82%)]\tLoss: 0.743030\n",
      "Train Epoch: 0 [28160/34001 (83%)]\tLoss: 0.707996\n",
      "Train Epoch: 0 [28480/34001 (84%)]\tLoss: 0.690003\n",
      "Train Epoch: 0 [28800/34001 (85%)]\tLoss: 0.672369\n",
      "Train Epoch: 0 [29120/34001 (86%)]\tLoss: 0.658526\n",
      "Train Epoch: 0 [29440/34001 (87%)]\tLoss: 0.663431\n",
      "Train Epoch: 0 [29760/34001 (87%)]\tLoss: 0.755641\n",
      "Train Epoch: 0 [30080/34001 (88%)]\tLoss: 0.654884\n",
      "Train Epoch: 0 [30400/34001 (89%)]\tLoss: 0.677205\n",
      "Train Epoch: 0 [30720/34001 (90%)]\tLoss: 0.636441\n",
      "Train Epoch: 0 [31040/34001 (91%)]\tLoss: 0.726700\n",
      "Train Epoch: 0 [31360/34001 (92%)]\tLoss: 0.743579\n",
      "Train Epoch: 0 [31680/34001 (93%)]\tLoss: 0.677922\n",
      "Train Epoch: 0 [32000/34001 (94%)]\tLoss: 0.659787\n",
      "Train Epoch: 0 [32320/34001 (95%)]\tLoss: 0.698567\n",
      "Train Epoch: 0 [32640/34001 (96%)]\tLoss: 0.658012\n",
      "Train Epoch: 0 [32960/34001 (97%)]\tLoss: 0.677841\n",
      "Train Epoch: 0 [33280/34001 (98%)]\tLoss: 0.639627\n",
      "Train Epoch: 0 [33600/34001 (99%)]\tLoss: 0.680006\n",
      "Train Epoch: 0 [33920/34001 (100%)]\tLoss: 0.734645\n",
      "Train Accuracy after epoch 0: 0.599747066262757\n",
      "Validation set: Average loss: 0.0211, Accuracy: 5067/8501 (60%)\n",
      "\n",
      "Train Epoch: 1 [0/34001 (0%)]\tLoss: 0.659120\n",
      "Train Epoch: 1 [320/34001 (1%)]\tLoss: 0.628316\n",
      "Train Epoch: 1 [640/34001 (2%)]\tLoss: 0.713269\n",
      "Train Epoch: 1 [960/34001 (3%)]\tLoss: 0.683727\n",
      "Train Epoch: 1 [1280/34001 (4%)]\tLoss: 0.666937\n",
      "Train Epoch: 1 [1600/34001 (5%)]\tLoss: 0.688605\n",
      "Train Epoch: 1 [1920/34001 (6%)]\tLoss: 0.727065\n",
      "Train Epoch: 1 [2240/34001 (7%)]\tLoss: 0.686881\n",
      "Train Epoch: 1 [2560/34001 (8%)]\tLoss: 0.717367\n",
      "Train Epoch: 1 [2880/34001 (8%)]\tLoss: 0.725353\n",
      "Train Epoch: 1 [3200/34001 (9%)]\tLoss: 0.723421\n",
      "Train Epoch: 1 [3520/34001 (10%)]\tLoss: 0.648989\n",
      "Train Epoch: 1 [3840/34001 (11%)]\tLoss: 0.654268\n",
      "Train Epoch: 1 [4160/34001 (12%)]\tLoss: 0.643684\n",
      "Train Epoch: 1 [4480/34001 (13%)]\tLoss: 0.704141\n",
      "Train Epoch: 1 [4800/34001 (14%)]\tLoss: 0.686934\n",
      "Train Epoch: 1 [5120/34001 (15%)]\tLoss: 0.659108\n",
      "Train Epoch: 1 [5440/34001 (16%)]\tLoss: 0.714001\n",
      "Train Epoch: 1 [5760/34001 (17%)]\tLoss: 0.635668\n",
      "Train Epoch: 1 [6080/34001 (18%)]\tLoss: 0.718279\n",
      "Train Epoch: 1 [6400/34001 (19%)]\tLoss: 0.692098\n",
      "Train Epoch: 1 [6720/34001 (20%)]\tLoss: 0.693158\n",
      "Train Epoch: 1 [7040/34001 (21%)]\tLoss: 0.671795\n",
      "Train Epoch: 1 [7360/34001 (22%)]\tLoss: 0.702740\n",
      "Train Epoch: 1 [7680/34001 (23%)]\tLoss: 0.593149\n",
      "Train Epoch: 1 [8000/34001 (24%)]\tLoss: 0.725629\n",
      "Train Epoch: 1 [8320/34001 (24%)]\tLoss: 0.629175\n",
      "Train Epoch: 1 [8640/34001 (25%)]\tLoss: 0.688165\n",
      "Train Epoch: 1 [8960/34001 (26%)]\tLoss: 0.667045\n",
      "Train Epoch: 1 [9280/34001 (27%)]\tLoss: 0.644478\n",
      "Train Epoch: 1 [9600/34001 (28%)]\tLoss: 0.718682\n",
      "Train Epoch: 1 [9920/34001 (29%)]\tLoss: 0.637386\n",
      "Train Epoch: 1 [10240/34001 (30%)]\tLoss: 0.667823\n",
      "Train Epoch: 1 [10560/34001 (31%)]\tLoss: 0.710529\n",
      "Train Epoch: 1 [10880/34001 (32%)]\tLoss: 0.609258\n",
      "Train Epoch: 1 [11200/34001 (33%)]\tLoss: 0.626611\n",
      "Train Epoch: 1 [11520/34001 (34%)]\tLoss: 0.688272\n",
      "Train Epoch: 1 [11840/34001 (35%)]\tLoss: 0.671368\n",
      "Train Epoch: 1 [12160/34001 (36%)]\tLoss: 0.740259\n",
      "Train Epoch: 1 [12480/34001 (37%)]\tLoss: 0.732890\n",
      "Train Epoch: 1 [12800/34001 (38%)]\tLoss: 0.695980\n",
      "Train Epoch: 1 [13120/34001 (39%)]\tLoss: 0.644905\n",
      "Train Epoch: 1 [13440/34001 (40%)]\tLoss: 0.656718\n",
      "Train Epoch: 1 [13760/34001 (40%)]\tLoss: 0.668792\n",
      "Train Epoch: 1 [14080/34001 (41%)]\tLoss: 0.721292\n",
      "Train Epoch: 1 [14400/34001 (42%)]\tLoss: 0.654015\n",
      "Train Epoch: 1 [14720/34001 (43%)]\tLoss: 0.724529\n",
      "Train Epoch: 1 [15040/34001 (44%)]\tLoss: 0.644555\n",
      "Train Epoch: 1 [15360/34001 (45%)]\tLoss: 0.666799\n",
      "Train Epoch: 1 [15680/34001 (46%)]\tLoss: 0.718735\n",
      "Train Epoch: 1 [16000/34001 (47%)]\tLoss: 0.654145\n",
      "Train Epoch: 1 [16320/34001 (48%)]\tLoss: 0.643622\n",
      "Train Epoch: 1 [16640/34001 (49%)]\tLoss: 0.630735\n",
      "Train Epoch: 1 [16960/34001 (50%)]\tLoss: 0.695532\n",
      "Train Epoch: 1 [17280/34001 (51%)]\tLoss: 0.641850\n",
      "Train Epoch: 1 [17600/34001 (52%)]\tLoss: 0.689055\n",
      "Train Epoch: 1 [17920/34001 (53%)]\tLoss: 0.662000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [18240/34001 (54%)]\tLoss: 0.750650\n",
      "Train Epoch: 1 [18560/34001 (55%)]\tLoss: 0.640015\n",
      "Train Epoch: 1 [18880/34001 (56%)]\tLoss: 0.631531\n",
      "Train Epoch: 1 [19200/34001 (56%)]\tLoss: 0.634054\n",
      "Train Epoch: 1 [19520/34001 (57%)]\tLoss: 0.686057\n",
      "Train Epoch: 1 [19840/34001 (58%)]\tLoss: 0.704487\n",
      "Train Epoch: 1 [20160/34001 (59%)]\tLoss: 0.680008\n",
      "Train Epoch: 1 [20480/34001 (60%)]\tLoss: 0.638284\n",
      "Train Epoch: 1 [20800/34001 (61%)]\tLoss: 0.693733\n",
      "Train Epoch: 1 [21120/34001 (62%)]\tLoss: 0.711981\n",
      "Train Epoch: 1 [21440/34001 (63%)]\tLoss: 0.711592\n",
      "Train Epoch: 1 [21760/34001 (64%)]\tLoss: 0.680640\n",
      "Train Epoch: 1 [22080/34001 (65%)]\tLoss: 0.652279\n",
      "Train Epoch: 1 [22400/34001 (66%)]\tLoss: 0.663490\n",
      "Train Epoch: 1 [22720/34001 (67%)]\tLoss: 0.675943\n",
      "Train Epoch: 1 [23040/34001 (68%)]\tLoss: 0.691160\n",
      "Train Epoch: 1 [23360/34001 (69%)]\tLoss: 0.682148\n",
      "Train Epoch: 1 [23680/34001 (70%)]\tLoss: 0.689690\n",
      "Train Epoch: 1 [24000/34001 (71%)]\tLoss: 0.708443\n",
      "Train Epoch: 1 [24320/34001 (71%)]\tLoss: 0.702999\n",
      "Train Epoch: 1 [24640/34001 (72%)]\tLoss: 0.668692\n",
      "Train Epoch: 1 [24960/34001 (73%)]\tLoss: 0.626813\n",
      "Train Epoch: 1 [25280/34001 (74%)]\tLoss: 0.663049\n",
      "Train Epoch: 1 [25600/34001 (75%)]\tLoss: 0.651257\n",
      "Train Epoch: 1 [25920/34001 (76%)]\tLoss: 0.659238\n",
      "Train Epoch: 1 [26240/34001 (77%)]\tLoss: 0.737818\n",
      "Train Epoch: 1 [26560/34001 (78%)]\tLoss: 0.665357\n",
      "Train Epoch: 1 [26880/34001 (79%)]\tLoss: 0.652022\n",
      "Train Epoch: 1 [27200/34001 (80%)]\tLoss: 0.632927\n",
      "Train Epoch: 1 [27520/34001 (81%)]\tLoss: 0.659438\n",
      "Train Epoch: 1 [27840/34001 (82%)]\tLoss: 0.685671\n",
      "Train Epoch: 1 [28160/34001 (83%)]\tLoss: 0.694213\n",
      "Train Epoch: 1 [28480/34001 (84%)]\tLoss: 0.696285\n",
      "Train Epoch: 1 [28800/34001 (85%)]\tLoss: 0.659424\n",
      "Train Epoch: 1 [29120/34001 (86%)]\tLoss: 0.650351\n",
      "Train Epoch: 1 [29440/34001 (87%)]\tLoss: 0.659547\n",
      "Train Epoch: 1 [29760/34001 (87%)]\tLoss: 0.727418\n",
      "Train Epoch: 1 [30080/34001 (88%)]\tLoss: 0.650846\n",
      "Train Epoch: 1 [30400/34001 (89%)]\tLoss: 0.677943\n",
      "Train Epoch: 1 [30720/34001 (90%)]\tLoss: 0.637991\n",
      "Train Epoch: 1 [31040/34001 (91%)]\tLoss: 0.714541\n",
      "Train Epoch: 1 [31360/34001 (92%)]\tLoss: 0.735154\n",
      "Train Epoch: 1 [31680/34001 (93%)]\tLoss: 0.649203\n",
      "Train Epoch: 1 [32000/34001 (94%)]\tLoss: 0.650011\n",
      "Train Epoch: 1 [32320/34001 (95%)]\tLoss: 0.692321\n",
      "Train Epoch: 1 [32640/34001 (96%)]\tLoss: 0.650720\n",
      "Train Epoch: 1 [32960/34001 (97%)]\tLoss: 0.678055\n",
      "Train Epoch: 1 [33280/34001 (98%)]\tLoss: 0.637683\n",
      "Train Epoch: 1 [33600/34001 (99%)]\tLoss: 0.689326\n",
      "Train Epoch: 1 [33920/34001 (100%)]\tLoss: 0.716081\n",
      "Train Accuracy after epoch 1: 0.598423575777183\n",
      "Validation set: Average loss: 0.0211, Accuracy: 5067/8501 (60%)\n",
      "\n",
      "Train Epoch: 2 [0/34001 (0%)]\tLoss: 0.654336\n",
      "Train Epoch: 2 [320/34001 (1%)]\tLoss: 0.634093\n",
      "Train Epoch: 2 [640/34001 (2%)]\tLoss: 0.713152\n",
      "Train Epoch: 2 [960/34001 (3%)]\tLoss: 0.686727\n",
      "Train Epoch: 2 [1280/34001 (4%)]\tLoss: 0.670340\n",
      "Train Epoch: 2 [1600/34001 (5%)]\tLoss: 0.693087\n",
      "Train Epoch: 2 [1920/34001 (6%)]\tLoss: 0.718834\n",
      "Train Epoch: 2 [2240/34001 (7%)]\tLoss: 0.690276\n",
      "Train Epoch: 2 [2560/34001 (8%)]\tLoss: 0.714090\n",
      "Train Epoch: 2 [2880/34001 (8%)]\tLoss: 0.722485\n",
      "Train Epoch: 2 [3200/34001 (9%)]\tLoss: 0.732970\n",
      "Train Epoch: 2 [3520/34001 (10%)]\tLoss: 0.645819\n",
      "Train Epoch: 2 [3840/34001 (11%)]\tLoss: 0.654168\n",
      "Train Epoch: 2 [4160/34001 (12%)]\tLoss: 0.656252\n",
      "Train Epoch: 2 [4480/34001 (13%)]\tLoss: 0.704180\n",
      "Train Epoch: 2 [4800/34001 (14%)]\tLoss: 0.686262\n",
      "Train Epoch: 2 [5120/34001 (15%)]\tLoss: 0.657045\n",
      "Train Epoch: 2 [5440/34001 (16%)]\tLoss: 0.715237\n",
      "Train Epoch: 2 [5760/34001 (17%)]\tLoss: 0.633703\n",
      "Train Epoch: 2 [6080/34001 (18%)]\tLoss: 0.716368\n",
      "Train Epoch: 2 [6400/34001 (19%)]\tLoss: 0.690600\n",
      "Train Epoch: 2 [6720/34001 (20%)]\tLoss: 0.697242\n",
      "Train Epoch: 2 [7040/34001 (21%)]\tLoss: 0.664788\n",
      "Train Epoch: 2 [7360/34001 (22%)]\tLoss: 0.698382\n",
      "Train Epoch: 2 [7680/34001 (23%)]\tLoss: 0.607733\n",
      "Train Epoch: 2 [8000/34001 (24%)]\tLoss: 0.712373\n",
      "Train Epoch: 2 [8320/34001 (24%)]\tLoss: 0.638510\n",
      "Train Epoch: 2 [8640/34001 (25%)]\tLoss: 0.687667\n",
      "Train Epoch: 2 [8960/34001 (26%)]\tLoss: 0.678191\n",
      "Train Epoch: 2 [9280/34001 (27%)]\tLoss: 0.644000\n",
      "Train Epoch: 2 [9600/34001 (28%)]\tLoss: 0.718354\n",
      "Train Epoch: 2 [9920/34001 (29%)]\tLoss: 0.642603\n",
      "Train Epoch: 2 [10240/34001 (30%)]\tLoss: 0.659170\n",
      "Train Epoch: 2 [10560/34001 (31%)]\tLoss: 0.709278\n",
      "Train Epoch: 2 [10880/34001 (32%)]\tLoss: 0.606318\n",
      "Train Epoch: 2 [11200/34001 (33%)]\tLoss: 0.618755\n",
      "Train Epoch: 2 [11520/34001 (34%)]\tLoss: 0.687990\n",
      "Train Epoch: 2 [11840/34001 (35%)]\tLoss: 0.656618\n",
      "Train Epoch: 2 [12160/34001 (36%)]\tLoss: 0.743527\n",
      "Train Epoch: 2 [12480/34001 (37%)]\tLoss: 0.737856\n",
      "Train Epoch: 2 [12800/34001 (38%)]\tLoss: 0.690757\n",
      "Train Epoch: 2 [13120/34001 (39%)]\tLoss: 0.638886\n",
      "Train Epoch: 2 [13440/34001 (40%)]\tLoss: 0.652582\n",
      "Train Epoch: 2 [13760/34001 (40%)]\tLoss: 0.679050\n",
      "Train Epoch: 2 [14080/34001 (41%)]\tLoss: 0.713928\n",
      "Train Epoch: 2 [14400/34001 (42%)]\tLoss: 0.655575\n",
      "Train Epoch: 2 [14720/34001 (43%)]\tLoss: 0.732862\n",
      "Train Epoch: 2 [15040/34001 (44%)]\tLoss: 0.638365\n",
      "Train Epoch: 2 [15360/34001 (45%)]\tLoss: 0.663572\n",
      "Train Epoch: 2 [15680/34001 (46%)]\tLoss: 0.721376\n",
      "Train Epoch: 2 [16000/34001 (47%)]\tLoss: 0.653543\n",
      "Train Epoch: 2 [16320/34001 (48%)]\tLoss: 0.639444\n",
      "Train Epoch: 2 [16640/34001 (49%)]\tLoss: 0.630412\n",
      "Train Epoch: 2 [16960/34001 (50%)]\tLoss: 0.702390\n",
      "Train Epoch: 2 [17280/34001 (51%)]\tLoss: 0.637982\n",
      "Train Epoch: 2 [17600/34001 (52%)]\tLoss: 0.693131\n",
      "Train Epoch: 2 [17920/34001 (53%)]\tLoss: 0.664418\n",
      "Train Epoch: 2 [18240/34001 (54%)]\tLoss: 0.744927\n",
      "Train Epoch: 2 [18560/34001 (55%)]\tLoss: 0.639699\n",
      "Train Epoch: 2 [18880/34001 (56%)]\tLoss: 0.628919\n",
      "Train Epoch: 2 [19200/34001 (56%)]\tLoss: 0.635366\n",
      "Train Epoch: 2 [19520/34001 (57%)]\tLoss: 0.690818\n",
      "Train Epoch: 2 [19840/34001 (58%)]\tLoss: 0.706023\n",
      "Train Epoch: 2 [20160/34001 (59%)]\tLoss: 0.672586\n",
      "Train Epoch: 2 [20480/34001 (60%)]\tLoss: 0.630588\n",
      "Train Epoch: 2 [20800/34001 (61%)]\tLoss: 0.683775\n",
      "Train Epoch: 2 [21120/34001 (62%)]\tLoss: 0.720927\n",
      "Train Epoch: 2 [21440/34001 (63%)]\tLoss: 0.711978\n",
      "Train Epoch: 2 [21760/34001 (64%)]\tLoss: 0.678602\n",
      "Train Epoch: 2 [22080/34001 (65%)]\tLoss: 0.647506\n",
      "Train Epoch: 2 [22400/34001 (66%)]\tLoss: 0.664923\n",
      "Train Epoch: 2 [22720/34001 (67%)]\tLoss: 0.677870\n",
      "Train Epoch: 2 [23040/34001 (68%)]\tLoss: 0.685878\n",
      "Train Epoch: 2 [23360/34001 (69%)]\tLoss: 0.679787\n",
      "Train Epoch: 2 [23680/34001 (70%)]\tLoss: 0.700320\n",
      "Train Epoch: 2 [24000/34001 (71%)]\tLoss: 0.709571\n",
      "Train Epoch: 2 [24320/34001 (71%)]\tLoss: 0.700824\n",
      "Train Epoch: 2 [24640/34001 (72%)]\tLoss: 0.668283\n",
      "Train Epoch: 2 [24960/34001 (73%)]\tLoss: 0.626148\n",
      "Train Epoch: 2 [25280/34001 (74%)]\tLoss: 0.668877\n",
      "Train Epoch: 2 [25600/34001 (75%)]\tLoss: 0.652087\n",
      "Train Epoch: 2 [25920/34001 (76%)]\tLoss: 0.664634\n",
      "Train Epoch: 2 [26240/34001 (77%)]\tLoss: 0.739956\n",
      "Train Epoch: 2 [26560/34001 (78%)]\tLoss: 0.664391\n",
      "Train Epoch: 2 [26880/34001 (79%)]\tLoss: 0.653046\n",
      "Train Epoch: 2 [27200/34001 (80%)]\tLoss: 0.638109\n",
      "Train Epoch: 2 [27520/34001 (81%)]\tLoss: 0.659686\n",
      "Train Epoch: 2 [27840/34001 (82%)]\tLoss: 0.672784\n",
      "Train Epoch: 2 [28160/34001 (83%)]\tLoss: 0.687792\n",
      "Train Epoch: 2 [28480/34001 (84%)]\tLoss: 0.699005\n",
      "Train Epoch: 2 [28800/34001 (85%)]\tLoss: 0.669647\n",
      "Train Epoch: 2 [29120/34001 (86%)]\tLoss: 0.638764\n",
      "Train Epoch: 2 [29440/34001 (87%)]\tLoss: 0.661292\n",
      "Train Epoch: 2 [29760/34001 (87%)]\tLoss: 0.733455\n",
      "Train Epoch: 2 [30080/34001 (88%)]\tLoss: 0.658951\n",
      "Train Epoch: 2 [30400/34001 (89%)]\tLoss: 0.679771\n",
      "Train Epoch: 2 [30720/34001 (90%)]\tLoss: 0.632942\n",
      "Train Epoch: 2 [31040/34001 (91%)]\tLoss: 0.701761\n",
      "Train Epoch: 2 [31360/34001 (92%)]\tLoss: 0.732401\n",
      "Train Epoch: 2 [31680/34001 (93%)]\tLoss: 0.649637\n",
      "Train Epoch: 2 [32000/34001 (94%)]\tLoss: 0.655232\n",
      "Train Epoch: 2 [32320/34001 (95%)]\tLoss: 0.685390\n",
      "Train Epoch: 2 [32640/34001 (96%)]\tLoss: 0.647628\n",
      "Train Epoch: 2 [32960/34001 (97%)]\tLoss: 0.680107\n",
      "Train Epoch: 2 [33280/34001 (98%)]\tLoss: 0.636204\n",
      "Train Epoch: 2 [33600/34001 (99%)]\tLoss: 0.685626\n",
      "Train Epoch: 2 [33920/34001 (100%)]\tLoss: 0.729929\n",
      "Train Accuracy after epoch 2: 0.598423575777183\n",
      "Validation set: Average loss: 0.0211, Accuracy: 5067/8501 (60%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-29 13:14:31,139]\u001b[0m Trial 6 finished with value: 0.596047523820727 and parameters: {'lr': 0.00046618703556985193, 'optimizer_name': 'AdamW', 'epochs': 3, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.4, 'scheduler': 'step'}. Best is trial 2 with value: 0.9349488295494648.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial params: {'lr': 0.0007264850487520609, 'optimizer_name': 'Adam', 'epochs': 2, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.8, 'scheduler': 'step'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/34001 (0%)]\tLoss: 0.696995\n",
      "Train Epoch: 0 [320/34001 (1%)]\tLoss: 0.655243\n",
      "Train Epoch: 0 [640/34001 (2%)]\tLoss: 0.604532\n",
      "Train Epoch: 0 [960/34001 (3%)]\tLoss: 0.672820\n",
      "Train Epoch: 0 [1280/34001 (4%)]\tLoss: 0.647557\n",
      "Train Epoch: 0 [1600/34001 (5%)]\tLoss: 0.724553\n",
      "Train Epoch: 0 [1920/34001 (6%)]\tLoss: 0.719099\n",
      "Train Epoch: 0 [2240/34001 (7%)]\tLoss: 0.733291\n",
      "Train Epoch: 0 [2560/34001 (8%)]\tLoss: 0.646331\n",
      "Train Epoch: 0 [2880/34001 (8%)]\tLoss: 0.590114\n",
      "Train Epoch: 0 [3200/34001 (9%)]\tLoss: 0.750274\n",
      "Train Epoch: 0 [3520/34001 (10%)]\tLoss: 0.646688\n",
      "Train Epoch: 0 [3840/34001 (11%)]\tLoss: 0.664197\n",
      "Train Epoch: 0 [4160/34001 (12%)]\tLoss: 0.707277\n",
      "Train Epoch: 0 [4480/34001 (13%)]\tLoss: 0.670372\n",
      "Train Epoch: 0 [4800/34001 (14%)]\tLoss: 0.684048\n",
      "Train Epoch: 0 [5120/34001 (15%)]\tLoss: 0.635058\n",
      "Train Epoch: 0 [5440/34001 (16%)]\tLoss: 0.623865\n",
      "Train Epoch: 0 [5760/34001 (17%)]\tLoss: 0.752336\n",
      "Train Epoch: 0 [6080/34001 (18%)]\tLoss: 0.615473\n",
      "Train Epoch: 0 [6400/34001 (19%)]\tLoss: 0.692859\n",
      "Train Epoch: 0 [6720/34001 (20%)]\tLoss: 0.725649\n",
      "Train Epoch: 0 [7040/34001 (21%)]\tLoss: 0.665451\n",
      "Train Epoch: 0 [7360/34001 (22%)]\tLoss: 0.711943\n",
      "Train Epoch: 0 [7680/34001 (23%)]\tLoss: 0.597351\n",
      "Train Epoch: 0 [8000/34001 (24%)]\tLoss: 0.668887\n",
      "Train Epoch: 0 [8320/34001 (24%)]\tLoss: 0.673603\n",
      "Train Epoch: 0 [8640/34001 (25%)]\tLoss: 0.682005\n",
      "Train Epoch: 0 [8960/34001 (26%)]\tLoss: 0.691690\n",
      "Train Epoch: 0 [9280/34001 (27%)]\tLoss: 0.657611\n",
      "Train Epoch: 0 [9600/34001 (28%)]\tLoss: 0.630369\n",
      "Train Epoch: 0 [9920/34001 (29%)]\tLoss: 0.680630\n",
      "Train Epoch: 0 [10240/34001 (30%)]\tLoss: 0.698712\n",
      "Train Epoch: 0 [10560/34001 (31%)]\tLoss: 0.678937\n",
      "Train Epoch: 0 [10880/34001 (32%)]\tLoss: 0.616307\n",
      "Train Epoch: 0 [11200/34001 (33%)]\tLoss: 0.722140\n",
      "Train Epoch: 0 [11520/34001 (34%)]\tLoss: 0.672224\n",
      "Train Epoch: 0 [11840/34001 (35%)]\tLoss: 0.684868\n",
      "Train Epoch: 0 [12160/34001 (36%)]\tLoss: 0.663256\n",
      "Train Epoch: 0 [12480/34001 (37%)]\tLoss: 0.717515\n",
      "Train Epoch: 0 [12800/34001 (38%)]\tLoss: 0.699137\n",
      "Train Epoch: 0 [13120/34001 (39%)]\tLoss: 0.678468\n",
      "Train Epoch: 0 [13440/34001 (40%)]\tLoss: 0.698250\n",
      "Train Epoch: 0 [13760/34001 (40%)]\tLoss: 0.682792\n",
      "Train Epoch: 0 [14080/34001 (41%)]\tLoss: 0.704293\n",
      "Train Epoch: 0 [14400/34001 (42%)]\tLoss: 0.712923\n",
      "Train Epoch: 0 [14720/34001 (43%)]\tLoss: 0.684146\n",
      "Train Epoch: 0 [15040/34001 (44%)]\tLoss: 0.635277\n",
      "Train Epoch: 0 [15360/34001 (45%)]\tLoss: 0.575951\n",
      "Train Epoch: 0 [15680/34001 (46%)]\tLoss: 0.679504\n",
      "Train Epoch: 0 [16000/34001 (47%)]\tLoss: 0.672302\n",
      "Train Epoch: 0 [16320/34001 (48%)]\tLoss: 0.717578\n",
      "Train Epoch: 0 [16640/34001 (49%)]\tLoss: 0.697115\n",
      "Train Epoch: 0 [16960/34001 (50%)]\tLoss: 0.670729\n",
      "Train Epoch: 0 [17280/34001 (51%)]\tLoss: 0.695678\n",
      "Train Epoch: 0 [17600/34001 (52%)]\tLoss: 0.692174\n",
      "Train Epoch: 0 [17920/34001 (53%)]\tLoss: 0.702777\n",
      "Train Epoch: 0 [18240/34001 (54%)]\tLoss: 0.651438\n",
      "Train Epoch: 0 [18560/34001 (55%)]\tLoss: 0.674817\n",
      "Train Epoch: 0 [18880/34001 (56%)]\tLoss: 0.707029\n",
      "Train Epoch: 0 [19200/34001 (56%)]\tLoss: 0.725211\n",
      "Train Epoch: 0 [19520/34001 (57%)]\tLoss: 0.663665\n",
      "Train Epoch: 0 [19840/34001 (58%)]\tLoss: 0.658203\n",
      "Train Epoch: 0 [20160/34001 (59%)]\tLoss: 0.686236\n",
      "Train Epoch: 0 [20480/34001 (60%)]\tLoss: 0.650227\n",
      "Train Epoch: 0 [20800/34001 (61%)]\tLoss: 0.691176\n",
      "Train Epoch: 0 [21120/34001 (62%)]\tLoss: 0.640194\n",
      "Train Epoch: 0 [21440/34001 (63%)]\tLoss: 0.589229\n",
      "Train Epoch: 0 [21760/34001 (64%)]\tLoss: 0.659030\n",
      "Train Epoch: 0 [22080/34001 (65%)]\tLoss: 0.645948\n",
      "Train Epoch: 0 [22400/34001 (66%)]\tLoss: 0.723949\n",
      "Train Epoch: 0 [22720/34001 (67%)]\tLoss: 0.684320\n",
      "Train Epoch: 0 [23040/34001 (68%)]\tLoss: 0.694903\n",
      "Train Epoch: 0 [23360/34001 (69%)]\tLoss: 0.673266\n",
      "Train Epoch: 0 [23680/34001 (70%)]\tLoss: 0.702523\n",
      "Train Epoch: 0 [24000/34001 (71%)]\tLoss: 0.680132\n",
      "Train Epoch: 0 [24320/34001 (71%)]\tLoss: 0.697501\n",
      "Train Epoch: 0 [24640/34001 (72%)]\tLoss: 0.689050\n",
      "Train Epoch: 0 [24960/34001 (73%)]\tLoss: 0.686150\n",
      "Train Epoch: 0 [25280/34001 (74%)]\tLoss: 0.666387\n",
      "Train Epoch: 0 [25600/34001 (75%)]\tLoss: 0.761316\n",
      "Train Epoch: 0 [25920/34001 (76%)]\tLoss: 0.756716\n",
      "Train Epoch: 0 [26240/34001 (77%)]\tLoss: 0.719730\n",
      "Train Epoch: 0 [26560/34001 (78%)]\tLoss: 0.654171\n",
      "Train Epoch: 0 [26880/34001 (79%)]\tLoss: 0.656241\n",
      "Train Epoch: 0 [27200/34001 (80%)]\tLoss: 0.737708\n",
      "Train Epoch: 0 [27520/34001 (81%)]\tLoss: 0.668928\n",
      "Train Epoch: 0 [27840/34001 (82%)]\tLoss: 0.689126\n",
      "Train Epoch: 0 [28160/34001 (83%)]\tLoss: 0.657249\n",
      "Train Epoch: 0 [28480/34001 (84%)]\tLoss: 0.698263\n",
      "Train Epoch: 0 [28800/34001 (85%)]\tLoss: 0.678151\n",
      "Train Epoch: 0 [29120/34001 (86%)]\tLoss: 0.660932\n",
      "Train Epoch: 0 [29440/34001 (87%)]\tLoss: 0.694435\n",
      "Train Epoch: 0 [29760/34001 (87%)]\tLoss: 0.658625\n",
      "Train Epoch: 0 [30080/34001 (88%)]\tLoss: 0.667846\n",
      "Train Epoch: 0 [30400/34001 (89%)]\tLoss: 0.631555\n",
      "Train Epoch: 0 [30720/34001 (90%)]\tLoss: 0.697859\n",
      "Train Epoch: 0 [31040/34001 (91%)]\tLoss: 0.674673\n",
      "Train Epoch: 0 [31360/34001 (92%)]\tLoss: 0.666031\n",
      "Train Epoch: 0 [31680/34001 (93%)]\tLoss: 0.685983\n",
      "Train Epoch: 0 [32000/34001 (94%)]\tLoss: 0.664475\n",
      "Train Epoch: 0 [32320/34001 (95%)]\tLoss: 0.663244\n",
      "Train Epoch: 0 [32640/34001 (96%)]\tLoss: 0.657000\n",
      "Train Epoch: 0 [32960/34001 (97%)]\tLoss: 0.691595\n",
      "Train Epoch: 0 [33280/34001 (98%)]\tLoss: 0.665704\n",
      "Train Epoch: 0 [33600/34001 (99%)]\tLoss: 0.666553\n",
      "Train Epoch: 0 [33920/34001 (100%)]\tLoss: 0.652998\n",
      "Train Accuracy after epoch 0: 0.5951883768124467\n",
      "Validation set: Average loss: 0.0210, Accuracy: 5109/8501 (60%)\n",
      "\n",
      "Train Epoch: 1 [0/34001 (0%)]\tLoss: 0.654215\n",
      "Train Epoch: 1 [320/34001 (1%)]\tLoss: 0.638726\n",
      "Train Epoch: 1 [640/34001 (2%)]\tLoss: 0.606951\n",
      "Train Epoch: 1 [960/34001 (3%)]\tLoss: 0.656746\n",
      "Train Epoch: 1 [1280/34001 (4%)]\tLoss: 0.626687\n",
      "Train Epoch: 1 [1600/34001 (5%)]\tLoss: 0.696760\n",
      "Train Epoch: 1 [1920/34001 (6%)]\tLoss: 0.689124\n",
      "Train Epoch: 1 [2240/34001 (7%)]\tLoss: 0.712449\n",
      "Train Epoch: 1 [2560/34001 (8%)]\tLoss: 0.647624\n",
      "Train Epoch: 1 [2880/34001 (8%)]\tLoss: 0.603671\n",
      "Train Epoch: 1 [3200/34001 (9%)]\tLoss: 0.746419\n",
      "Train Epoch: 1 [3520/34001 (10%)]\tLoss: 0.632149\n",
      "Train Epoch: 1 [3840/34001 (11%)]\tLoss: 0.662852\n",
      "Train Epoch: 1 [4160/34001 (12%)]\tLoss: 0.721503\n",
      "Train Epoch: 1 [4480/34001 (13%)]\tLoss: 0.663455\n",
      "Train Epoch: 1 [4800/34001 (14%)]\tLoss: 0.675124\n",
      "Train Epoch: 1 [5120/34001 (15%)]\tLoss: 0.620181\n",
      "Train Epoch: 1 [5440/34001 (16%)]\tLoss: 0.619359\n",
      "Train Epoch: 1 [5760/34001 (17%)]\tLoss: 0.720364\n",
      "Train Epoch: 1 [6080/34001 (18%)]\tLoss: 0.599734\n",
      "Train Epoch: 1 [6400/34001 (19%)]\tLoss: 0.699038\n",
      "Train Epoch: 1 [6720/34001 (20%)]\tLoss: 0.747449\n",
      "Train Epoch: 1 [7040/34001 (21%)]\tLoss: 0.660193\n",
      "Train Epoch: 1 [7360/34001 (22%)]\tLoss: 0.691750\n",
      "Train Epoch: 1 [7680/34001 (23%)]\tLoss: 0.615354\n",
      "Train Epoch: 1 [8000/34001 (24%)]\tLoss: 0.686328\n",
      "Train Epoch: 1 [8320/34001 (24%)]\tLoss: 0.653229\n",
      "Train Epoch: 1 [8640/34001 (25%)]\tLoss: 0.684621\n",
      "Train Epoch: 1 [8960/34001 (26%)]\tLoss: 0.696588\n",
      "Train Epoch: 1 [9280/34001 (27%)]\tLoss: 0.658821\n",
      "Train Epoch: 1 [9600/34001 (28%)]\tLoss: 0.639985\n",
      "Train Epoch: 1 [9920/34001 (29%)]\tLoss: 0.675772\n",
      "Train Epoch: 1 [10240/34001 (30%)]\tLoss: 0.701460\n",
      "Train Epoch: 1 [10560/34001 (31%)]\tLoss: 0.678351\n",
      "Train Epoch: 1 [10880/34001 (32%)]\tLoss: 0.652016\n",
      "Train Epoch: 1 [11200/34001 (33%)]\tLoss: 0.695275\n",
      "Train Epoch: 1 [11520/34001 (34%)]\tLoss: 0.668906\n",
      "Train Epoch: 1 [11840/34001 (35%)]\tLoss: 0.686518\n",
      "Train Epoch: 1 [12160/34001 (36%)]\tLoss: 0.682364\n",
      "Train Epoch: 1 [12480/34001 (37%)]\tLoss: 0.710699\n",
      "Train Epoch: 1 [12800/34001 (38%)]\tLoss: 0.689751\n",
      "Train Epoch: 1 [13120/34001 (39%)]\tLoss: 0.678067\n",
      "Train Epoch: 1 [13440/34001 (40%)]\tLoss: 0.712382\n",
      "Train Epoch: 1 [13760/34001 (40%)]\tLoss: 0.673899\n",
      "Train Epoch: 1 [14080/34001 (41%)]\tLoss: 0.687625\n",
      "Train Epoch: 1 [14400/34001 (42%)]\tLoss: 0.711591\n",
      "Train Epoch: 1 [14720/34001 (43%)]\tLoss: 0.656428\n",
      "Train Epoch: 1 [15040/34001 (44%)]\tLoss: 0.651334\n",
      "Train Epoch: 1 [15360/34001 (45%)]\tLoss: 0.586925\n",
      "Train Epoch: 1 [15680/34001 (46%)]\tLoss: 0.703888\n",
      "Train Epoch: 1 [16000/34001 (47%)]\tLoss: 0.665707\n",
      "Train Epoch: 1 [16320/34001 (48%)]\tLoss: 0.720981\n",
      "Train Epoch: 1 [16640/34001 (49%)]\tLoss: 0.703815\n",
      "Train Epoch: 1 [16960/34001 (50%)]\tLoss: 0.679453\n",
      "Train Epoch: 1 [17280/34001 (51%)]\tLoss: 0.700169\n",
      "Train Epoch: 1 [17600/34001 (52%)]\tLoss: 0.694873\n",
      "Train Epoch: 1 [17920/34001 (53%)]\tLoss: 0.693051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [18240/34001 (54%)]\tLoss: 0.654845\n",
      "Train Epoch: 1 [18560/34001 (55%)]\tLoss: 0.673421\n",
      "Train Epoch: 1 [18880/34001 (56%)]\tLoss: 0.720895\n",
      "Train Epoch: 1 [19200/34001 (56%)]\tLoss: 0.731014\n",
      "Train Epoch: 1 [19520/34001 (57%)]\tLoss: 0.666895\n",
      "Train Epoch: 1 [19840/34001 (58%)]\tLoss: 0.662550\n",
      "Train Epoch: 1 [20160/34001 (59%)]\tLoss: 0.688265\n",
      "Train Epoch: 1 [20480/34001 (60%)]\tLoss: 0.657762\n",
      "Train Epoch: 1 [20800/34001 (61%)]\tLoss: 0.686383\n",
      "Train Epoch: 1 [21120/34001 (62%)]\tLoss: 0.632216\n",
      "Train Epoch: 1 [21440/34001 (63%)]\tLoss: 0.598413\n",
      "Train Epoch: 1 [21760/34001 (64%)]\tLoss: 0.655625\n",
      "Train Epoch: 1 [22080/34001 (65%)]\tLoss: 0.638736\n",
      "Train Epoch: 1 [22400/34001 (66%)]\tLoss: 0.734173\n",
      "Train Epoch: 1 [22720/34001 (67%)]\tLoss: 0.695534\n",
      "Train Epoch: 1 [23040/34001 (68%)]\tLoss: 0.687455\n",
      "Train Epoch: 1 [23360/34001 (69%)]\tLoss: 0.672468\n",
      "Train Epoch: 1 [23680/34001 (70%)]\tLoss: 0.704852\n",
      "Train Epoch: 1 [24000/34001 (71%)]\tLoss: 0.682867\n",
      "Train Epoch: 1 [24320/34001 (71%)]\tLoss: 0.692486\n",
      "Train Epoch: 1 [24640/34001 (72%)]\tLoss: 0.689858\n",
      "Train Epoch: 1 [24960/34001 (73%)]\tLoss: 0.684016\n",
      "Train Epoch: 1 [25280/34001 (74%)]\tLoss: 0.659498\n",
      "Train Epoch: 1 [25600/34001 (75%)]\tLoss: 0.745504\n",
      "Train Epoch: 1 [25920/34001 (76%)]\tLoss: 0.766938\n",
      "Train Epoch: 1 [26240/34001 (77%)]\tLoss: 0.742088\n",
      "Train Epoch: 1 [26560/34001 (78%)]\tLoss: 0.656890\n",
      "Train Epoch: 1 [26880/34001 (79%)]\tLoss: 0.661447\n",
      "Train Epoch: 1 [27200/34001 (80%)]\tLoss: 0.721460\n",
      "Train Epoch: 1 [27520/34001 (81%)]\tLoss: 0.667224\n",
      "Train Epoch: 1 [27840/34001 (82%)]\tLoss: 0.692833\n",
      "Train Epoch: 1 [28160/34001 (83%)]\tLoss: 0.658999\n",
      "Train Epoch: 1 [28480/34001 (84%)]\tLoss: 0.704326\n",
      "Train Epoch: 1 [28800/34001 (85%)]\tLoss: 0.679939\n",
      "Train Epoch: 1 [29120/34001 (86%)]\tLoss: 0.659337\n",
      "Train Epoch: 1 [29440/34001 (87%)]\tLoss: 0.697700\n",
      "Train Epoch: 1 [29760/34001 (87%)]\tLoss: 0.660563\n",
      "Train Epoch: 1 [30080/34001 (88%)]\tLoss: 0.662289\n",
      "Train Epoch: 1 [30400/34001 (89%)]\tLoss: 0.629046\n",
      "Train Epoch: 1 [30720/34001 (90%)]\tLoss: 0.694073\n",
      "Train Epoch: 1 [31040/34001 (91%)]\tLoss: 0.687201\n",
      "Train Epoch: 1 [31360/34001 (92%)]\tLoss: 0.660304\n",
      "Train Epoch: 1 [31680/34001 (93%)]\tLoss: 0.668563\n",
      "Train Epoch: 1 [32000/34001 (94%)]\tLoss: 0.658583\n",
      "Train Epoch: 1 [32320/34001 (95%)]\tLoss: 0.659423\n",
      "Train Epoch: 1 [32640/34001 (96%)]\tLoss: 0.651558\n",
      "Train Epoch: 1 [32960/34001 (97%)]\tLoss: 0.689108\n",
      "Train Epoch: 1 [33280/34001 (98%)]\tLoss: 0.666692\n",
      "Train Epoch: 1 [33600/34001 (99%)]\tLoss: 0.660659\n",
      "Train Epoch: 1 [33920/34001 (100%)]\tLoss: 0.658500\n",
      "Train Accuracy after epoch 1: 0.5971883179906473\n",
      "Validation set: Average loss: 0.0211, Accuracy: 5109/8501 (60%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-29 13:34:00,055]\u001b[0m Trial 7 finished with value: 0.6009881190448183 and parameters: {'lr': 0.0007264850487520609, 'optimizer_name': 'Adam', 'epochs': 2, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.8, 'scheduler': 'step'}. Best is trial 2 with value: 0.9349488295494648.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial params: {'lr': 5.514824762681477e-06, 'optimizer_name': 'AdamW', 'epochs': 5, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.8, 'scheduler': 'exponential'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/34001 (0%)]\tLoss: 0.698128\n",
      "Train Epoch: 0 [320/34001 (1%)]\tLoss: 0.653895\n",
      "Train Epoch: 0 [640/34001 (2%)]\tLoss: 0.585923\n",
      "Train Epoch: 0 [960/34001 (3%)]\tLoss: 0.501377\n",
      "Train Epoch: 0 [1280/34001 (4%)]\tLoss: 0.489554\n",
      "Train Epoch: 0 [1600/34001 (5%)]\tLoss: 0.334182\n",
      "Train Epoch: 0 [1920/34001 (6%)]\tLoss: 0.543383\n",
      "Train Epoch: 0 [2240/34001 (7%)]\tLoss: 0.266424\n",
      "Train Epoch: 0 [2560/34001 (8%)]\tLoss: 0.370092\n",
      "Train Epoch: 0 [2880/34001 (8%)]\tLoss: 0.265230\n",
      "Train Epoch: 0 [3200/34001 (9%)]\tLoss: 0.344021\n",
      "Train Epoch: 0 [3520/34001 (10%)]\tLoss: 0.284838\n",
      "Train Epoch: 0 [3840/34001 (11%)]\tLoss: 0.412189\n",
      "Train Epoch: 0 [4160/34001 (12%)]\tLoss: 0.260352\n",
      "Train Epoch: 0 [4480/34001 (13%)]\tLoss: 0.341396\n",
      "Train Epoch: 0 [4800/34001 (14%)]\tLoss: 0.453749\n",
      "Train Epoch: 0 [5120/34001 (15%)]\tLoss: 0.219170\n",
      "Train Epoch: 0 [5440/34001 (16%)]\tLoss: 0.268107\n",
      "Train Epoch: 0 [5760/34001 (17%)]\tLoss: 0.256214\n",
      "Train Epoch: 0 [6080/34001 (18%)]\tLoss: 0.289737\n",
      "Train Epoch: 0 [6400/34001 (19%)]\tLoss: 0.210732\n",
      "Train Epoch: 0 [6720/34001 (20%)]\tLoss: 0.102479\n",
      "Train Epoch: 0 [7040/34001 (21%)]\tLoss: 0.348701\n",
      "Train Epoch: 0 [7360/34001 (22%)]\tLoss: 0.328191\n",
      "Train Epoch: 0 [7680/34001 (23%)]\tLoss: 0.174504\n",
      "Train Epoch: 0 [8000/34001 (24%)]\tLoss: 0.442295\n",
      "Train Epoch: 0 [8320/34001 (24%)]\tLoss: 0.210209\n",
      "Train Epoch: 0 [8640/34001 (25%)]\tLoss: 0.123865\n",
      "Train Epoch: 0 [8960/34001 (26%)]\tLoss: 0.291501\n",
      "Train Epoch: 0 [9280/34001 (27%)]\tLoss: 0.169197\n",
      "Train Epoch: 0 [9600/34001 (28%)]\tLoss: 0.124502\n",
      "Train Epoch: 0 [9920/34001 (29%)]\tLoss: 0.309102\n",
      "Train Epoch: 0 [10240/34001 (30%)]\tLoss: 0.375562\n",
      "Train Epoch: 0 [10560/34001 (31%)]\tLoss: 0.184480\n",
      "Train Epoch: 0 [10880/34001 (32%)]\tLoss: 0.265605\n",
      "Train Epoch: 0 [11200/34001 (33%)]\tLoss: 0.179608\n",
      "Train Epoch: 0 [11520/34001 (34%)]\tLoss: 0.367813\n",
      "Train Epoch: 0 [11840/34001 (35%)]\tLoss: 0.181261\n",
      "Train Epoch: 0 [12160/34001 (36%)]\tLoss: 0.277937\n",
      "Train Epoch: 0 [12480/34001 (37%)]\tLoss: 0.166440\n",
      "Train Epoch: 0 [12800/34001 (38%)]\tLoss: 0.109628\n",
      "Train Epoch: 0 [13120/34001 (39%)]\tLoss: 0.227414\n",
      "Train Epoch: 0 [13440/34001 (40%)]\tLoss: 0.248974\n",
      "Train Epoch: 0 [13760/34001 (40%)]\tLoss: 0.307273\n",
      "Train Epoch: 0 [14080/34001 (41%)]\tLoss: 0.109180\n",
      "Train Epoch: 0 [14400/34001 (42%)]\tLoss: 0.193718\n",
      "Train Epoch: 0 [14720/34001 (43%)]\tLoss: 0.118222\n",
      "Train Epoch: 0 [15040/34001 (44%)]\tLoss: 0.132778\n",
      "Train Epoch: 0 [15360/34001 (45%)]\tLoss: 0.093885\n",
      "Train Epoch: 0 [15680/34001 (46%)]\tLoss: 0.232049\n",
      "Train Epoch: 0 [16000/34001 (47%)]\tLoss: 0.191897\n",
      "Train Epoch: 0 [16320/34001 (48%)]\tLoss: 0.206453\n",
      "Train Epoch: 0 [16640/34001 (49%)]\tLoss: 0.127794\n",
      "Train Epoch: 0 [16960/34001 (50%)]\tLoss: 0.229015\n",
      "Train Epoch: 0 [17280/34001 (51%)]\tLoss: 0.382677\n",
      "Train Epoch: 0 [17600/34001 (52%)]\tLoss: 0.154112\n",
      "Train Epoch: 0 [17920/34001 (53%)]\tLoss: 0.090613\n",
      "Train Epoch: 0 [18240/34001 (54%)]\tLoss: 0.224845\n",
      "Train Epoch: 0 [18560/34001 (55%)]\tLoss: 0.227619\n",
      "Train Epoch: 0 [18880/34001 (56%)]\tLoss: 0.294189\n",
      "Train Epoch: 0 [19200/34001 (56%)]\tLoss: 0.312495\n",
      "Train Epoch: 0 [19520/34001 (57%)]\tLoss: 0.313874\n",
      "Train Epoch: 0 [19840/34001 (58%)]\tLoss: 0.209477\n",
      "Train Epoch: 0 [20160/34001 (59%)]\tLoss: 0.136585\n",
      "Train Epoch: 0 [20480/34001 (60%)]\tLoss: 0.230174\n",
      "Train Epoch: 0 [20800/34001 (61%)]\tLoss: 0.253625\n",
      "Train Epoch: 0 [21120/34001 (62%)]\tLoss: 0.275686\n",
      "Train Epoch: 0 [21440/34001 (63%)]\tLoss: 0.317781\n",
      "Train Epoch: 0 [21760/34001 (64%)]\tLoss: 0.250225\n",
      "Train Epoch: 0 [22080/34001 (65%)]\tLoss: 0.225598\n",
      "Train Epoch: 0 [22400/34001 (66%)]\tLoss: 0.124253\n",
      "Train Epoch: 0 [22720/34001 (67%)]\tLoss: 0.225149\n",
      "Train Epoch: 0 [23040/34001 (68%)]\tLoss: 0.075711\n",
      "Train Epoch: 0 [23360/34001 (69%)]\tLoss: 0.160241\n",
      "Train Epoch: 0 [23680/34001 (70%)]\tLoss: 0.222549\n",
      "Train Epoch: 0 [24000/34001 (71%)]\tLoss: 0.195167\n",
      "Train Epoch: 0 [24320/34001 (71%)]\tLoss: 0.203773\n",
      "Train Epoch: 0 [24640/34001 (72%)]\tLoss: 0.237400\n",
      "Train Epoch: 0 [24960/34001 (73%)]\tLoss: 0.146122\n",
      "Train Epoch: 0 [25280/34001 (74%)]\tLoss: 0.195995\n",
      "Train Epoch: 0 [25600/34001 (75%)]\tLoss: 0.252591\n",
      "Train Epoch: 0 [25920/34001 (76%)]\tLoss: 0.269804\n",
      "Train Epoch: 0 [26240/34001 (77%)]\tLoss: 0.182557\n",
      "Train Epoch: 0 [26560/34001 (78%)]\tLoss: 0.266105\n",
      "Train Epoch: 0 [26880/34001 (79%)]\tLoss: 0.239505\n",
      "Train Epoch: 0 [27200/34001 (80%)]\tLoss: 0.150478\n",
      "Train Epoch: 0 [27520/34001 (81%)]\tLoss: 0.122002\n",
      "Train Epoch: 0 [27840/34001 (82%)]\tLoss: 0.185377\n",
      "Train Epoch: 0 [28160/34001 (83%)]\tLoss: 0.103983\n",
      "Train Epoch: 0 [28480/34001 (84%)]\tLoss: 0.164804\n",
      "Train Epoch: 0 [28800/34001 (85%)]\tLoss: 0.103743\n",
      "Train Epoch: 0 [29120/34001 (86%)]\tLoss: 0.217639\n",
      "Train Epoch: 0 [29440/34001 (87%)]\tLoss: 0.060124\n",
      "Train Epoch: 0 [29760/34001 (87%)]\tLoss: 0.282424\n",
      "Train Epoch: 0 [30080/34001 (88%)]\tLoss: 0.207184\n",
      "Train Epoch: 0 [30400/34001 (89%)]\tLoss: 0.222171\n",
      "Train Epoch: 0 [30720/34001 (90%)]\tLoss: 0.157671\n",
      "Train Epoch: 0 [31040/34001 (91%)]\tLoss: 0.051909\n",
      "Train Epoch: 0 [31360/34001 (92%)]\tLoss: 0.071653\n",
      "Train Epoch: 0 [31680/34001 (93%)]\tLoss: 0.106085\n",
      "Train Epoch: 0 [32000/34001 (94%)]\tLoss: 0.043575\n",
      "Train Epoch: 0 [32320/34001 (95%)]\tLoss: 0.228926\n",
      "Train Epoch: 0 [32640/34001 (96%)]\tLoss: 0.227132\n",
      "Train Epoch: 0 [32960/34001 (97%)]\tLoss: 0.266257\n",
      "Train Epoch: 0 [33280/34001 (98%)]\tLoss: 0.161061\n",
      "Train Epoch: 0 [33600/34001 (99%)]\tLoss: 0.091673\n",
      "Train Epoch: 0 [33920/34001 (100%)]\tLoss: 0.302372\n",
      "Train Accuracy after epoch 0: 0.8965030440281169\n",
      "Validation set: Average loss: 0.0056, Accuracy: 7839/8501 (92%)\n",
      "\n",
      "Train Epoch: 1 [0/34001 (0%)]\tLoss: 0.166500\n",
      "Train Epoch: 1 [320/34001 (1%)]\tLoss: 0.168949\n",
      "Train Epoch: 1 [640/34001 (2%)]\tLoss: 0.392314\n",
      "Train Epoch: 1 [960/34001 (3%)]\tLoss: 0.074035\n",
      "Train Epoch: 1 [1280/34001 (4%)]\tLoss: 0.199425\n",
      "Train Epoch: 1 [1600/34001 (5%)]\tLoss: 0.138565\n",
      "Train Epoch: 1 [1920/34001 (6%)]\tLoss: 0.144099\n",
      "Train Epoch: 1 [2240/34001 (7%)]\tLoss: 0.097655\n",
      "Train Epoch: 1 [2560/34001 (8%)]\tLoss: 0.132183\n",
      "Train Epoch: 1 [2880/34001 (8%)]\tLoss: 0.086611\n",
      "Train Epoch: 1 [3200/34001 (9%)]\tLoss: 0.139599\n",
      "Train Epoch: 1 [3520/34001 (10%)]\tLoss: 0.138107\n",
      "Train Epoch: 1 [3840/34001 (11%)]\tLoss: 0.324641\n",
      "Train Epoch: 1 [4160/34001 (12%)]\tLoss: 0.155341\n",
      "Train Epoch: 1 [4480/34001 (13%)]\tLoss: 0.251355\n",
      "Train Epoch: 1 [4800/34001 (14%)]\tLoss: 0.258309\n",
      "Train Epoch: 1 [5120/34001 (15%)]\tLoss: 0.086054\n",
      "Train Epoch: 1 [5440/34001 (16%)]\tLoss: 0.119493\n",
      "Train Epoch: 1 [5760/34001 (17%)]\tLoss: 0.262756\n",
      "Train Epoch: 1 [6080/34001 (18%)]\tLoss: 0.215907\n",
      "Train Epoch: 1 [6400/34001 (19%)]\tLoss: 0.148154\n",
      "Train Epoch: 1 [6720/34001 (20%)]\tLoss: 0.040397\n",
      "Train Epoch: 1 [7040/34001 (21%)]\tLoss: 0.318722\n",
      "Train Epoch: 1 [7360/34001 (22%)]\tLoss: 0.156417\n",
      "Train Epoch: 1 [7680/34001 (23%)]\tLoss: 0.131007\n",
      "Train Epoch: 1 [8000/34001 (24%)]\tLoss: 0.275243\n",
      "Train Epoch: 1 [8320/34001 (24%)]\tLoss: 0.062142\n",
      "Train Epoch: 1 [8640/34001 (25%)]\tLoss: 0.070239\n",
      "Train Epoch: 1 [8960/34001 (26%)]\tLoss: 0.209167\n",
      "Train Epoch: 1 [9280/34001 (27%)]\tLoss: 0.106950\n",
      "Train Epoch: 1 [9600/34001 (28%)]\tLoss: 0.090217\n",
      "Train Epoch: 1 [9920/34001 (29%)]\tLoss: 0.273103\n",
      "Train Epoch: 1 [10240/34001 (30%)]\tLoss: 0.258406\n",
      "Train Epoch: 1 [10560/34001 (31%)]\tLoss: 0.079125\n",
      "Train Epoch: 1 [10880/34001 (32%)]\tLoss: 0.079886\n",
      "Train Epoch: 1 [11200/34001 (33%)]\tLoss: 0.147866\n",
      "Train Epoch: 1 [11520/34001 (34%)]\tLoss: 0.283345\n",
      "Train Epoch: 1 [11840/34001 (35%)]\tLoss: 0.152595\n",
      "Train Epoch: 1 [12160/34001 (36%)]\tLoss: 0.225987\n",
      "Train Epoch: 1 [12480/34001 (37%)]\tLoss: 0.114158\n",
      "Train Epoch: 1 [12800/34001 (38%)]\tLoss: 0.063542\n",
      "Train Epoch: 1 [13120/34001 (39%)]\tLoss: 0.152972\n",
      "Train Epoch: 1 [13440/34001 (40%)]\tLoss: 0.180805\n",
      "Train Epoch: 1 [13760/34001 (40%)]\tLoss: 0.282075\n",
      "Train Epoch: 1 [14080/34001 (41%)]\tLoss: 0.066640\n",
      "Train Epoch: 1 [14400/34001 (42%)]\tLoss: 0.162621\n",
      "Train Epoch: 1 [14720/34001 (43%)]\tLoss: 0.133960\n",
      "Train Epoch: 1 [15040/34001 (44%)]\tLoss: 0.108388\n",
      "Train Epoch: 1 [15360/34001 (45%)]\tLoss: 0.074097\n",
      "Train Epoch: 1 [15680/34001 (46%)]\tLoss: 0.220169\n",
      "Train Epoch: 1 [16000/34001 (47%)]\tLoss: 0.113985\n",
      "Train Epoch: 1 [16320/34001 (48%)]\tLoss: 0.126704\n",
      "Train Epoch: 1 [16640/34001 (49%)]\tLoss: 0.122643\n",
      "Train Epoch: 1 [16960/34001 (50%)]\tLoss: 0.170169\n",
      "Train Epoch: 1 [17280/34001 (51%)]\tLoss: 0.240656\n",
      "Train Epoch: 1 [17600/34001 (52%)]\tLoss: 0.108329\n",
      "Train Epoch: 1 [17920/34001 (53%)]\tLoss: 0.083159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [18240/34001 (54%)]\tLoss: 0.163557\n",
      "Train Epoch: 1 [18560/34001 (55%)]\tLoss: 0.166552\n",
      "Train Epoch: 1 [18880/34001 (56%)]\tLoss: 0.227346\n",
      "Train Epoch: 1 [19200/34001 (56%)]\tLoss: 0.227558\n",
      "Train Epoch: 1 [19520/34001 (57%)]\tLoss: 0.159492\n",
      "Train Epoch: 1 [19840/34001 (58%)]\tLoss: 0.205119\n",
      "Train Epoch: 1 [20160/34001 (59%)]\tLoss: 0.087869\n",
      "Train Epoch: 1 [20480/34001 (60%)]\tLoss: 0.193275\n",
      "Train Epoch: 1 [20800/34001 (61%)]\tLoss: 0.226337\n",
      "Train Epoch: 1 [21120/34001 (62%)]\tLoss: 0.189857\n",
      "Train Epoch: 1 [21440/34001 (63%)]\tLoss: 0.257110\n",
      "Train Epoch: 1 [21760/34001 (64%)]\tLoss: 0.173993\n",
      "Train Epoch: 1 [22080/34001 (65%)]\tLoss: 0.165174\n",
      "Train Epoch: 1 [22400/34001 (66%)]\tLoss: 0.088752\n",
      "Train Epoch: 1 [22720/34001 (67%)]\tLoss: 0.226583\n",
      "Train Epoch: 1 [23040/34001 (68%)]\tLoss: 0.046426\n",
      "Train Epoch: 1 [23360/34001 (69%)]\tLoss: 0.105704\n",
      "Train Epoch: 1 [23680/34001 (70%)]\tLoss: 0.193968\n",
      "Train Epoch: 1 [24000/34001 (71%)]\tLoss: 0.147922\n",
      "Train Epoch: 1 [24320/34001 (71%)]\tLoss: 0.181423\n",
      "Train Epoch: 1 [24640/34001 (72%)]\tLoss: 0.229570\n",
      "Train Epoch: 1 [24960/34001 (73%)]\tLoss: 0.117100\n",
      "Train Epoch: 1 [25280/34001 (74%)]\tLoss: 0.140884\n",
      "Train Epoch: 1 [25600/34001 (75%)]\tLoss: 0.188755\n",
      "Train Epoch: 1 [25920/34001 (76%)]\tLoss: 0.276443\n",
      "Train Epoch: 1 [26240/34001 (77%)]\tLoss: 0.211797\n",
      "Train Epoch: 1 [26560/34001 (78%)]\tLoss: 0.187439\n",
      "Train Epoch: 1 [26880/34001 (79%)]\tLoss: 0.160020\n",
      "Train Epoch: 1 [27200/34001 (80%)]\tLoss: 0.135936\n",
      "Train Epoch: 1 [27520/34001 (81%)]\tLoss: 0.061276\n",
      "Train Epoch: 1 [27840/34001 (82%)]\tLoss: 0.116976\n",
      "Train Epoch: 1 [28160/34001 (83%)]\tLoss: 0.064200\n",
      "Train Epoch: 1 [28480/34001 (84%)]\tLoss: 0.169842\n",
      "Train Epoch: 1 [28800/34001 (85%)]\tLoss: 0.088330\n",
      "Train Epoch: 1 [29120/34001 (86%)]\tLoss: 0.218481\n",
      "Train Epoch: 1 [29440/34001 (87%)]\tLoss: 0.038435\n",
      "Train Epoch: 1 [29760/34001 (87%)]\tLoss: 0.201211\n",
      "Train Epoch: 1 [30080/34001 (88%)]\tLoss: 0.201180\n",
      "Train Epoch: 1 [30400/34001 (89%)]\tLoss: 0.154230\n",
      "Train Epoch: 1 [30720/34001 (90%)]\tLoss: 0.155063\n",
      "Train Epoch: 1 [31040/34001 (91%)]\tLoss: 0.044698\n",
      "Train Epoch: 1 [31360/34001 (92%)]\tLoss: 0.066282\n",
      "Train Epoch: 1 [31680/34001 (93%)]\tLoss: 0.093601\n",
      "Train Epoch: 1 [32000/34001 (94%)]\tLoss: 0.064423\n",
      "Train Epoch: 1 [32320/34001 (95%)]\tLoss: 0.204569\n",
      "Train Epoch: 1 [32640/34001 (96%)]\tLoss: 0.165371\n",
      "Train Epoch: 1 [32960/34001 (97%)]\tLoss: 0.147642\n",
      "Train Epoch: 1 [33280/34001 (98%)]\tLoss: 0.109369\n",
      "Train Epoch: 1 [33600/34001 (99%)]\tLoss: 0.091057\n",
      "Train Epoch: 1 [33920/34001 (100%)]\tLoss: 0.206417\n",
      "Train Accuracy after epoch 1: 0.9321490544395753\n",
      "Validation set: Average loss: 0.0051, Accuracy: 7900/8501 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/34001 (0%)]\tLoss: 0.161152\n",
      "Train Epoch: 2 [320/34001 (1%)]\tLoss: 0.179756\n",
      "Train Epoch: 2 [640/34001 (2%)]\tLoss: 0.361071\n",
      "Train Epoch: 2 [960/34001 (3%)]\tLoss: 0.047461\n",
      "Train Epoch: 2 [1280/34001 (4%)]\tLoss: 0.188796\n",
      "Train Epoch: 2 [1600/34001 (5%)]\tLoss: 0.131570\n",
      "Train Epoch: 2 [1920/34001 (6%)]\tLoss: 0.146478\n",
      "Train Epoch: 2 [2240/34001 (7%)]\tLoss: 0.084621\n",
      "Train Epoch: 2 [2560/34001 (8%)]\tLoss: 0.115878\n",
      "Train Epoch: 2 [2880/34001 (8%)]\tLoss: 0.058598\n",
      "Train Epoch: 2 [3200/34001 (9%)]\tLoss: 0.112031\n",
      "Train Epoch: 2 [3520/34001 (10%)]\tLoss: 0.128248\n",
      "Train Epoch: 2 [3840/34001 (11%)]\tLoss: 0.251168\n",
      "Train Epoch: 2 [4160/34001 (12%)]\tLoss: 0.151240\n",
      "Train Epoch: 2 [4480/34001 (13%)]\tLoss: 0.307207\n",
      "Train Epoch: 2 [4800/34001 (14%)]\tLoss: 0.175849\n",
      "Train Epoch: 2 [5120/34001 (15%)]\tLoss: 0.060196\n",
      "Train Epoch: 2 [5440/34001 (16%)]\tLoss: 0.130876\n",
      "Train Epoch: 2 [5760/34001 (17%)]\tLoss: 0.142038\n",
      "Train Epoch: 2 [6080/34001 (18%)]\tLoss: 0.124012\n",
      "Train Epoch: 2 [6400/34001 (19%)]\tLoss: 0.105281\n",
      "Train Epoch: 2 [6720/34001 (20%)]\tLoss: 0.035930\n",
      "Train Epoch: 2 [7040/34001 (21%)]\tLoss: 0.252584\n",
      "Train Epoch: 2 [7360/34001 (22%)]\tLoss: 0.148735\n",
      "Train Epoch: 2 [7680/34001 (23%)]\tLoss: 0.111998\n",
      "Train Epoch: 2 [8000/34001 (24%)]\tLoss: 0.202410\n",
      "Train Epoch: 2 [8320/34001 (24%)]\tLoss: 0.048723\n",
      "Train Epoch: 2 [8640/34001 (25%)]\tLoss: 0.077769\n",
      "Train Epoch: 2 [8960/34001 (26%)]\tLoss: 0.194846\n",
      "Train Epoch: 2 [9280/34001 (27%)]\tLoss: 0.055802\n",
      "Train Epoch: 2 [9600/34001 (28%)]\tLoss: 0.111163\n",
      "Train Epoch: 2 [9920/34001 (29%)]\tLoss: 0.275282\n",
      "Train Epoch: 2 [10240/34001 (30%)]\tLoss: 0.231588\n",
      "Train Epoch: 2 [10560/34001 (31%)]\tLoss: 0.047766\n",
      "Train Epoch: 2 [10880/34001 (32%)]\tLoss: 0.051327\n",
      "Train Epoch: 2 [11200/34001 (33%)]\tLoss: 0.121300\n",
      "Train Epoch: 2 [11520/34001 (34%)]\tLoss: 0.193321\n",
      "Train Epoch: 2 [11840/34001 (35%)]\tLoss: 0.155177\n",
      "Train Epoch: 2 [12160/34001 (36%)]\tLoss: 0.191657\n",
      "Train Epoch: 2 [12480/34001 (37%)]\tLoss: 0.095690\n",
      "Train Epoch: 2 [12800/34001 (38%)]\tLoss: 0.057270\n",
      "Train Epoch: 2 [13120/34001 (39%)]\tLoss: 0.184606\n",
      "Train Epoch: 2 [13440/34001 (40%)]\tLoss: 0.132522\n",
      "Train Epoch: 2 [13760/34001 (40%)]\tLoss: 0.270708\n",
      "Train Epoch: 2 [14080/34001 (41%)]\tLoss: 0.073417\n",
      "Train Epoch: 2 [14400/34001 (42%)]\tLoss: 0.128286\n",
      "Train Epoch: 2 [14720/34001 (43%)]\tLoss: 0.095076\n",
      "Train Epoch: 2 [15040/34001 (44%)]\tLoss: 0.075271\n",
      "Train Epoch: 2 [15360/34001 (45%)]\tLoss: 0.068135\n",
      "Train Epoch: 2 [15680/34001 (46%)]\tLoss: 0.203209\n",
      "Train Epoch: 2 [16000/34001 (47%)]\tLoss: 0.081476\n",
      "Train Epoch: 2 [16320/34001 (48%)]\tLoss: 0.065595\n",
      "Train Epoch: 2 [16640/34001 (49%)]\tLoss: 0.085877\n",
      "Train Epoch: 2 [16960/34001 (50%)]\tLoss: 0.118711\n",
      "Train Epoch: 2 [17280/34001 (51%)]\tLoss: 0.209453\n",
      "Train Epoch: 2 [17600/34001 (52%)]\tLoss: 0.091799\n",
      "Train Epoch: 2 [17920/34001 (53%)]\tLoss: 0.064823\n",
      "Train Epoch: 2 [18240/34001 (54%)]\tLoss: 0.137201\n",
      "Train Epoch: 2 [18560/34001 (55%)]\tLoss: 0.158587\n",
      "Train Epoch: 2 [18880/34001 (56%)]\tLoss: 0.194416\n",
      "Train Epoch: 2 [19200/34001 (56%)]\tLoss: 0.146673\n",
      "Train Epoch: 2 [19520/34001 (57%)]\tLoss: 0.111689\n",
      "Train Epoch: 2 [19840/34001 (58%)]\tLoss: 0.176263\n",
      "Train Epoch: 2 [20160/34001 (59%)]\tLoss: 0.053996\n",
      "Train Epoch: 2 [20480/34001 (60%)]\tLoss: 0.197303\n",
      "Train Epoch: 2 [20800/34001 (61%)]\tLoss: 0.204978\n",
      "Train Epoch: 2 [21120/34001 (62%)]\tLoss: 0.158578\n",
      "Train Epoch: 2 [21440/34001 (63%)]\tLoss: 0.219534\n",
      "Train Epoch: 2 [21760/34001 (64%)]\tLoss: 0.098907\n",
      "Train Epoch: 2 [22080/34001 (65%)]\tLoss: 0.102542\n",
      "Train Epoch: 2 [22400/34001 (66%)]\tLoss: 0.078457\n",
      "Train Epoch: 2 [22720/34001 (67%)]\tLoss: 0.127907\n",
      "Train Epoch: 2 [23040/34001 (68%)]\tLoss: 0.046156\n",
      "Train Epoch: 2 [23360/34001 (69%)]\tLoss: 0.189512\n",
      "Train Epoch: 2 [23680/34001 (70%)]\tLoss: 0.158346\n",
      "Train Epoch: 2 [24000/34001 (71%)]\tLoss: 0.124253\n",
      "Train Epoch: 2 [24320/34001 (71%)]\tLoss: 0.114398\n",
      "Train Epoch: 2 [24640/34001 (72%)]\tLoss: 0.190719\n",
      "Train Epoch: 2 [24960/34001 (73%)]\tLoss: 0.087382\n",
      "Train Epoch: 2 [25280/34001 (74%)]\tLoss: 0.169725\n",
      "Train Epoch: 2 [25600/34001 (75%)]\tLoss: 0.220911\n",
      "Train Epoch: 2 [25920/34001 (76%)]\tLoss: 0.235980\n",
      "Train Epoch: 2 [26240/34001 (77%)]\tLoss: 0.194893\n",
      "Train Epoch: 2 [26560/34001 (78%)]\tLoss: 0.156878\n",
      "Train Epoch: 2 [26880/34001 (79%)]\tLoss: 0.121860\n",
      "Train Epoch: 2 [27200/34001 (80%)]\tLoss: 0.120667\n",
      "Train Epoch: 2 [27520/34001 (81%)]\tLoss: 0.043305\n",
      "Train Epoch: 2 [27840/34001 (82%)]\tLoss: 0.131925\n",
      "Train Epoch: 2 [28160/34001 (83%)]\tLoss: 0.049425\n",
      "Train Epoch: 2 [28480/34001 (84%)]\tLoss: 0.116899\n",
      "Train Epoch: 2 [28800/34001 (85%)]\tLoss: 0.046567\n",
      "Train Epoch: 2 [29120/34001 (86%)]\tLoss: 0.253253\n",
      "Train Epoch: 2 [29440/34001 (87%)]\tLoss: 0.024978\n",
      "Train Epoch: 2 [29760/34001 (87%)]\tLoss: 0.094164\n",
      "Train Epoch: 2 [30080/34001 (88%)]\tLoss: 0.184790\n",
      "Train Epoch: 2 [30400/34001 (89%)]\tLoss: 0.156396\n",
      "Train Epoch: 2 [30720/34001 (90%)]\tLoss: 0.086941\n",
      "Train Epoch: 2 [31040/34001 (91%)]\tLoss: 0.042661\n",
      "Train Epoch: 2 [31360/34001 (92%)]\tLoss: 0.051137\n",
      "Train Epoch: 2 [31680/34001 (93%)]\tLoss: 0.050334\n",
      "Train Epoch: 2 [32000/34001 (94%)]\tLoss: 0.058307\n",
      "Train Epoch: 2 [32320/34001 (95%)]\tLoss: 0.193449\n",
      "Train Epoch: 2 [32640/34001 (96%)]\tLoss: 0.092858\n",
      "Train Epoch: 2 [32960/34001 (97%)]\tLoss: 0.058636\n",
      "Train Epoch: 2 [33280/34001 (98%)]\tLoss: 0.109419\n",
      "Train Epoch: 2 [33600/34001 (99%)]\tLoss: 0.053911\n",
      "Train Epoch: 2 [33920/34001 (100%)]\tLoss: 0.192674\n",
      "Train Accuracy after epoch 2: 0.9439428252110232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-04-29 14:02:48,889]\u001b[0m Trial 8 failed with parameters: {'lr': 5.514824762681477e-06, 'optimizer_name': 'AdamW', 'epochs': 5, 'model': 'distilbert', 'batch_size': 32, 'gamma': 0.8, 'scheduler': 'exponential'} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bibl1\\anaconda3\\envs\\edutech\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\bibl1\\AppData\\Local\\Temp\\ipykernel_23128\\822735883.py\", line 61, in objective\n",
      "    avg_val_loss, val_accuracy = validate(model, device, test_loader)\n",
      "  File \"C:\\Users\\bibl1\\AppData\\Local\\Temp\\ipykernel_23128\\2436376795.py\", line 15, in validate\n",
      "    pred = logits.argmax(dim=1, keepdim=True)\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-04-29 14:02:48,890]\u001b[0m Trial 8 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m global_val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 10\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m best_val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaim_detection\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      8\u001b[0m                             direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      9\u001b[0m                             sampler\u001b[38;5;241m=\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m---> 10\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\edutech\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\edutech\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\edutech\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\edutech\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\edutech\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[30], line 61\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     60\u001b[0m     avg_train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m train(model, device, train_loader, optimizer, epoch)\n\u001b[1;32m---> 61\u001b[0m     avg_val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m avg_val_loss \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m best_val_loss:\n\u001b[0;32m     64\u001b[0m         best_val_loss \u001b[38;5;241m=\u001b[39m avg_val_loss\n",
      "Cell \u001b[1;32mIn[24], line 15\u001b[0m, in \u001b[0;36mvalidate\u001b[1;34m(model, device, val_loader)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#             val_loss += F.nll_loss(logits, target, reduction='sum').item()\u001b[39;00m\n\u001b[0;32m     14\u001b[0m             val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 15\u001b[0m             pred \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m             correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39meq(target\u001b[38;5;241m.\u001b[39mview_as(pred))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     18\u001b[0m     val_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m val_set_size\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_val_accuracy = 0\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963e267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
